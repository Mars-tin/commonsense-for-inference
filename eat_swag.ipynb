{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eat-tuned.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eJDJ8zF9LH6y",
        "XCTR69kyKAu1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIUz6Xrk7Mn"
      },
      "source": [
        "# EAT: Tuned with Knowledge\n",
        "\n",
        "EECS 595 Final Project, Task 3: EAT\n",
        "\n",
        "Credit: Ziqiao Ma\n",
        "\n",
        "Last update: 2020.12.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7BQNAknKvsn"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrMYC3StmXpm"
      },
      "source": [
        "## Colab setups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LeDbHL6oLFI"
      },
      "source": [
        "Run this cell load the autoreload extension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUiA5dX3yFGX"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np_T5dXFoSrk"
      },
      "source": [
        "Run the following cell to mount your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s36tQoeYVLA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe581d60-619b-4e6d-9463-6ab0baf7c461"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KMR3UxKoedX"
      },
      "source": [
        "Fill in the Google Drive path where you uploaded the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKGqTElTolf8"
      },
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/eecs595/eat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLAALYTAo25M"
      },
      "source": [
        "Test if script files are located."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KVidViv0AO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3b1e45-5dce-4443-c4f7-15ff1910c1ab"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['eat-baseline.ipynb', 'eat-reference.ipynb', 'eat-tuned.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB4aTTM14sCE"
      },
      "source": [
        "Check if dataset file is located, you should see `eat_test_unlabeled.json` and `eat_train.json` in the folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lrrOs4d4lkE",
        "outputId": "1a97f48c-f67b-4f1c-d6f0-941fb4b8760b"
      },
      "source": [
        "!ls /content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/EAT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eat_test_unlabeled.json  eat_train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4WOO9tvye13"
      },
      "source": [
        "## Dependency installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m03X-0vLyLM0"
      },
      "source": [
        "import json\n",
        "import codecs\n",
        "import pandas as pd\n",
        "\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "# from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCt4UVaGNCgI"
      },
      "source": [
        "Install `sentencepiece` for `XLNetTokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmfU0rSbNCzm",
        "outputId": "0eea19e4-8ec8-4b63-d5e4-7e482d153958"
      },
      "source": [
        "!pip install sentencepiece\n",
        "\n",
        "import sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aern7KsY01YS"
      },
      "source": [
        "Install `transformers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hTWa5DGyvQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f4829a-018f-4eb1-a1f3-67601fee4bce"
      },
      "source": [
        "!pip install transformers\n",
        "# !pip install transformers==2.0.0\n",
        "\n",
        "from transformers import BertModel, RobertaModel\n",
        "from transformers import (AdamW, get_linear_schedule_with_warmup, AutoModelForQuestionAnswering,\n",
        "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
        "                          XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
        "                          GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h30zNEx1QmcC"
      },
      "source": [
        "Install `sentence-transformers` for [Sentence Bert](https://arxiv.org/abs/1908.10084) [1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33oNDf08QmzW"
      },
      "source": [
        "# !pip install -U sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzgCrnTGzNYv"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tv9jG1yzT5m"
      },
      "source": [
        "SEED = 0\n",
        "DEVICE = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def select_field(feature_list, field_name):\n",
        "    return [\n",
        "        [choice[field_name] for choice in feature.choices_features]\n",
        "        for feature in feature_list\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_model(model='all'):\n",
        "    if model == 'bert':\n",
        "        return BertConfig, BertForSequenceClassification, BertTokenizer\n",
        "    elif model == 'xlnet':\n",
        "        return XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer\n",
        "    elif model == 'roberta':\n",
        "        return RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n",
        "    elif model == 'gpt2':\n",
        "        raise NotImplemented\n",
        "        # return GPT2Config, AutoModelForQuestionAnswering, GPT2Tokenizer\n",
        "    raise NotImplemented\n",
        "\n",
        "\n",
        "def load_optimizer(args, model, train_size, learning_rate):\n",
        "    num_training_steps = train_size * args.num_train_epochs\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters, lr=learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "    return model, optimizer, scheduler\n",
        "\n",
        "\n",
        "def freeze(model, model_name):\n",
        "    if model_name == 'bert':\n",
        "        for param in model.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif model_name == 'xlnet':\n",
        "        for param in model.xlnet.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif model_name == 'roberta':\n",
        "        for param in model.roberta.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif model_name == 'gpt2':\n",
        "        for param in model.gpt2.parameters():\n",
        "            param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_accuracy(preds, labels, lengths=None, inspect=60):\n",
        "    if inspect:\n",
        "        print('\\nThe first {} predictions'.format(inspect), preds[0:inspect])\n",
        "    if lengths is None:\n",
        "        return (preds == labels).mean()\n",
        "\n",
        "    i, correct = 0, 0\n",
        "    for n in lengths:\n",
        "        pred, label = 1, 1\n",
        "        for p in preds[i:i + n]:\n",
        "            pred *= p\n",
        "        for l in labels[i:i + n]:\n",
        "            label *= l\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "        i += n\n",
        "    return correct / len(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tABNFRGMKz6Y"
      },
      "source": [
        "# Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ofGNJGHKOK7"
      },
      "source": [
        "## Task Dataset\n",
        "\n",
        "EAT (Everyday Actions in Text) is from the SLED (Situated Language and Embodied Dialogue) group created by Shane Storks. The dataset is in the form of a story of 5 sequential events represented by natural language texts $\\{t_1,t_2,\\cdots,t_5\\}$ respectively. The model aims to identify whether the story is plausible using common sense reasoning and specify at which event the story becomes implausible, if any. Such plausible inference requires the model to have a strong background knowledge and a comprehensive ability to perform common sense reasoning and causal reasoning, since whether an event is plausible in the story is high dependent on the previous events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hOJK9v-KQo2"
      },
      "source": [
        "def load_data(dataset='commonsense_qa', preview=-1):\n",
        "\n",
        "    assert dataset in {'commonsense_qa', 'conv_entail', 'eat'}\n",
        "\n",
        "    if dataset == 'commonsense_qa':\n",
        "        ds = load_dataset('commonsense_qa')\n",
        "\n",
        "        if preview > 0:\n",
        "            print('\\nLoading an example...')\n",
        "            data_tr = ds.data['train']\n",
        "            question = data_tr['question']\n",
        "            choices = data_tr['choices']\n",
        "            answerKey = data_tr['answerKey']\n",
        "            print(question[preview])\n",
        "            for label, text in zip(choices[preview]['label'], choices[preview]['text']):\n",
        "                print(label, text)\n",
        "            print('Ans:', answerKey[preview])\n",
        "\n",
        "    elif dataset == 'conv_entail':\n",
        "        dev_file = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/Conversational_Entailment/dev_set.json'\n",
        "        act_file = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/Conversational_Entailment/act_tag.json'\n",
        "        dev_set = codecs.open(dev_file, 'r', encoding='utf-8').read()\n",
        "        act_tag = codecs.open(act_file, 'r', encoding='utf-8').read()\n",
        "        ds = json.loads(dev_set), json.loads(act_tag)\n",
        "\n",
        "        if preview > 0:\n",
        "            print('Preview not yet implemented for this dataset.')\n",
        "\n",
        "    else:\n",
        "        file_name = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/EAT/eat_train.json'\n",
        "        eat = codecs.open(file_name, 'r', encoding='utf-8').read()\n",
        "        ds = json.loads(eat)\n",
        "\n",
        "        if preview > 0:\n",
        "            print('\\nLoading an example...')\n",
        "            story = ds[preview]['story']\n",
        "            label = ds[preview]['label']\n",
        "            bp = ds[preview]['breakpoint']\n",
        "            for line in story:\n",
        "                print(line)\n",
        "            print(label)\n",
        "            print(bp)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def load_data_frame(preview=True):\n",
        "    \n",
        "    file_name = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/EAT/eat_train.json'\n",
        "    df = pd.read_json(file_name)\n",
        "    \n",
        "    if preview:\n",
        "        print(df.head())\n",
        "        print(len(df))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRiu77jjKfoH"
      },
      "source": [
        "Run the following code to preview the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Er-oLRB6dmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2a1448-46a9-42bc-f3ad-18b2f8dfcf87"
      },
      "source": [
        "ds = load_data(dataset='eat', preview=5)\n",
        "print('\\nDataset size:', len(ds))\n",
        "df = load_data_frame()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading an example...\n",
            "John grabbed the ladder and put it in his truck.\n",
            "John put a drill and rope in the bucket and also put that in this truck.\n",
            "John drove the bicycle to the neighbor's house.\n",
            "John picked up a few other things and put them near the bucket in the truck.\n",
            "John drove his truck to a neighbor's house to help them repair their storm-damaged shed.\n",
            "0\n",
            "3\n",
            "\n",
            "Dataset size: 1044\n",
            "                                               story  ...       id\n",
            "0  [Tom took out the cooked meat., Tom took out t...  ...  train_0\n",
            "1  [John went to the bathroom., John decided to t...  ...  train_1\n",
            "2  [Ann drank from her water bottle., Ann decided...  ...  train_2\n",
            "3  [Tom put on his shoes., Tom packed a suitcase....  ...  train_3\n",
            "4  [Ann sat down on the couch., Ann reached for t...  ...  train_4\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "1044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Vq_hBp7PNB"
      },
      "source": [
        "Get the length of all the messages in the train set to choose appropriate a padding length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "HMBZjpF27Jyg",
        "outputId": "c8932da9-2bfa-4265-a7bd-8efa5ae2a624"
      },
      "source": [
        "df = load_data_frame(preview=False)\n",
        "df['text'] = df['story'].apply(lambda x: ' '.join(x).lower())\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(\n",
        "    df['text'], df['label'], random_state=SEED, test_size=0.15, stratify=df['label'])\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c9afdccc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIxGTEYezc1L"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS5CDydVzfAB"
      },
      "source": [
        "MAXLENGTH = 120\n",
        "\n",
        "def embed(df):\n",
        "    sent_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "    embedding = df['story'].apply(lambda x: sent_model.encode(x, convert_to_tensor=True, device=device))\n",
        "    return embedding\n",
        "\n",
        "\n",
        "class EATProcessor():\n",
        "    def __init__(self, tokenizer, args=None):\n",
        "\n",
        "        test_size = 0.15\n",
        "        if args is not None:\n",
        "            test_size = args.test_size\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.double_sep = 'roberta' in tokenizer.name_or_path\n",
        "        self.cls_token = tokenizer.cls_token\n",
        "        self.sep_token = tokenizer.sep_token\n",
        "\n",
        "        df = load_data_frame(preview=False)\n",
        "        self.train_story, self.val_story, self.train_bp, self.val_bp = train_test_split(\n",
        "            df['story'], df['breakpoint'], random_state=SEED, test_size=test_size, stratify=df['breakpoint'])\n",
        "        \n",
        "    def truncate_seq_pair(self, token_a, token_b, max_len=MAXLENGTH-3):\n",
        "        \"\"\"\n",
        "        Truncates a sequence pair in place to the maximum length.\n",
        "\n",
        "        This is a simple heuristic which will always truncate the longer sequence one token at a time.\n",
        "        This makes more sense than truncating an equal percent of tokens from each,\n",
        "        since if one sequence is very short then each token that's truncated\n",
        "        likely contains more information than a longer sequence.\n",
        "\n",
        "        However, since we'd better not to remove tokens of options and questions,\n",
        "        you can choose to use a bigger length or only pop from context\n",
        "        \"\"\"\n",
        "\n",
        "        while True:\n",
        "            total_length = len(token_a) + len(token_b)\n",
        "            if total_length <= max_len:\n",
        "                break\n",
        "            if len(token_a) > len(token_b):\n",
        "                token_a.pop()\n",
        "            else:\n",
        "                warning = 'Attention! you are removing from token_b (swag task is ok). ' \\\n",
        "                          'If you are training ARC and RACE (you are popping question + options), ' \\\n",
        "                          'you need to try to use a bigger max seq length!'\n",
        "                print(warning)\n",
        "                token_b.pop()\n",
        "\n",
        "    def tokenize_and_encode(self, prev, next, max_len=MAXLENGTH):\n",
        "        \"\"\"\n",
        "        Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "        Max is 512 if using BERT-based models, higher for longformer (2000+)\n",
        "        \"\"\"\n",
        "\n",
        "        num_seps = 1\n",
        "        extra_tokens = 3\n",
        "        if self.double_sep:\n",
        "            num_seps += 1\n",
        "            extra_tokens += 1\n",
        "\n",
        "        token_a = self.tokenizer.tokenize(prev)\n",
        "        token_b = self.tokenizer.tokenize(next)\n",
        "        self.truncate_seq_pair(token_a, token_b, max_len-extra_tokens)\n",
        "\n",
        "        token_a = [self.cls_token] + token_a + ([self.sep_token] * num_seps)\n",
        "        token_type_ids = [0] * len(token_a)\n",
        "        token_b = token_b + [self.sep_token]\n",
        "        token_type_ids += [1] * len(token_b)\n",
        "        tokens = token_a + token_b\n",
        "\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        mask = [1] * len(input_ids)\n",
        "\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        input_ids += ([0] * padding_length)\n",
        "        mask += ([0] * padding_length)\n",
        "        token_type_ids += ([0] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_len\n",
        "        assert len(token_type_ids) == max_len\n",
        "        assert len(mask) == max_len\n",
        "            \n",
        "        return input_ids, token_type_ids, mask\n",
        "\n",
        "    def load_features(self, split='train'):\n",
        "\n",
        "        if split == 'train':\n",
        "            stories, bps = self.train_story, self.train_bp\n",
        "        else:\n",
        "            stories, bps = self.val_story, self.val_bp\n",
        "\n",
        "        labels, seqs, segs, masks, lengths = [], [], [], [], []\n",
        "        for story, breakpoint in zip(stories, bps):\n",
        "\n",
        "            if breakpoint == -1:\n",
        "                lengths.append(len(story)-1)\n",
        "                for i in range(1, len(story)):\n",
        "                    prev = ' '.join(story[:i])\n",
        "                    next = story[i]\n",
        "                    seq, seg, mask = self.tokenize_and_encode(prev, next)\n",
        "                    seqs.append(seq)\n",
        "                    segs.append(seg)\n",
        "                    masks.append(mask)\n",
        "                    labels.append(1)\n",
        "            else:\n",
        "                lengths.append(breakpoint)\n",
        "                for i in range(1, breakpoint):\n",
        "                    prev = ' '.join(story[:i])\n",
        "                    next = story[i]\n",
        "                    seq, seg, mask = self.tokenize_and_encode(prev, next)\n",
        "                    seqs.append(seq)\n",
        "                    segs.append(seg)\n",
        "                    masks.append(mask)\n",
        "                    labels.append(1)\n",
        "                prev = ' '.join(story[:breakpoint])\n",
        "                next = story[breakpoint]\n",
        "                seq, seg, mask = self.tokenize_and_encode(prev, next)\n",
        "                seqs.append(seq)\n",
        "                segs.append(seg)\n",
        "                masks.append(mask)\n",
        "                labels.append(0)\n",
        "\n",
        "        labels = torch.tensor(labels)\n",
        "        seqs = torch.tensor(seqs)\n",
        "        segs = torch.tensor(segs)\n",
        "        masks = torch.tensor(masks)\n",
        "\n",
        "        dataset = TensorDataset(seqs, masks, segs, labels)\n",
        "\n",
        "        return dataset, lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7ORg7HJqjWO",
        "outputId": "d0016caa-4b42-4fca-d68e-eef204c45292"
      },
      "source": [
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenizer_rbt = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "processor_tmp = EATProcessor(tokenizer_bert)\n",
        "prev = 'John grabbed the ladder and put it in his truck.'\n",
        "next = 'John put a drill and rope in the bucket and also put that in this truck.'\n",
        "seq, seg, mask = processor_tmp.tokenize_and_encode(prev, next)\n",
        "print(prev, next)\n",
        "print(seq)\n",
        "print(seg)\n",
        "print(mask)\n",
        "print(len(seq), len(seg), len(mask))\n",
        "\n",
        "processor_tmp = EATProcessor(tokenizer_rbt)\n",
        "prev = 'John grabbed the ladder and put it in his truck.'\n",
        "next = 'John put a drill and rope in the bucket and also put that in this truck.'\n",
        "seq, seg, mask = processor_tmp.tokenize_and_encode(prev, next)\n",
        "print(prev, next)\n",
        "print(seq)\n",
        "print(seg)\n",
        "print(mask)\n",
        "print(len(seq), len(seg), len(mask))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "John grabbed the ladder and put it in his truck. John put a drill and rope in the bucket and also put that in this truck.\n",
            "[101, 2198, 4046, 1996, 10535, 1998, 2404, 2009, 1999, 2010, 4744, 1012, 102, 2198, 2404, 1037, 12913, 1998, 8164, 1999, 1996, 13610, 1998, 2036, 2404, 2008, 1999, 2023, 4744, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "120 120 120\n",
            "John grabbed the ladder and put it in his truck. John put a drill and rope in the bucket and also put that in this truck.\n",
            "[0, 10567, 7249, 5, 16348, 8, 342, 24, 11, 39, 2484, 4, 2, 2, 10567, 342, 10, 9888, 8, 17434, 11, 5, 14792, 8, 67, 342, 14, 11, 42, 2484, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "120 120 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy237KUt4bfZ"
      },
      "source": [
        "## Knowledge Dataset\n",
        "\n",
        "Situations with Adversarial Generations ([SWAG](https://rowanzellers.com/swag/)) from Zellers et al. is a benchmark dataset of about 113,000 beginnings of small texts each with four possible endings [2]. Given the context each text provides, systems decide which of the four endings is most plausible in\n",
        "a task referred to as commonsense NLI.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApoNGj8A4gYY",
        "outputId": "088e6aca-b95c-49b2-9f09-1dd7aa2739e5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv\n",
        "!mkdir swag/\n",
        "!mv train.csv swag/train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-13 22:58:03--  https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28243333 (27M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  26.93M  91.9MB/s    in 0.3s    \n",
            "\n",
            "2020-12-13 22:58:04 (91.9 MB/s) - ‘train.csv’ saved [28243333/28243333]\n",
            "\n",
            "mkdir: cannot create directory ‘swag/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScP8KsAt5Mf0"
      },
      "source": [
        "def load_swag(preview=False):\n",
        "    \n",
        "    file_name = 'swag/train.csv'\n",
        "    df = pd.read_csv(file_name)\n",
        "\n",
        "    if preview:\n",
        "        print(len(df))\n",
        "        row = df.iloc[[preview]]\n",
        "        print(row['sent1'][1])\n",
        "        print(row['sent2'][1], '...')\n",
        "        print(row['ending0'][1])\n",
        "        print(row['ending1'][1])\n",
        "        print(row['ending2'][1])\n",
        "        print(row['ending3'][1])\n",
        "        print(row['label'][1])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdAjdixUfq9b",
        "outputId": "95b743b0-c8ab-485d-c60e-ca9a6009c05c"
      },
      "source": [
        "df = load_swag(preview=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73546\n",
            "A drum line passes by walking down the street playing their instruments.\n",
            "Members of the procession ...\n",
            "are playing ping pong and celebrating one left each in quick.\n",
            "wait slowly towards the cadets.\n",
            "continues to play as well along the crowd along with the band being interviewed.\n",
            "continue to play marching, interspersed.\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXZR0GqLb0n3"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxw1c_kc5pWY"
      },
      "source": [
        "class SWAGProcessor():\n",
        "    \"\"\"\n",
        "    Load the processed SWAG dataset\n",
        "    max(size) = 73546\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, args=None):\n",
        "        self.test_size = 0.15\n",
        "        if args is not None:\n",
        "            self.test_size = args.test_size\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.double_sep = 'roberta' in tokenizer.name_or_path\n",
        "        self.cls_token = tokenizer.cls_token\n",
        "        self.sep_token = tokenizer.sep_token\n",
        "\n",
        "        self.df = load_swag()\n",
        "    \n",
        "    def truncate_seq_pair(self, token_a, token_b, max_len=MAXLENGTH-3):\n",
        "        \"\"\"\n",
        "        Truncates a sequence pair in place to the maximum length.\n",
        "\n",
        "        This is a simple heuristic which will always truncate the longer sequence one token at a time.\n",
        "        This makes more sense than truncating an equal percent of tokens from each,\n",
        "        since if one sequence is very short then each token that's truncated\n",
        "        likely contains more information than a longer sequence.\n",
        "\n",
        "        However, since we'd better not to remove tokens of options and questions,\n",
        "        you can choose to use a bigger length or only pop from context\n",
        "        \"\"\"\n",
        "\n",
        "        while True:\n",
        "            total_length = len(token_a) + len(token_b)\n",
        "            if total_length <= max_len:\n",
        "                break\n",
        "            if len(token_a) > len(token_b):\n",
        "                token_a.pop()\n",
        "            else:\n",
        "                warning = 'Attention! you are removing from token_b (swag task is ok). ' \\\n",
        "                          'If you are training ARC and RACE (you are popping question + options), ' \\\n",
        "                          'you need to try to use a bigger max seq length!'\n",
        "                print(warning)\n",
        "                token_b.pop()\n",
        "\n",
        "    def tokenize_and_encode(self, prev, next, max_len=MAXLENGTH):\n",
        "        \"\"\"\n",
        "        Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "        Max is 512 if using BERT-based models, higher for longformer (2000+)\n",
        "        \"\"\"\n",
        "\n",
        "        num_seps = 1\n",
        "        extra_tokens = 3\n",
        "        if self.double_sep:\n",
        "            num_seps += 1\n",
        "            extra_tokens += 1\n",
        "\n",
        "        token_a = self.tokenizer.tokenize(prev)\n",
        "        token_b = self.tokenizer.tokenize(next)\n",
        "        self.truncate_seq_pair(token_a, token_b, max_len-extra_tokens)\n",
        "\n",
        "        token_a = [self.cls_token] + token_a + ([self.sep_token] * num_seps)  \n",
        "        token_type_ids = [0] * len(token_a)\n",
        "        token_b = token_b + [self.sep_token]\n",
        "        token_type_ids += [1] * len(token_b)\n",
        "        tokens = token_a + token_b\n",
        "\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        mask = [1] * len(input_ids)\n",
        "\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        input_ids += ([0] * padding_length)\n",
        "        mask += ([0] * padding_length)\n",
        "        token_type_ids += ([0] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_len\n",
        "        assert len(token_type_ids) == max_len\n",
        "        assert len(mask) == max_len\n",
        "            \n",
        "        return input_ids, token_type_ids, mask\n",
        "\n",
        "    def load_features(self, size=73546, balanced=False):\n",
        "\n",
        "        print(\"Creating features from dataset...\")\n",
        "        labels, seqs, segs, masks = [], [], [], []\n",
        "        for row in df.iterrows():\n",
        "\n",
        "            label = row[1]['label']\n",
        "            start = row[1]['sent1']\n",
        "            sent = row[1]['sent2']\n",
        "\n",
        "            if balanced:\n",
        "                ending = row[1]['ending{}'.format(label)]\n",
        "                next = ' '.join([sent, ending]).lower()\n",
        "                seq, seg, mask = self.tokenize_and_encode(start, next)\n",
        "                seqs.append(seq)\n",
        "                segs.append(seg)\n",
        "                masks.append(mask)\n",
        "                labels.append(1)\n",
        "                ending = row[1]['ending{}'.format((label+1)%4)]\n",
        "                next = ' '.join([sent, ending]).lower()\n",
        "                seq, seg, mask = self.tokenize_and_encode(start, next)\n",
        "                seqs.append(seq)\n",
        "                segs.append(seg)\n",
        "                masks.append(mask)\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                for i in range(3):\n",
        "                    ending = row[1]['ending{}'.format(i)]\n",
        "                    next = ' '.join([sent, ending]).lower()\n",
        "\n",
        "                    seq, seg, mask = self.tokenize_and_encode(start, next)\n",
        "                    seqs.append(seq)\n",
        "                    segs.append(seg)\n",
        "                    masks.append(mask)\n",
        "                    if i == label:\n",
        "                        labels.append(1)\n",
        "                    else:\n",
        "                        labels.append(0)\n",
        "                        \n",
        "            size -= 1\n",
        "            if size == 0:\n",
        "                break\n",
        "\n",
        "        train_seq, val_seq, train_seg, val_seg, train_mask, val_mask, train_labels, val_labels = train_test_split(\n",
        "            seqs, segs, masks, labels, random_state=SEED, test_size=self.test_size, stratify=labels)\n",
        "        \n",
        "        a = torch.tensor(train_seq)\n",
        "        train_seq = torch.tensor(train_seq)\n",
        "        val_seq = torch.tensor(val_seq)\n",
        "        train_seg = torch.tensor(train_seg)\n",
        "        val_seg = torch.tensor(val_seg)\n",
        "        train_mask = torch.tensor(train_mask)\n",
        "        val_mask = torch.tensor(val_mask)\n",
        "        train_labels = torch.tensor(train_labels)\n",
        "        train_y = torch.tensor(train_labels)\n",
        "        val_y = torch.tensor(val_labels)\n",
        "\n",
        "        train_data = TensorDataset(train_seq, train_mask, train_seg, train_y)\n",
        "        val_data = TensorDataset(val_seq, val_mask, val_seg, val_y)\n",
        "\n",
        "        return train_data, val_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XtyXWPe3lKW",
        "outputId": "6b886114-5da9-4791-883f-890e1ccb127c"
      },
      "source": [
        "processor_tmp_2 = SWAGProcessor(tokenizer_rbt)\n",
        "\n",
        "prev = 'A drum line passes by walking down the street playing their instruments.'\n",
        "next = 'Members of the procession are playing ping pong and celebrating one left each in quick.'\n",
        "seq, seg, mask = processor_tmp_2.tokenize_and_encode(prev, next)\n",
        "print(prev, next)\n",
        "print(seq)\n",
        "print(seg)\n",
        "print(mask)\n",
        "print(len(seq), len(seg), len(mask))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A drum line passes by walking down the street playing their instruments. Members of the procession are playing ping pong and celebrating one left each in quick.\n",
            "[0, 250, 12638, 516, 3974, 30, 3051, 159, 5, 2014, 816, 49, 9571, 4, 2, 2, 31339, 9, 5, 21191, 32, 816, 36477, 181, 1657, 8, 6146, 65, 314, 349, 11, 2119, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "120 120 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92jFJkifK6Cf"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ0BLRx-zmep"
      },
      "source": [
        "## Pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJKT-_9boL-c"
      },
      "source": [
        "Self-designed model for flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew2T52OZq0tD"
      },
      "source": [
        "class MyBertClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=1, dropout_rate=0.2, freeze=False):\n",
        "        super(MyBertClassifier, self).__init__()\n",
        "        \n",
        "        if num_labels == 1:\n",
        "            self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            self.loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(self.num_embed(model_name), 64)\n",
        "        self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(64, num_labels)\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def num_embed(self, model_name):\n",
        "        embedding_size = {\n",
        "            'bert-base-uncased': 768,\n",
        "            'bert-large-uncased': 1024\n",
        "        }\n",
        "        return embedding_size[model_name]\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
        "\n",
        "        out = self.bert(input_ids=input_ids, \n",
        "                        attention_mask=attention_mask, \n",
        "                        token_type_ids=token_type_ids)\n",
        "        x = out.pooler_output\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # x = np.argmax(x, axis=1)\n",
        "            loss = self.loss_fct(x.flatten(), labels.float())\n",
        "        \n",
        "        return loss, x\n",
        "\n",
        "\n",
        "class MyRobertaClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, model_name='roberta-base', num_labels=2, dropout_rate=0.2, freeze=False):\n",
        "        super(MyRobertaClassifier, self).__init__()\n",
        "        \n",
        "        if num_labels == 1:\n",
        "            self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            self.loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(self.num_embed(model_name), 64)\n",
        "        self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(64, num_labels)\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.roberta.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def num_embed(self, model_name):\n",
        "        embedding_size = {\n",
        "            'roberta-base': 768,\n",
        "            'roberta-large': 1024,\n",
        "        }\n",
        "        return embedding_size[model_name]\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
        "\n",
        "        out = self.roberta(input_ids=input_ids, \n",
        "                           attention_mask=attention_mask, \n",
        "                           token_type_ids=None)\n",
        "        x = out.pooler_output\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # x = np.argmax(x, axis=1)\n",
        "            loss = self.loss_fct(x.flatten(), labels.float())\n",
        "        \n",
        "        return loss, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWb1PKeZztGS"
      },
      "source": [
        "## Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Sn_8byoIR2"
      },
      "source": [
        "Training and evaluation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3xQdSF8zl1q"
      },
      "source": [
        "def fine_tune(args, model, tokenizer):\n",
        "\n",
        "    processor = SWAGProcessor(tokenizer, args)\n",
        "    dataset_tr, dataset_val = processor.load_features(size=args.pretrain_size)\n",
        "\n",
        "    print('\\n Loading training dataset')\n",
        "    sampler_tr = RandomSampler(dataset_tr)\n",
        "    dataloader_tr = DataLoader(dataset_tr, sampler=sampler_tr, batch_size=args.batch_size)\n",
        "\n",
        "    print('\\n Loading validation dataset')\n",
        "    sampler_val = SequentialSampler(dataset_val)\n",
        "    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n",
        "\n",
        "    model, optimizer, scheduler = load_optimizer(args, model, len(dataloader_tr), args.learning_rate_tune)\n",
        "\n",
        "    tr_loss = 0.00\n",
        "    num_steps = 0\n",
        "\n",
        "    model.train()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=False, leave=True, position=1)\n",
        "\n",
        "    for _ in train_iterator:\n",
        "        disable = False\n",
        "        if len(dataloader_tr) > 1000:\n",
        "            disable = True\n",
        "        epoch_iterator = tqdm(dataloader_tr, desc=\"Iteration\", disable=disable, leave=True, position=1)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            batch = tuple(b.to(args.device) for b in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n",
        "                      'labels': batch[3]}\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            num_steps += 1\n",
        "\n",
        "            if args.logging_steps_tune > 0 and num_steps % args.logging_steps_tune == 0:\n",
        "                results = evaluate(args, model, dataloader_val)\n",
        "                print(\"\\n val acc: {}, val loss: {}\"\n",
        "                      .format(str(results['val_acc']), str(results['val_loss'])))\n",
        "\n",
        "    loss = tr_loss / num_steps\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(args, model, tokenizer):\n",
        "    train_epoch = 1\n",
        "    \n",
        "    processor = EATProcessor(tokenizer, args)\n",
        "    dataset_tr, lengths_tr = processor.load_features(split='train')\n",
        "    dataset_val, lengths_val = processor.load_features(split='val')\n",
        "\n",
        "    print('\\n Loading training dataset')\n",
        "    sampler_tr = RandomSampler(dataset_tr)\n",
        "    dataloader_tr = DataLoader(dataset_tr, sampler=sampler_tr, batch_size=args.batch_size)\n",
        "\n",
        "    print('\\n Loading validation dataset')\n",
        "    sampler_val = SequentialSampler(dataset_val)\n",
        "    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n",
        "\n",
        "    num_steps = 0\n",
        "    best_steps = 0\n",
        "    tr_loss = 0.0\n",
        "    best_val_acc, best_val_loss = 0.0, 99999999999.0\n",
        "    best_model = None\n",
        "\n",
        "    _, optimizer, scheduler = load_optimizer(args, model, len(dataloader_tr), args.learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    train_iterator = trange(int(train_epoch), desc=\"Epoch\", disable=False, leave=True, position=1)\n",
        "\n",
        "    for _ in train_iterator:\n",
        "\n",
        "        epoch_iterator = tqdm(dataloader_tr, desc=\"Iteration\", disable=False, leave=True, position=1)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            batch = tuple(b.to(args.device) for b in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n",
        "                      'labels': batch[3]}\n",
        "                      \n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            num_steps += 1\n",
        "\n",
        "            if args.logging_steps > 0 and num_steps % args.logging_steps == 0:\n",
        "                results = evaluate(args, model, dataloader_val, lengths_val)\n",
        "                print(\"\\n val acc: {}, val loss: {}\"\n",
        "                      .format(str(results['val_acc']), str(results['val_loss'])))\n",
        "                if results[\"val_loss\"] < best_val_loss:\n",
        "                    best_val_acc, best_val_loss = results[\"val_acc\"], results[\"val_loss\"]\n",
        "                    best_steps = num_steps\n",
        "                    best_model = deepcopy(model)\n",
        "\n",
        "    loss = tr_loss / num_steps\n",
        "    results = evaluate(args, model, dataloader_tr, lengths_tr)\n",
        "    print(\"\\n Final train acc: {}, Final train loss: {}\".format(str(results['val_acc']), str(results['val_loss'])))\n",
        "    print(\"\\n Best val acc: {}, Best val loss: {}\".format(best_val_acc, best_val_loss))\n",
        "\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def evaluate(args, model, dataloader, lengths=None):\n",
        "\n",
        "    val_loss = 0.0\n",
        "    num_steps = 0\n",
        "    preds, labels = None, None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Validation\", disable=True, leave=True, position=1):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n",
        "                      'labels': batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            loss, logits = outputs[:2]\n",
        " \n",
        "            val_loss += loss.mean().item()\n",
        "\n",
        "        num_steps += 1\n",
        "\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            labels = inputs['labels'].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    loss = val_loss / num_steps\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    acc = get_accuracy(preds, labels, lengths)\n",
        "    result = {\"val_acc\": acc, \"val_loss\": loss}\n",
        "    results.update(result)\n",
        "    \n",
        "    # classification_report(labels, preds)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def test(args, tokenizer, model):\n",
        "\n",
        "    processor = EATProcessor(tokenizer, args)\n",
        "    dataset_val, lengths_val = processor.load_features(split='val')\n",
        "    sampler_val = SequentialSampler(dataset_val)\n",
        "    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n",
        "    results = evaluate(args, model, dataloader_val, lengths_val)\n",
        "    print(\"\\n Final val acc: {}, val loss: {}\".format(str(results['val_acc']), str(results['val_loss'])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyv6xGVDhfEZ"
      },
      "source": [
        "def main(args):\n",
        "\n",
        "    print('Using device', args.device)\n",
        "    print('Using model', args.model_type)\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    num_labels = 2\n",
        "\n",
        "    config_class, model_class, tokenizer_class = load_model(args.model_type)\n",
        "    config = config_class.from_pretrained(\n",
        "        args.config_name if args.config_name else args.model_name,\n",
        "        num_labels=num_labels, finetuning_task=args.task_name)\n",
        "    tokenizer = tokenizer_class.from_pretrained(\n",
        "        args.tokenizer_name if args.tokenizer_name else args.model_name,\n",
        "        do_lower_case=True)\n",
        "    \n",
        "    model = model_class.from_pretrained(args.model_name, config=config,\n",
        "                                        from_tf=bool('.ckpt' in args.model_name))\n",
        "    # model = MyBertClassifier(model_name=args.config_name, num_labels=1)\n",
        "    \n",
        "    model.to(args.device)\n",
        "\n",
        "    print('\\nTuning...')\n",
        "    model = fine_tune(args, model, tokenizer)\n",
        "    # model = freeze(model, args.model_name)\n",
        "\n",
        "    print('\\nTraining...')\n",
        "    best_model = train(args, model, tokenizer)\n",
        "\n",
        "    print('\\nTesting...')\n",
        "    test(args, tokenizer, best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGOc8j4Hz7OC"
      },
      "source": [
        "The default model is based on `bert`, and\n",
        "```\n",
        "parser.add_argument(\"--model_name\", type=str, default='bert-base-uncased',\n",
        "                    help=\"Path to pre-trained model or shortcut name. See https://huggingface.co/models\")\n",
        "```\n",
        "\n",
        "This would leads to\n",
        "\n",
        "```\n",
        "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
        "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
        "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Using custom data configuration default\n",
        "```\n",
        "\n",
        "Should check https://huggingface.co/models for other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfotIM0w1oX-"
      },
      "source": [
        "Setup parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_F0_crozIjr"
      },
      "source": [
        "def run(model_type='bert',\n",
        "        model_name='bert-base-uncased',\n",
        "        task_name=None,\n",
        "        batch_size=64,\n",
        "        lr=1e-5,\n",
        "        lr_tune=1e-5,\n",
        "        epochs=1,\n",
        "        pretrain_size=1000,\n",
        "        logging_steps=50,\n",
        "        logging_steps_tune=200):\n",
        "  \n",
        "    parser = argparse.ArgumentParser(description=\"Common sense question answering\")\n",
        "    parser.add_argument(\"--model_type\", type=str, default=model_type,\n",
        "                        help=\"Model: <str> [ bert | xlnet | roberta | gpt2 ]\")\n",
        "    parser.add_argument(\"--task_name\", default=task_name, type=str, required=False,\n",
        "                        help=\"The name of the task to train: <str> [ commonqa ]\")\n",
        "    parser.add_argument(\"--model_name\", type=str,\n",
        "                        default=model_name,\n",
        "                        help=\"Path to pre-trained model or shortcut name.\"\n",
        "                              \"See https://huggingface.co/models\")\n",
        "    parser.add_argument(\"--config_name\", type=str,\n",
        "                        default=model_name,\n",
        "                        help=\"Pre-trained config name or path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=model_name, type=str,\n",
        "                        help=\"Pre-trained tokenizer name or path if not the same as model_name\")\n",
        "\n",
        "    parser.add_argument(\"--max_seq_length\", default=100, type=int,\n",
        "                        help=\"The maximum total input sequence length after tokenization. \"\n",
        "                                \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
        "    parser.add_argument(\"--batch_size\", default=batch_size, type=int,\n",
        "                        help=\"Batch size for training.\")\n",
        "\n",
        "    parser.add_argument(\"--learning_rate\", default=lr, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--learning_rate_tune\", default=lr_tune, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--test_size\", default=0.2, type=float,\n",
        "                        help=\"The ratio of size of validation set\")\n",
        "    parser.add_argument(\"--pretrain_size\", default=pretrain_size, type=float,\n",
        "                        help=\"The ratio of size of validation set\")\n",
        "        \n",
        "    parser.add_argument(\"--num_train_epochs\", default=epochs, type=int,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=epochs//6, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument('--logging_steps', type=int, default=logging_steps,\n",
        "                        help=\"Log every n updates steps.\")\n",
        "    parser.add_argument('--logging_steps_tune', type=int, default=logging_steps_tune,\n",
        "                        help=\"Log every n updates steps.\")\n",
        "\n",
        "    parser.add_argument('--fp16', type=bool, default=True,\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                              \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed: <int>\")\n",
        "    parser.add_argument(\"--device\", default=DEVICE)\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmE312LRWvvR"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q2ViC5Qzig3"
      },
      "source": [
        "### Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2blaf7izt3j"
      },
      "source": [
        "model_type = 'bert'\n",
        "model_name = 'bert-base-uncased'\n",
        "task_name = None\n",
        "batch_size = 16\n",
        "epochs = 6\n",
        "lr_tune = 5e-5\n",
        "lr = 5e-6\n",
        "# pretrain_size = 73546\n",
        "pretrain_size = 50000\n",
        "logging_steps = 20\n",
        "logging_steps_tune = 200\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SrvSbCkEuUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827dd16b-6f0c-46fb-e71e-e917ad45e5ea"
      },
      "source": [
        "model_type = 'bert'\n",
        "model_name = 'bert-large-uncased'\n",
        "task_name = None\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "lr_tune = 5e-5\n",
        "lr = 1e-5\n",
        "pretrain_size = 1000\n",
        "logging_steps = 20\n",
        "logging_steps_tune = 20\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "Using model bert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tuning...\n",
            "Creating features from dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 0/150 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loading training dataset\n",
            "\n",
            " Loading validation dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   1%|          | 1/150 [00:01<02:58,  1.20s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 2/150 [00:02<02:56,  1.19s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 3/150 [00:03<02:56,  1.20s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 4/150 [00:04<02:54,  1.19s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 5/150 [00:05<02:52,  1.19s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 6/150 [00:07<02:51,  1.19s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 7/150 [00:08<02:50,  1.19s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 8/150 [00:09<02:49,  1.19s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 9/150 [00:10<02:48,  1.19s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 10/150 [00:11<02:48,  1.20s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 11/150 [00:13<02:47,  1.20s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 12/150 [00:14<02:46,  1.21s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 13/150 [00:15<02:46,  1.21s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 14/150 [00:16<02:45,  1.22s/it]\u001b[A\n",
            "Iteration:  10%|█         | 15/150 [00:18<02:45,  1.23s/it]\u001b[A\n",
            "Iteration:  11%|█         | 16/150 [00:19<02:44,  1.23s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 17/150 [00:20<02:43,  1.23s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 18/150 [00:21<02:43,  1.24s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 19/150 [00:23<02:42,  1.24s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 20/150 [00:41<13:36,  6.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.7533333333333333, val loss: 0.5568474374319378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  14%|█▍        | 21/150 [00:42<10:16,  4.78s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 22/150 [00:43<07:56,  3.72s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 23/150 [00:44<06:18,  2.98s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 24/150 [00:46<05:10,  2.46s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 25/150 [00:47<04:22,  2.10s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 26/150 [00:48<03:48,  1.84s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 27/150 [00:49<03:24,  1.66s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 28/150 [00:51<03:07,  1.54s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 29/150 [00:52<02:54,  1.45s/it]\u001b[A\n",
            "Iteration:  20%|██        | 30/150 [00:53<02:46,  1.38s/it]\u001b[A\n",
            "Iteration:  21%|██        | 31/150 [00:54<02:39,  1.34s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 32/150 [00:56<02:33,  1.30s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 33/150 [00:57<02:29,  1.28s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 34/150 [00:58<02:26,  1.26s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 35/150 [00:59<02:23,  1.25s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 36/150 [01:00<02:21,  1.24s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 37/150 [01:02<02:19,  1.23s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 38/150 [01:03<02:17,  1.23s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 39/150 [01:04<02:15,  1.22s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 40/150 [01:21<10:41,  5.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.7533333333333333, val loss: 0.5193860609280435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  27%|██▋       | 41/150 [01:22<08:04,  4.44s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 42/150 [01:23<06:15,  3.48s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 43/150 [01:24<04:59,  2.80s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 44/150 [01:25<04:06,  2.33s/it]\u001b[A\n",
            "Iteration:  30%|███       | 45/150 [01:27<03:29,  1.99s/it]\u001b[A\n",
            "Iteration:  31%|███       | 46/150 [01:28<03:02,  1.76s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 47/150 [01:29<02:44,  1.60s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 48/150 [01:30<02:31,  1.48s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 49/150 [01:32<02:21,  1.41s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 50/150 [01:33<02:15,  1.35s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 51/150 [01:34<02:09,  1.31s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 52/150 [01:35<02:05,  1.28s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 53/150 [01:36<02:02,  1.27s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 54/150 [01:38<02:00,  1.25s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 55/150 [01:39<01:58,  1.24s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 56/150 [01:40<01:56,  1.24s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 57/150 [01:41<01:55,  1.24s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 58/150 [01:43<01:54,  1.24s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 59/150 [01:44<01:52,  1.24s/it]\u001b[A\n",
            "Iteration:  40%|████      | 60/150 [02:01<09:01,  6.01s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "\n",
            " val acc: 0.7533333333333333, val loss: 0.5125216653472499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  41%|████      | 61/150 [02:02<06:47,  4.58s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 62/150 [02:03<05:14,  3.57s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 63/150 [02:05<04:09,  2.87s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 64/150 [02:06<03:25,  2.38s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 65/150 [02:07<02:53,  2.04s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 66/150 [02:08<02:30,  1.79s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 67/150 [02:10<02:14,  1.62s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 68/150 [02:11<02:03,  1.50s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 69/150 [02:12<01:55,  1.42s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 70/150 [02:13<01:48,  1.36s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 71/150 [02:15<01:44,  1.32s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 72/150 [02:16<01:40,  1.29s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 73/150 [02:17<01:37,  1.27s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 74/150 [02:18<01:35,  1.25s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 75/150 [02:19<01:33,  1.25s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 76/150 [02:21<01:31,  1.24s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 77/150 [02:22<01:29,  1.23s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 78/150 [02:23<01:28,  1.23s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 79/150 [02:24<01:27,  1.23s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 80/150 [02:41<06:55,  5.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.7533333333333333, val loss: 0.4998805687615746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  54%|█████▍    | 81/150 [02:42<05:11,  4.52s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 82/150 [02:44<03:59,  3.53s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 83/150 [02:45<03:09,  2.84s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 84/150 [02:46<02:35,  2.35s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 85/150 [02:47<02:10,  2.01s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 86/150 [02:49<01:53,  1.78s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 87/150 [02:50<01:41,  1.61s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 88/150 [02:51<01:32,  1.50s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 89/150 [02:52<01:26,  1.42s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 90/150 [02:53<01:21,  1.36s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 91/150 [02:55<01:17,  1.32s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 92/150 [02:56<01:14,  1.29s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 93/150 [02:57<01:12,  1.27s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 94/150 [02:58<01:10,  1.26s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 95/150 [03:00<01:08,  1.25s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 96/150 [03:01<01:07,  1.24s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 97/150 [03:02<01:05,  1.24s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 98/150 [03:03<01:04,  1.23s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 99/150 [03:04<01:02,  1.23s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 100/150 [03:21<04:58,  5.97s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.7566666666666667, val loss: 0.4823375281534697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  67%|██████▋   | 101/150 [03:23<03:42,  4.54s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 102/150 [03:24<02:50,  3.55s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 103/150 [03:25<02:14,  2.85s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 104/150 [03:26<01:48,  2.37s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 105/150 [03:28<01:31,  2.03s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 106/150 [03:29<01:18,  1.79s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 107/150 [03:30<01:09,  1.62s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 108/150 [03:31<01:03,  1.50s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 109/150 [03:33<00:58,  1.42s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 110/150 [03:34<00:54,  1.36s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 111/150 [03:35<00:51,  1.32s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 112/150 [03:36<00:49,  1.30s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 113/150 [03:37<00:47,  1.28s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 114/150 [03:39<00:45,  1.26s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 115/150 [03:40<00:43,  1.25s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 116/150 [03:41<00:42,  1.24s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 117/150 [03:42<00:40,  1.24s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 118/150 [03:44<00:39,  1.23s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 119/150 [03:45<00:38,  1.23s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 120/150 [04:02<02:59,  5.97s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "\n",
            " val acc: 0.7666666666666667, val loss: 0.475768318301753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  81%|████████  | 121/150 [04:03<02:11,  4.55s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 122/150 [04:04<01:39,  3.56s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 123/150 [04:06<01:17,  2.86s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 124/150 [04:07<01:01,  2.37s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 125/150 [04:08<00:50,  2.03s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 126/150 [04:09<00:42,  1.79s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 127/150 [04:10<00:37,  1.62s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 128/150 [04:12<00:33,  1.50s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 129/150 [04:13<00:29,  1.42s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 130/150 [04:14<00:27,  1.36s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 131/150 [04:15<00:25,  1.32s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 132/150 [04:17<00:23,  1.29s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 133/150 [04:18<00:21,  1.28s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 134/150 [04:19<00:20,  1.27s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 135/150 [04:20<00:18,  1.25s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 136/150 [04:22<00:17,  1.24s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 137/150 [04:23<00:16,  1.24s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 138/150 [04:24<00:14,  1.24s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 139/150 [04:25<00:13,  1.24s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 140/150 [04:42<00:59,  5.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
            "\n",
            " val acc: 0.775, val loss: 0.47159328821458313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  94%|█████████▍| 141/150 [04:43<00:40,  4.53s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 142/150 [04:45<00:28,  3.54s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 143/150 [04:46<00:19,  2.85s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 144/150 [04:47<00:14,  2.36s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 145/150 [04:48<00:10,  2.02s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 146/150 [04:50<00:07,  1.78s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 147/150 [04:51<00:04,  1.61s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 148/150 [04:52<00:02,  1.50s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 149/150 [04:53<00:01,  1.42s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 150/150 [04:54<00:00,  1.97s/it]\n",
            "\n",
            "Epoch: 100%|██████████| 1/1 [04:54<00:00, 294.97s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 0/188 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loading training dataset\n",
            "\n",
            " Loading validation dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   1%|          | 1/188 [00:01<03:52,  1.24s/it]\u001b[A\n",
            "Iteration:   1%|          | 2/188 [00:02<03:50,  1.24s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 3/188 [00:03<03:47,  1.23s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 4/188 [00:04<03:46,  1.23s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 5/188 [00:06<03:44,  1.23s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 6/188 [00:07<03:43,  1.23s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 7/188 [00:08<03:41,  1.22s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 8/188 [00:09<03:40,  1.23s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 9/188 [00:11<03:39,  1.22s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 10/188 [00:12<03:38,  1.23s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 11/188 [00:13<03:37,  1.23s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 12/188 [00:14<03:35,  1.23s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 13/188 [00:15<03:34,  1.23s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 14/188 [00:17<03:33,  1.23s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 15/188 [00:18<03:32,  1.23s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 16/188 [00:19<03:31,  1.23s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 17/188 [00:20<03:30,  1.23s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 18/188 [00:22<03:29,  1.23s/it]\u001b[A\n",
            "Iteration:  10%|█         | 19/188 [00:23<03:27,  1.23s/it]\u001b[A\n",
            "Iteration:  11%|█         | 20/188 [00:44<20:07,  7.19s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.4036301057389442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|█         | 21/188 [00:45<15:01,  5.40s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 22/188 [00:46<11:28,  4.15s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 23/188 [00:48<08:59,  3.27s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 24/188 [00:49<07:15,  2.66s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 25/188 [00:50<06:03,  2.23s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 26/188 [00:51<05:12,  1.93s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 27/188 [00:52<04:36,  1.72s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 28/188 [00:54<04:11,  1.57s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 29/188 [00:55<03:53,  1.47s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 30/188 [00:56<03:40,  1.40s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 31/188 [00:57<03:31,  1.35s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 32/188 [00:59<03:24,  1.31s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 33/188 [01:00<03:18,  1.28s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 34/188 [01:01<03:13,  1.26s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 35/188 [01:02<03:11,  1.25s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 36/188 [01:03<03:08,  1.24s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 37/188 [01:05<03:06,  1.23s/it]\u001b[A\n",
            "Iteration:  20%|██        | 38/188 [01:06<03:04,  1.23s/it]\u001b[A\n",
            "Iteration:  21%|██        | 39/188 [01:07<03:02,  1.22s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 40/188 [01:28<17:29,  7.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.3880135033358919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  22%|██▏       | 41/188 [01:29<13:02,  5.33s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 42/188 [01:30<09:57,  4.09s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 43/188 [01:32<07:47,  3.23s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 44/188 [01:33<06:17,  2.62s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 45/188 [01:34<05:14,  2.20s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 46/188 [01:35<04:30,  1.91s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 47/188 [01:36<03:59,  1.70s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 48/188 [01:38<03:37,  1.55s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 49/188 [01:39<03:21,  1.45s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 50/188 [01:40<03:10,  1.38s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 51/188 [01:41<03:02,  1.33s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 52/188 [01:42<02:56,  1.30s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 53/188 [01:44<02:51,  1.27s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 54/188 [01:45<02:48,  1.26s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 55/188 [01:46<02:45,  1.24s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 56/188 [01:47<02:43,  1.24s/it]\u001b[A\n",
            "Iteration:  30%|███       | 57/188 [01:49<02:41,  1.23s/it]\u001b[A\n",
            "Iteration:  31%|███       | 58/188 [01:50<02:39,  1.23s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 59/188 [01:51<02:38,  1.23s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 60/188 [02:12<15:09,  7.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.36922383657161223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  32%|███▏      | 61/188 [02:13<11:17,  5.34s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 62/188 [02:14<08:36,  4.10s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 63/188 [02:16<06:45,  3.24s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 64/188 [02:17<05:26,  2.64s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 65/188 [02:18<04:31,  2.21s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 66/188 [02:19<03:53,  1.91s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 67/188 [02:20<03:26,  1.71s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 68/188 [02:22<03:07,  1.56s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 69/188 [02:23<02:53,  1.46s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 70/188 [02:24<02:43,  1.39s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 71/188 [02:25<02:37,  1.34s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 72/188 [02:27<02:31,  1.31s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 73/188 [02:28<02:27,  1.28s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 74/188 [02:29<02:24,  1.27s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 75/188 [02:30<02:21,  1.26s/it]\u001b[A\n",
            "Iteration:  40%|████      | 76/188 [02:31<02:19,  1.25s/it]\u001b[A\n",
            "Iteration:  41%|████      | 77/188 [02:33<02:17,  1.24s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 78/188 [02:34<02:16,  1.24s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 79/188 [02:35<02:14,  1.24s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 80/188 [02:56<12:53,  7.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.3555502176601836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  43%|████▎     | 81/188 [02:57<09:35,  5.38s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 82/188 [02:59<07:18,  4.13s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 83/188 [03:00<05:42,  3.26s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 84/188 [03:01<04:36,  2.66s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 85/188 [03:02<03:49,  2.23s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 86/188 [03:03<03:16,  1.93s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 87/188 [03:05<02:53,  1.72s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 88/188 [03:06<02:37,  1.58s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 89/188 [03:07<02:25,  1.47s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 90/188 [03:08<02:17,  1.40s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 91/188 [03:10<02:10,  1.35s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 92/188 [03:11<02:05,  1.31s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 93/188 [03:12<02:02,  1.29s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 94/188 [03:13<01:59,  1.27s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 95/188 [03:15<01:56,  1.26s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 96/188 [03:16<01:54,  1.25s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 97/188 [03:17<01:52,  1.24s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 98/188 [03:18<01:51,  1.23s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 99/188 [03:19<01:49,  1.23s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 100/188 [03:40<10:23,  7.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.36857878274105965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  54%|█████▎    | 101/188 [03:41<07:43,  5.33s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 102/188 [03:43<05:52,  4.10s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 103/188 [03:44<04:34,  3.23s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 104/188 [03:45<03:40,  2.63s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 105/188 [03:46<03:02,  2.20s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 106/188 [03:48<02:36,  1.91s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 107/188 [03:49<02:17,  1.70s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 108/188 [03:50<02:04,  1.55s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 109/188 [03:51<01:54,  1.45s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 110/188 [03:52<01:47,  1.38s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 111/188 [03:54<01:42,  1.33s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 112/188 [03:55<01:38,  1.30s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 113/188 [03:56<01:35,  1.27s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 114/188 [03:57<01:33,  1.26s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 115/188 [03:58<01:30,  1.24s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 116/188 [04:00<01:28,  1.23s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 117/188 [04:01<01:27,  1.23s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 118/188 [04:02<01:25,  1.22s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 119/188 [04:03<01:24,  1.22s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 120/188 [04:24<08:02,  7.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.3631977058471517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  64%|██████▍   | 121/188 [04:25<05:56,  5.32s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 122/188 [04:27<04:30,  4.09s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 123/188 [04:28<03:30,  3.23s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 124/188 [04:29<02:48,  2.63s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 125/188 [04:30<02:18,  2.21s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 126/188 [04:31<01:58,  1.91s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 127/188 [04:33<01:43,  1.70s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 128/188 [04:34<01:33,  1.56s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 129/188 [04:35<01:25,  1.46s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 130/188 [04:36<01:20,  1.39s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 131/188 [04:37<01:16,  1.34s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 132/188 [04:39<01:12,  1.30s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 133/188 [04:40<01:10,  1.28s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 134/188 [04:41<01:08,  1.26s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 135/188 [04:42<01:06,  1.25s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 136/188 [04:44<01:04,  1.24s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 137/188 [04:45<01:03,  1.24s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 138/188 [04:46<01:01,  1.23s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 139/188 [04:47<01:00,  1.23s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 140/188 [05:08<05:43,  7.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.33660707052083727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  75%|███████▌  | 141/188 [05:09<04:12,  5.38s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 142/188 [05:11<03:10,  4.13s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 143/188 [05:12<02:26,  3.26s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 144/188 [05:13<01:56,  2.65s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 145/188 [05:14<01:35,  2.23s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 146/188 [05:16<01:21,  1.93s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 147/188 [05:17<01:10,  1.72s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 148/188 [05:18<01:02,  1.57s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 149/188 [05:19<00:57,  1.47s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 150/188 [05:21<00:52,  1.39s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 151/188 [05:22<00:49,  1.34s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 152/188 [05:23<00:47,  1.31s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 153/188 [05:24<00:45,  1.29s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 154/188 [05:25<00:43,  1.27s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 155/188 [05:27<00:41,  1.26s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 156/188 [05:28<00:39,  1.25s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 157/188 [05:29<00:38,  1.24s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 158/188 [05:30<00:37,  1.24s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 159/188 [05:32<00:35,  1.24s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 160/188 [05:52<03:19,  7.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.34232060642952616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  86%|████████▌ | 161/188 [05:54<02:24,  5.34s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 162/188 [05:55<01:46,  4.10s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 163/188 [05:56<01:20,  3.24s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 164/188 [05:57<01:03,  2.63s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 165/188 [05:58<00:50,  2.21s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 166/188 [06:00<00:42,  1.91s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 167/188 [06:01<00:35,  1.71s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 168/188 [06:02<00:31,  1.56s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 169/188 [06:03<00:27,  1.46s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 170/188 [06:05<00:24,  1.38s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 171/188 [06:06<00:22,  1.33s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 172/188 [06:07<00:20,  1.29s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 173/188 [06:08<00:19,  1.27s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 174/188 [06:09<00:17,  1.25s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 175/188 [06:11<00:16,  1.24s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 176/188 [06:12<00:14,  1.24s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 177/188 [06:13<00:13,  1.23s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 178/188 [06:14<00:12,  1.23s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 179/188 [06:16<00:11,  1.23s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 180/188 [06:36<00:56,  7.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.3334676631270571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  96%|█████████▋| 181/188 [06:38<00:37,  5.33s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 182/188 [06:39<00:24,  4.10s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 183/188 [06:40<00:16,  3.23s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 184/188 [06:41<00:10,  2.63s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 185/188 [06:42<00:06,  2.20s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 186/188 [06:44<00:03,  1.91s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 187/188 [06:45<00:01,  1.70s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 188/188 [06:45<00:00,  2.16s/it]\n",
            "\n",
            "Epoch: 100%|██████████| 1/1 [06:45<00:00, 405.98s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " Final train acc: 0.5952095808383233, Final train loss: 0.2755654909192248\n",
            "\n",
            " Best val acc: 0.5023923444976076, Best val loss: 0.4036301057389442\n",
            "\n",
            "Testing...\n",
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " Final val acc: 0.5023923444976076, val loss: 0.4036301057389442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-SzMPOoIaz-",
        "outputId": "53359467-2c95-4ef1-be87-95b19c2768e9"
      },
      "source": [
        "model_type = 'bert'\n",
        "model_name = 'bert-large-uncased'\n",
        "task_name = None\n",
        "batch_size = 16\n",
        "epochs = 6\n",
        "lr_tune = 2e-5\n",
        "lr = 5e-5\n",
        "pretrain_size = 10000\n",
        "logging_steps = 20\n",
        "logging_steps_tune = 250\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "Using model bert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tuning...\n",
            "Creating features from dataset...\n",
            "Attention! you are removing from token_b (swag task is ok). If you are training ARC and RACE (you are popping question + options), you need to try to use a bigger max seq length!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n",
            "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loading training dataset\n",
            "\n",
            " Loading validation dataset\n",
            "\n",
            "The first 60 predictions [0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.7873333333333333, val loss: 0.4665636904637019\n",
            "\n",
            "The first 60 predictions [0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.7711666666666667, val loss: 0.4830284248987834\n",
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.7585, val loss: 0.5132558157841365\n",
            "\n",
            "The first 60 predictions [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.802, val loss: 0.43396337485313413\n",
            "\n",
            "The first 60 predictions [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.8003333333333333, val loss: 0.44776561202605564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  17%|█▋        | 1/6 [45:00<3:45:01, 2700.24s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.8115, val loss: 0.40654739691813785\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.8085, val loss: 0.47883103393514953\n",
            "\n",
            "The first 60 predictions [0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8111666666666667, val loss: 0.4395154359738032\n",
            "\n",
            "The first 60 predictions [0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.78, val loss: 0.5365045765737693\n",
            "\n",
            "The first 60 predictions [0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.7943333333333333, val loss: 0.4659607127507528\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8003333333333333, val loss: 0.4678900956213474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  33%|███▎      | 2/6 [1:30:22<3:00:27, 2706.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.81, val loss: 0.45635979073743027\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.812, val loss: 0.8208587189440926\n",
            "\n",
            "The first 60 predictions [0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8163333333333334, val loss: 0.7430574367257456\n",
            "\n",
            "The first 60 predictions [0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8093333333333333, val loss: 0.705051413765798\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8076666666666666, val loss: 0.6712787340432406\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
            "\n",
            " val acc: 0.8128333333333333, val loss: 0.672408910634617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  50%|█████     | 3/6 [2:16:27<2:16:13, 2724.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8115, val loss: 0.5967149828622739\n",
            "\n",
            "The first 60 predictions [0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8183333333333334, val loss: 0.8917592056312909\n",
            "\n",
            "The first 60 predictions [0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8068333333333333, val loss: 0.9092960957655062\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8145, val loss: 0.7904912623874844\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8095, val loss: 0.8217088034326832\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1]\n",
            "\n",
            " val acc: 0.8101666666666667, val loss: 0.851754492794474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  67%|██████▋   | 4/6 [3:02:31<1:31:12, 2736.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8045, val loss: 0.8634140171011289\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8171666666666667, val loss: 0.9837861055405811\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8106666666666666, val loss: 1.0176312897861934\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8186666666666667, val loss: 0.986801527693402\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8085, val loss: 1.02788861178436\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8101666666666667, val loss: 1.001316809314924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  83%|████████▎ | 5/6 [3:48:33<45:43, 2743.93s/it]  \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.818, val loss: 0.9405725289539745\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8235, val loss: 1.0417789642479427\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8171666666666667, val loss: 1.0261754489252344\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8135, val loss: 1.0589251904667665\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8145, val loss: 1.0767322651529685\n",
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.8151666666666667, val loss: 1.0821263412835542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 100%|██████████| 6/6 [4:34:33<00:00, 2745.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
            "\n",
            " val acc: 0.816, val loss: 1.082403503245907\n",
            "\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 0/188 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loading training dataset\n",
            "\n",
            " Loading validation dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   1%|          | 1/188 [00:01<03:53,  1.25s/it]\u001b[A\n",
            "Iteration:   1%|          | 2/188 [00:02<03:51,  1.24s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 3/188 [00:03<03:49,  1.24s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 4/188 [00:04<03:47,  1.24s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 5/188 [00:06<03:46,  1.24s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 6/188 [00:07<03:44,  1.24s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 7/188 [00:08<03:43,  1.24s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 8/188 [00:09<03:42,  1.24s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 9/188 [00:11<03:40,  1.23s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 10/188 [00:12<03:40,  1.24s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 11/188 [00:13<03:38,  1.23s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 12/188 [00:14<03:37,  1.23s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 13/188 [00:16<03:36,  1.23s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 14/188 [00:17<03:35,  1.24s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 15/188 [00:18<03:33,  1.24s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 16/188 [00:19<03:32,  1.23s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 17/188 [00:21<03:31,  1.24s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 18/188 [00:22<03:31,  1.24s/it]\u001b[A\n",
            "Iteration:  10%|█         | 19/188 [00:23<03:29,  1.24s/it]\u001b[A\n",
            "Iteration:  11%|█         | 20/188 [00:44<20:10,  7.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.4036297126019255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|█         | 21/188 [00:45<15:02,  5.40s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 22/188 [00:47<11:28,  4.15s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 23/188 [00:48<08:58,  3.26s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 24/188 [00:49<07:14,  2.65s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 25/188 [00:50<06:01,  2.22s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 26/188 [00:51<05:11,  1.92s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 27/188 [00:53<04:35,  1.71s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 28/188 [00:54<04:09,  1.56s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 29/188 [00:55<03:51,  1.46s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 30/188 [00:56<03:38,  1.39s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 31/188 [00:57<03:29,  1.34s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 32/188 [00:59<03:22,  1.30s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 33/188 [01:00<03:18,  1.28s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 34/188 [01:01<03:14,  1.26s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 35/188 [01:02<03:11,  1.25s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 36/188 [01:04<03:08,  1.24s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 37/188 [01:05<03:05,  1.23s/it]\u001b[A\n",
            "Iteration:  20%|██        | 38/188 [01:06<03:04,  1.23s/it]\u001b[A\n",
            "Iteration:  21%|██        | 39/188 [01:07<03:02,  1.22s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 40/188 [01:28<17:33,  7.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.4013578324241841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  22%|██▏       | 41/188 [01:29<13:05,  5.34s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 42/188 [01:31<09:59,  4.11s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 43/188 [01:32<07:48,  3.23s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 44/188 [01:33<06:18,  2.63s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 45/188 [01:34<05:14,  2.20s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 46/188 [01:35<04:30,  1.91s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 47/188 [01:37<03:59,  1.70s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 48/188 [01:38<03:37,  1.55s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 49/188 [01:39<03:21,  1.45s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 50/188 [01:40<03:10,  1.38s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 51/188 [01:41<03:02,  1.33s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 52/188 [01:43<02:56,  1.30s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 53/188 [01:44<02:52,  1.28s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 54/188 [01:45<02:48,  1.26s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 55/188 [01:46<02:46,  1.25s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 56/188 [01:48<02:43,  1.24s/it]\u001b[A\n",
            "Iteration:  30%|███       | 57/188 [01:49<02:41,  1.23s/it]\u001b[A\n",
            "Iteration:  31%|███       | 58/188 [01:50<02:39,  1.23s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 59/188 [01:51<02:37,  1.22s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 60/188 [02:12<15:10,  7.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.4178283667944847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  32%|███▏      | 61/188 [02:13<11:18,  5.34s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 62/188 [02:14<08:36,  4.10s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 63/188 [02:16<06:43,  3.23s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 64/188 [02:17<05:26,  2.63s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 65/188 [02:18<04:31,  2.21s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 66/188 [02:19<03:52,  1.91s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 67/188 [02:21<03:25,  1.70s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 68/188 [02:22<03:06,  1.55s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 69/188 [02:23<02:52,  1.45s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 70/188 [02:24<02:42,  1.38s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 71/188 [02:25<02:35,  1.33s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 72/188 [02:27<02:30,  1.29s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 73/188 [02:28<02:26,  1.27s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 74/188 [02:29<02:23,  1.25s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 75/188 [02:30<02:20,  1.24s/it]\u001b[A\n",
            "Iteration:  40%|████      | 76/188 [02:31<02:18,  1.24s/it]\u001b[A\n",
            "Iteration:  41%|████      | 77/188 [02:33<02:16,  1.23s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 78/188 [02:34<02:15,  1.23s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 79/188 [02:35<02:14,  1.23s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 80/188 [02:56<12:51,  7.14s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0]\n",
            "\n",
            " val acc: 0.5502392344497608, val loss: 0.5284035605319003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  43%|████▎     | 81/188 [02:57<09:33,  5.36s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 82/188 [02:59<07:16,  4.12s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 83/188 [03:00<05:41,  3.25s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 84/188 [03:01<04:34,  2.64s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 85/188 [03:02<03:48,  2.22s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 86/188 [03:03<03:15,  1.92s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 87/188 [03:05<02:52,  1.71s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 88/188 [03:06<02:36,  1.56s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 89/188 [03:07<02:24,  1.46s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 90/188 [03:08<02:16,  1.39s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 91/188 [03:10<02:10,  1.34s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 92/188 [03:11<02:05,  1.31s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 93/188 [03:12<02:01,  1.28s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 94/188 [03:13<01:58,  1.26s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 95/188 [03:14<01:56,  1.25s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 96/188 [03:16<01:54,  1.24s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 97/188 [03:17<01:52,  1.24s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 98/188 [03:18<01:50,  1.23s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 99/188 [03:19<01:49,  1.23s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 100/188 [03:40<10:26,  7.12s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.36831063412605447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  54%|█████▎    | 101/188 [03:41<07:45,  5.35s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 102/188 [03:43<05:53,  4.11s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 103/188 [03:44<04:35,  3.24s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 104/188 [03:45<03:41,  2.63s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 105/188 [03:46<03:02,  2.20s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 106/188 [03:47<02:36,  1.91s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 107/188 [03:49<02:17,  1.70s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 108/188 [03:50<02:04,  1.55s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 109/188 [03:51<01:54,  1.45s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 110/188 [03:52<01:47,  1.38s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 111/188 [03:54<01:42,  1.33s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 112/188 [03:55<01:38,  1.30s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 113/188 [03:56<01:35,  1.27s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 114/188 [03:57<01:32,  1.26s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 115/188 [03:58<01:30,  1.24s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 116/188 [04:00<01:28,  1.24s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 117/188 [04:01<01:27,  1.23s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 118/188 [04:02<01:25,  1.22s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 119/188 [04:03<01:24,  1.22s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 120/188 [04:24<07:58,  7.04s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0]\n",
            "\n",
            " val acc: 0.5311004784688995, val loss: 0.42169642226493104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  64%|██████▍   | 121/188 [04:25<05:54,  5.29s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 122/188 [04:26<04:28,  4.06s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 123/188 [04:27<03:28,  3.21s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 124/188 [04:29<02:47,  2.61s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 125/188 [04:30<02:18,  2.19s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 126/188 [04:31<01:57,  1.90s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 127/188 [04:32<01:43,  1.69s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 128/188 [04:34<01:33,  1.55s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 129/188 [04:35<01:25,  1.45s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 130/188 [04:36<01:19,  1.38s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 131/188 [04:37<01:15,  1.33s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 132/188 [04:38<01:12,  1.30s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 133/188 [04:40<01:09,  1.27s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 134/188 [04:41<01:07,  1.26s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 135/188 [04:42<01:05,  1.24s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 136/188 [04:43<01:04,  1.24s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 137/188 [04:45<01:02,  1.23s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 138/188 [04:46<01:01,  1.22s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 139/188 [04:47<00:59,  1.22s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 140/188 [05:08<05:40,  7.10s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5167464114832536, val loss: 0.35531742807398453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  75%|███████▌  | 141/188 [05:09<04:10,  5.33s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 142/188 [05:10<03:08,  4.09s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 143/188 [05:11<02:25,  3.23s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 144/188 [05:13<01:55,  2.63s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 145/188 [05:14<01:34,  2.20s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 146/188 [05:15<01:20,  1.91s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 147/188 [05:16<01:09,  1.70s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 148/188 [05:17<01:02,  1.55s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 149/188 [05:19<00:56,  1.45s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 150/188 [05:20<00:52,  1.38s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 151/188 [05:21<00:49,  1.33s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 152/188 [05:22<00:46,  1.30s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 153/188 [05:24<00:44,  1.27s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 154/188 [05:25<00:42,  1.25s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 155/188 [05:26<00:41,  1.24s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 156/188 [05:27<00:39,  1.24s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 157/188 [05:28<00:38,  1.23s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 158/188 [05:30<00:36,  1.23s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 159/188 [05:31<00:35,  1.22s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 160/188 [05:52<03:18,  7.10s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.49760765550239233, val loss: 0.4459370517350258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  86%|████████▌ | 161/188 [05:53<02:23,  5.33s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 162/188 [05:54<01:46,  4.09s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 163/188 [05:55<01:20,  3.23s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 164/188 [05:56<01:02,  2.62s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 165/188 [05:58<00:50,  2.20s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 166/188 [05:59<00:41,  1.90s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 167/188 [06:00<00:35,  1.70s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 168/188 [06:01<00:31,  1.56s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 169/188 [06:03<00:27,  1.46s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 170/188 [06:04<00:24,  1.38s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 171/188 [06:05<00:22,  1.33s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 172/188 [06:06<00:20,  1.30s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 173/188 [06:07<00:19,  1.27s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 174/188 [06:09<00:17,  1.26s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 175/188 [06:10<00:16,  1.24s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 176/188 [06:11<00:14,  1.23s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 177/188 [06:12<00:13,  1.23s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 178/188 [06:13<00:12,  1.23s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 179/188 [06:15<00:11,  1.22s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 180/188 [06:35<00:56,  7.08s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " val acc: 0.5023923444976076, val loss: 0.40650987910463454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  96%|█████████▋| 181/188 [06:37<00:37,  5.31s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 182/188 [06:38<00:24,  4.08s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 183/188 [06:39<00:16,  3.22s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 184/188 [06:40<00:10,  2.62s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 185/188 [06:41<00:06,  2.20s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 186/188 [06:43<00:03,  1.90s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 187/188 [06:44<00:01,  1.70s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 188/188 [06:45<00:00,  2.15s/it]\n",
            "\n",
            "Epoch: 100%|██████████| 1/1 [06:45<00:00, 405.07s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " Final train acc: 0.5976047904191617, Final train loss: 0.4078329834452969\n",
            "\n",
            " Best val acc: 0.5167464114832536, Best val loss: 0.35531742807398453\n",
            "\n",
            "Testing...\n",
            "\n",
            "The first 60 predictions [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "\n",
            " Final val acc: 0.5167464114832536, val loss: 0.35531742807398453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6fiwtSutf70"
      },
      "source": [
        "model_type = 'bert'\n",
        "model_name = 'textattack/bert-base-uncased-MNLI'\n",
        "task_name = 'mnli'\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "lr_tune = 1e-4\n",
        "lr = 1e-5\n",
        "# pretrain_size = 73546\n",
        "pretrain_size = 5\n",
        "logging_steps = 20\n",
        "logging_steps_tune = 50\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG2qop8DzoG_"
      },
      "source": [
        "### XLNet\n",
        "\n",
        "Remeber that `XLNet` pads from the left. Need to change the preprocessor!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F95juCVzp01"
      },
      "source": [
        "model_type = 'xlnet'\n",
        "model_name = 'xlnet-base-cased'\n",
        "task_name = None\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "lr_tune = 1e-4\n",
        "lr = 1e-5\n",
        "pretrain_size = 70000\n",
        "logging_steps = 10\n",
        "logging_steps_tune = 1000\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GJvvByG0O-j"
      },
      "source": [
        "### RoBerta\n",
        "\n",
        "Check this for `</s>` rules: https://github.com/pytorch/fairseq/issues/1654\n",
        "\n",
        "I may have got it wrong. Check `Data Preprocessing` sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzQ0FVTI0Q2F"
      },
      "source": [
        "model_type = 'roberta'\n",
        "model_name = 'roberta-base'\n",
        "task_name = None\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "lr_tune = 1e-5\n",
        "lr = 2e-6\n",
        "# pretrain_size = 73546\n",
        "pretrain_size = 1000\n",
        "logging_steps = 10\n",
        "logging_steps_tune = 20\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohrBUjsIiES0"
      },
      "source": [
        "model_type = 'roberta'\n",
        "model_name = 'roberta-large'\n",
        "task_name = None\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "lr_tune = 1e-6\n",
        "lr = 5e-6\n",
        "# pretrain_size = 73546\n",
        "pretrain_size = 1000\n",
        "logging_steps = 10\n",
        "logging_steps_tune = 25\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rprO1FhzIJj"
      },
      "source": [
        "Try `roberta-large-mnli`, but this one has a `neutral` label (`num_labels = 3`).\n",
        "\n",
        "There is an [online entailment](https://huggingface.co/roberta-large-mnli?text=John+grabbed+the+ladder+and+put+it+in+his+truck.+%3C%2Fs%3E%3C%2Fs%3E+John+put+a+drill+and+rope+in+the+bucket+and+also+put+that+in+this+truck) model to play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A29iZ1dduQHK"
      },
      "source": [
        "# Try roberta-large-mnli\n",
        "model_type = 'roberta'\n",
        "model_name = 'roberta-large-mnli'\n",
        "task_name = 'mnli'\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "lr_tune = 1e-6\n",
        "lr = 5e-6\n",
        "# pretrain_size = 73546\n",
        "pretrain_size = 1000\n",
        "logging_steps = 10\n",
        "logging_steps_tune = 25\n",
        "\n",
        "run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCTR69kyKAu1"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\n",
        "\n",
        "[2] Zellers, R., Bisk, Y., Schwartz, R., & Choi, Y. (2018). Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326."
      ]
    }
  ]
}