{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"csqa.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"orbnCUQtyTTB"},"source":["# Commonsense QA"]},{"cell_type":"markdown","metadata":{"id":"VlUMPA6NQx9e"},"source":["## Colab setups"]},{"cell_type":"code","metadata":{"id":"xUiA5dX3yFGX","executionInfo":{"status":"ok","timestamp":1605167624005,"user_tz":300,"elapsed":690,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"s36tQoeYVLA0","executionInfo":{"status":"ok","timestamp":1605167624180,"user_tz":300,"elapsed":847,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"d1860849-9dc1-4957-80fa-fdfeb9a517bc","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9KVidViv0AO2","executionInfo":{"status":"ok","timestamp":1605167624183,"user_tz":300,"elapsed":846,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/eecs595/commonsense_qa'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","sys.path.append(GOOGLE_DRIVE_PATH)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4WOO9tvye13"},"source":["## Dependency installation."]},{"cell_type":"code","metadata":{"id":"m03X-0vLyLM0","executionInfo":{"status":"ok","timestamp":1605167624624,"user_tz":300,"elapsed":1284,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["import json\n","import codecs\n","import argparse\n","from copy import deepcopy\n","from tqdm import tqdm, trange\n","\n","import random\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2OCyMtR0xMb"},"source":["Install `datasets`"]},{"cell_type":"code","metadata":{"id":"FI1b9vGk0tfK","executionInfo":{"status":"ok","timestamp":1605167628610,"user_tz":300,"elapsed":5260,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"e055022e-dc82-4bd0-da13-a1ca8029acb5","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install datasets\n","from datasets import load_dataset"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aern7KsY01YS"},"source":["Install `transformers`"]},{"cell_type":"code","metadata":{"id":"5hTWa5DGyvQh","executionInfo":{"status":"ok","timestamp":1605167631549,"user_tz":300,"elapsed":8192,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"98f87e39-81d5-4b55-a7d0-d3cd21872691","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install transformers\n","from transformers import (AdamW, get_linear_schedule_with_warmup,\n","                          BertConfig, BertForMultipleChoice, BertTokenizer,\n","                          XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer,\n","                          RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qzgCrnTGzNYv"},"source":["## Utilities"]},{"cell_type":"code","metadata":{"id":"4tv9jG1yzT5m","executionInfo":{"status":"ok","timestamp":1605167631551,"user_tz":300,"elapsed":8190,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def set_seed(seed):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","\n","def load_model(model='all'):\n","    if model == 'bert':\n","        return BertConfig, BertForMultipleChoice, BertTokenizer\n","    elif model == 'xlnet':\n","        return XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer\n","    elif model == 'roberta':\n","        return RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer\n","    elif model == 'gpt2':\n","        raise NotImplemented\n","    raise NotImplemented\n","\n","\n","def load_optimizer(args, model, train_size):\n","    num_training_steps = train_size // args.num_train_epochs\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters()\n","                    if not any(nd in n for nd in no_decay)],\n","         'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.named_parameters()\n","                    if any(nd in n for nd in no_decay)],\n","         'weight_decay': 0.0}\n","    ]\n","\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=num_training_steps)\n","\n","    return model, optimizer, scheduler\n","\n","\n","def load_data(dataset='commonsense_qa', preview=-1):\n","\n","    assert dataset in {'commonsense_qa', 'conv_entail', 'eat'}\n","\n","    if dataset == 'commonsense_qa':\n","        ds = load_dataset('commonsense_qa')\n","\n","        if preview > 0:\n","            data_tr = ds.data['train']\n","            question = data_tr['question']\n","            choices = data_tr['choices']\n","            answerKey = data_tr['answerKey']\n","            print(question[preview])\n","            for label, text in zip(choices[preview]['label'], choices[preview]['text']):\n","                print(label, text)\n","            print(answerKey[preview])\n","\n","    elif dataset == 'conv_entail':\n","        dev_set = codecs.open('data/conv_entail/dev_set.json', 'r', encoding='utf-8').read()\n","        act_tag = codecs.open('data/conv_entail/act_tag.json', 'r', encoding='utf-8').read()\n","        ds = json.loads(dev_set), json.loads(act_tag)\n","\n","        if preview > 0:\n","            print('Preview not yet implemented for this dataset.')\n","\n","    else:\n","        eat = codecs.open('data/eat/eat_train.json', 'r', encoding='utf-8').read()\n","        ds = json.loads(eat)\n","\n","        if preview > 0:\n","            story = ds[preview]['story']\n","            label = ds[preview]['label']\n","            bp = ds[preview]['breakpoint']\n","            for line in story:\n","                print(line)\n","            print(label)\n","            print(bp)\n","\n","    return ds"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Er-oLRB6dmw","executionInfo":{"status":"ok","timestamp":1605167631552,"user_tz":300,"elapsed":8182,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"34191b5e-255c-4e45-e757-256d1ad6eb27","colab":{"base_uri":"https://localhost:8080/"}},"source":["load_data(dataset='commonsense_qa', preview=5)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Using custom data configuration default\n","Reusing dataset commonsense_qa (/root/.cache/huggingface/datasets/commonsense_qa/default/0.1.0/0e60f0ee8c8509e854ed897f65eb5b2e6ca22578d64cbc3812c79b527d7a7a29)\n"],"name":"stderr"},{"output_type":"stream","text":["What home entertainment equipment requires cable?\n","A radio shack\n","B substation\n","C cabinet\n","D television\n","E desk\n","D\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({'train': Dataset(features: {'answerKey': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'choices': Sequence(feature={'label': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None)}, num_rows: 9741), 'validation': Dataset(features: {'answerKey': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'choices': Sequence(feature={'label': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None)}, num_rows: 1221), 'test': Dataset(features: {'answerKey': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'choices': Sequence(feature={'label': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None)}, num_rows: 1140)})"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"rIxGTEYezc1L"},"source":["## Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"HS5CDydVzfAB","executionInfo":{"status":"ok","timestamp":1605167632000,"user_tz":300,"elapsed":8627,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["class InputExample(object):\n","    \"\"\"\n","    A single multiple choice question.\n","    \"\"\"\n","\n","    def __init__(self, example_id, question, answers, label):\n","        self.example_id = example_id\n","        self.question = question\n","        self.answers = answers\n","        self.label = label\n","\n","\n","class InputFeatures(object):\n","    \"\"\"\n","    A single feature converted from an example.\n","    \"\"\"\n","\n","    def __init__(self, example_id, choices_features, label):\n","        self.example_id = example_id\n","        self.label = label\n","        self.choices_features = [\n","            {'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids}\n","            for _, input_ids, input_mask, segment_ids in choices_features\n","        ]\n","\n","\n","class CommonsenseQAProcessor:\n","    \"\"\"\n","    A Commonsense QA Data Processor\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.dataset = None\n","        self.labels = [0, 1, 2, 3, 4]\n","        self.LABELS = ['A', 'B', 'C', 'D', 'E']\n","\n","    def get_split(self, split='train'):\n","        if self.dataset is None:\n","            self.dataset = load_data(dataset='commonsense_qa', preview=-1)\n","        return self.dataset[split]\n","\n","    def create_examples(self, split='train'):\n","        examples = []\n","        data_tr = self.get_split(split)\n","        example_id = 0\n","\n","        for question, choices, answerKey in zip(data_tr['question'], data_tr['choices'], data_tr['answerKey']):\n","            answers = np.array(choices['text'])\n","            label = self.LABELS.index(answerKey)\n","            examples.append(InputExample(\n","                example_id=example_id, question=question,\n","                answers=answers, label=label\n","            ))\n","            example_id += 1\n","\n","        return examples\n","\n","\n","def truncate_seq_pair(tokens_a, tokens_b, max_length):\n","    \"\"\"\n","    Truncates a sequence pair in place to the maximum length.\n","\n","    This is a simple heuristic which will always truncate the longer sequence one token at a time.\n","    This makes more sense than truncating an equal percent of tokens from each,\n","    since if one sequence is very short then each token that's truncated\n","    likely contains more information than a longer sequence.\n","\n","    However, since we'd better not to remove tokens of options and questions,\n","    you can choose to use a bigger length or only pop from context\n","    \"\"\"\n","\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) > len(tokens_b):\n","            tokens_a.pop()\n","        else:\n","            warning = 'Attention! you are removing from token_b (swag task is ok). ' \\\n","                      'If you are training ARC and RACE (you are popping question + options), ' \\\n","                      'you need to try to use a bigger max seq length!'\n","            print(warning)\n","            tokens_b.pop()\n","\n","\n","def examples_to_features(examples, label_list, max_seq_length, tokenizer,\n","                         cls_token_at_end=False,\n","                         cls_token='[CLS]',\n","                         cls_token_segment_id=1,\n","                         sep_token='[SEP]',\n","                         sequence_a_segment_id=0,\n","                         sequence_b_segment_id=1,\n","                         sep_token_extra=False,\n","                         pad_token_segment_id=0,\n","                         pad_on_left=False,\n","                         pad_token=0,\n","                         mask_padding_with_zero=True):\n","    \"\"\"\n","    Convert Commonsense QA examples to features.\n","\n","    The convention in BERT is:\n","    (a) For sequence pairs:\n","    tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","    type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","\n","    (b) For single sequences:\n","    tokens:   [CLS] the dog is hairy . [SEP]\n","    type_ids:   0   0   0   0  0     0   0\n","\n","    Where \"type_ids\" are used to indicate whether this is the first sequence or the second sequence.\n","    The embedding vectors for `type=0` and `type=1` were learned during pre-training\n","    and are added to the word piece embedding vector (and position vector).\n","    This is not *strictly* necessary since the [SEP] token unambiguously separates the sequences,\n","    but it makes it easier for the model to learn the concept of sequences.\n","\n","    For classification tasks, the first vector (corresponding to [CLS]) is used as as the \"sentence vector\".\n","    Note that this only makes sense because the entire model is fine-tuned.\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in tqdm(enumerate(examples), desc=\"Converting examples to features\", disable=True):\n","\n","        choices_features = []\n","        for ending_idx, (question, answers) in enumerate(zip(example.question, example.answers)):\n","\n","            tokens_a = tokenizer.tokenize(example.question)\n","            if example.question.find(\"_\") != -1:\n","                tokens_b = tokenizer.tokenize(example.question.replace(\"_\", answers))\n","            else:\n","                tokens_b = tokenizer.tokenize(answers)\n","\n","            special_tokens_count = 4 if sep_token_extra else 3\n","            truncate_seq_pair(tokens_a, tokens_b, max_seq_length - special_tokens_count)\n","\n","            tokens = tokens_a + [sep_token]\n","            if sep_token_extra:\n","                tokens += [sep_token]\n","\n","            segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","            if tokens_b:\n","                tokens += tokens_b + [sep_token]\n","                segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n","\n","            if cls_token_at_end:\n","                tokens = tokens + [cls_token]\n","                segment_ids = segment_ids + [cls_token_segment_id]\n","            else:\n","                tokens = [cls_token] + tokens\n","                segment_ids = [cls_token_segment_id] + segment_ids\n","\n","            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            # The mask has 1 for real tokens and 0 for padding tokens.\n","            # Only real tokens are attended to.\n","            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","            # Zero-pad up to the sequence length.\n","            padding_length = max_seq_length - len(input_ids)\n","\n","            if pad_on_left:\n","                input_ids = ([pad_token] * padding_length) + input_ids\n","                input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n","                segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","\n","            else:\n","                input_ids = input_ids + ([pad_token] * padding_length)\n","                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n","                segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n","\n","            assert len(input_ids) == max_seq_length\n","            assert len(input_mask) == max_seq_length\n","            assert len(segment_ids) == max_seq_length\n","\n","            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n","\n","        label = label_map[example.label]\n","\n","        if ex_index < 0:\n","            print(\"*** Example ***\")\n","            print(\"race_id: {}\".format(example.example_id))\n","            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n","                print(\"choice: {}\".format(choice_idx))\n","                print(\"tokens: {}\".format(' '.join(tokens)))\n","                print(\"input_ids: {}\".format(' '.join(map(str, input_ids))))\n","                print(\"input_mask: {}\".format(' '.join(map(str, input_mask))))\n","                print(\"segment_ids: {}\".format(' '.join(map(str, segment_ids))))\n","                print(\"label: {}\".format(label))\n","\n","        features.append(InputFeatures(\n","            example_id=example.example_id,\n","            choices_features=choices_features,\n","            label=label\n","        ))\n","\n","    return features\n","\n","\n","def load_features(args, tokenizer, mode='train'):\n","    \"\"\"\n","    Load the processed Commonsense QA dataset\n","    \"\"\"\n","\n","    def select_field(feature_list, field_name):\n","        return [\n","            [choice[field_name] for choice in feature.choices_features]\n","            for feature in feature_list\n","        ]\n","\n","    assert mode in {'train', 'validation', 'test'}\n","    print(\"Creating features from dataset...\")\n","\n","    processor = CommonsenseQAProcessor()\n","    label_list = processor.labels\n","    examples = processor.create_examples(split=mode)\n","\n","    print(\"Training number:\", str(len(examples)))\n","    features = examples_to_features(examples, label_list, args.max_seq_length, tokenizer,\n","                                    cls_token_at_end=bool(args.model_type in ['xlnet']),\n","                                    cls_token=tokenizer.cls_token,\n","                                    sep_token=tokenizer.sep_token,\n","                                    sep_token_extra=bool(args.model_type in ['roberta']),\n","                                    cls_token_segment_id=2 if args.model_type in ['xlnet'] else 0,\n","                                    pad_on_left=bool(args.model_type in ['xlnet']),\n","                                    pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0)\n","\n","    # Convert to Tensors and build dataset\n","    all_input_ids = torch.tensor(select_field(features, 'input_ids'), dtype=torch.long)\n","    all_input_mask = torch.tensor(select_field(features, 'input_mask'), dtype=torch.long)\n","    all_segment_ids = torch.tensor(select_field(features, 'segment_ids'), dtype=torch.long)\n","    all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)\n","\n","    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","    return dataset"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQ0BLRx-zmep"},"source":["## Training, Validation and Testing"]},{"cell_type":"code","metadata":{"id":"V3xQdSF8zl1q","executionInfo":{"status":"ok","timestamp":1605167904679,"user_tz":300,"elapsed":347,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def train(args, model, tokenizer):\n","\n","    print('\\n Loading training dataset')\n","    dataset_tr = load_features(args, tokenizer, mode='train')\n","    sampler_tr = RandomSampler(dataset_tr)\n","    dataloader_tr = DataLoader(dataset_tr, sampler=sampler_tr, batch_size=args.batch_size)\n","\n","    print('\\n Loading validation dataset')\n","    dataset_val = load_features(args, tokenizer, 'validation')\n","    sampler_val = SequentialSampler(dataset_val)\n","    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n","\n","    model, optimizer, scheduler = load_optimizer(args, model, len(dataloader_tr))\n","\n","    num_steps = 0\n","    best_steps = 0\n","    tr_loss = 0.0\n","    best_val_acc, best_val_loss = 0.0, 99999999999.0\n","    best_model = None\n","\n","    model.zero_grad()\n","    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=False, leave=True, position=1)\n","\n","    for _ in train_iterator:\n","\n","        epoch_iterator = tqdm(dataloader_tr, desc=\"Iteration\", disable=False, leave=True, position=1)\n","        for step, batch in enumerate(epoch_iterator):\n","\n","            model.train()\n","\n","            batch = tuple(b.to(args.device) for b in batch)\n","            inputs = {'input_ids': batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n","                      'labels': batch[3]}\n","            outputs = model(**inputs)\n","            loss = outputs[0]\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","            tr_loss += loss.item()\n","\n","            optimizer.step()\n","            scheduler.step()\n","            model.zero_grad()\n","            num_steps += 1\n","\n","            if args.logging_steps > 0 and num_steps % args.logging_steps == 0:\n","                results = evaluate(args, model, dataloader_val)\n","                print(\"\\n val acc: {}, val loss: {}\"\n","                      .format(str(results['val_acc']), str(results['val_loss'])))\n","                if results[\"val_acc\"] > best_val_acc:\n","                    best_val_acc, best_val_loss = results[\"val_acc\"], results[\"val_loss\"]\n","                    best_steps = num_steps\n","                    best_model = deepcopy(model)\n","\n","    loss = tr_loss / num_steps\n","\n","    return best_model\n","\n","\n","def evaluate(args, model, dataloader):\n","\n","    val_loss = 0.0\n","    num_steps = 0\n","    preds, labels = None, None\n","\n","    results = {}\n","\n","    for batch in tqdm(dataloader, desc=\"Validation\", disable=True, leave=True, position=1):\n","        model.eval()\n","        batch = tuple(t.to(args.device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids': batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n","                      'labels': batch[3]}\n","            outputs = model(**inputs)\n","            loss, logits = outputs[:2]\n","\n","            val_loss += loss.mean().item()\n","\n","        num_steps += 1\n","\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            labels = inputs['labels'].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n","\n","    loss = val_loss / num_steps\n","    preds = np.argmax(preds, axis=1)\n","    acc = (preds == labels).mean()\n","    result = {\"val_acc\": acc, \"val_loss\": loss}\n","    results.update(result)\n","\n","    return results\n","\n","\n","def test(args, model):\n","    dataset = load_features(args, tokenizer, mode='test')\n","    sampler = RandomSampler(dataset)\n","    dataloader_tr = DataLoader(dataset, sampler=sampler, batch_size=args.batch_size)\n","    results = evaluate(args, model, dataloader_val)\n","    print(\"test acc: %s, test loss: %s\",\n","          str(results['val_acc']), str(results['val_loss']))"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWb1PKeZztGS"},"source":["## Run experiment"]},{"cell_type":"code","metadata":{"id":"G2blaf7izt3j","executionInfo":{"status":"error","timestamp":1605168299350,"user_tz":300,"elapsed":392492,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"15e63b42-35dd-4bf0-8b39-0c988746b581","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def main(args):\n","\n","    print('Using device', args.device)\n","    set_seed(args.seed)\n","\n","    if args.mode == 'train':\n","        processor = CommonsenseQAProcessor()\n","        num_labels = len(processor.labels)\n","\n","        config_class, model_class, tokenizer_class = load_model(args.model_type)\n","        config = config_class.from_pretrained(\n","            args.config_name if args.config_name else args.model_name,\n","            num_labels=num_labels, finetuning_task=args.task_name)\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.tokenizer_name if args.tokenizer_name else args.model_name,\n","            do_lower_case=True)\n","        model = model_class.from_pretrained(\n","            args.model_name, from_tf=bool('.ckpt' in args.model_name), config=config)\n","\n","        model.to(args.device)\n","\n","        best_model = train(args, model, tokenizer)\n","\n","        test(args, best_model)\n","\n","\n","if __name__ == \"__main__\":\n","\n","    parser = argparse.ArgumentParser(description=\"Common sense question answering\")\n","\n","    parser.add_argument(\"--mode\", type=str, default='train',\n","                        help=\"Mode: <str> [ train | test ]\")\n","    parser.add_argument(\"--model_type\", type=str, default='bert',\n","                        help=\"Model: <str> [ bert | xlnet | roberta | gpt2 ]\")\n","    parser.add_argument(\"--task_name\", default=None, type=str, required=False,\n","                        help=\"The name of the task to train: <str> [ commonqa ]\")\n","    parser.add_argument(\"--model_name\", type=str,\n","                        default='bert-base-uncased',\n","                        help=\"Path to pre-trained model or shortcut name.\"\n","                             \"See https://huggingface.co/models\")\n","    parser.add_argument(\"--config_name\", type=str,\n","                        default='bert-base-uncased',\n","                        help=\"Pre-trained config name or path\")\n","    parser.add_argument(\"--tokenizer_name\", default='bert-base-uncased', type=str,\n","                        help=\"Pre-trained tokenizer name or path if not the same as model_name\")\n","\n","    parser.add_argument(\"--max_seq_length\", default=128, type=int,\n","                        help=\"The maximum total input sequence length after tokenization. \"\n","                             \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n","    parser.add_argument(\"--batch_size\", default=8, type=int,\n","                        help=\"Batch size for training.\")\n","\n","    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n","                        help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n","                        help=\"Weight decay if we apply some.\")\n","    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n","                        help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n","                        help=\"Max gradient norm.\")\n","\n","    parser.add_argument(\"--num_train_epochs\", default=3.0, type=float,\n","                        help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n","                        help=\"Linear warmup over warmup_steps.\")\n","    parser.add_argument('--logging_steps', type=int, default=5,\n","                        help=\"Log every n updates steps.\")\n","\n","    parser.add_argument('--fp16', action='store_true',\n","                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n","    parser.add_argument('--opt_level', type=str, default='O1',\n","                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","                             \"See details at https://nvidia.github.io/apex/amp.html\")\n","\n","    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed: <int>\")\n","    parser.add_argument(\"--device\", default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","\n","    args, unknown = parser.parse_known_args()\n","    main(args)\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Using device cuda\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using custom data configuration default\n","Reusing dataset commonsense_qa (/root/.cache/huggingface/datasets/commonsense_qa/default/0.1.0/0e60f0ee8c8509e854ed897f65eb5b2e6ca22578d64cbc3812c79b527d7a7a29)\n"],"name":"stderr"},{"output_type":"stream","text":["\n"," Loading training dataset\n","Creating features from dataset...\n","Training number: 9741\n"],"name":"stdout"},{"output_type":"stream","text":["Using custom data configuration default\n","Reusing dataset commonsense_qa (/root/.cache/huggingface/datasets/commonsense_qa/default/0.1.0/0e60f0ee8c8509e854ed897f65eb5b2e6ca22578d64cbc3812c79b527d7a7a29)\n"],"name":"stderr"},{"output_type":"stream","text":["\n"," Loading validation dataset\n","Creating features from dataset...\n","Training number: 1221\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n","Iteration:   0%|          | 0/1218 [00:00<?, ?it/s]\u001b[A\n","Iteration:   0%|          | 1/1218 [00:44<15:03:58, 44.57s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," val acc: 0.15561015561015562, val loss: 1.658754655738282\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   0%|          | 2/1218 [01:31<15:15:27, 45.17s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," val acc: 0.2325962325962326, val loss: 1.6178529558618084\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   0%|          | 3/1218 [02:18<15:28:01, 45.83s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," val acc: 0.2882882882882883, val loss: 1.5971957959380805\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   0%|          | 4/1218 [03:06<15:40:40, 46.49s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," val acc: 0.3185913185913186, val loss: 1.5881999692106559\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   0%|          | 5/1218 [03:55<15:53:49, 47.18s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," val acc: 0.32678132678132676, val loss: 1.5841971651401394\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   0%|          | 6/1218 [04:44<16:03:54, 47.72s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," val acc: 0.32432432432432434, val loss: 1.578283889620912\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   1%|          | 7/1218 [05:33<16:14:08, 48.26s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," val acc: 0.31695331695331697, val loss: 1.5731143468345692\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-e7c7a3f4ebc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-e7c7a3f4ebc6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-a5b818f03037>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, tokenizer)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 print(\"\\n val acc: {}, val loss: {}\"\n\u001b[1;32m     52\u001b[0m                       .format(str(results['val_acc']), str(results['val_loss'])))\n","\u001b[0;32m<ipython-input-19-a5b818f03037>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, dataloader)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mnum_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"iaqkSRroA93Z"},"source":["## Todo\n","\n","The current model is based on `bert`.\n","```\n","    parser.add_argument(\"--model_name\", type=str,\n","                        default='bert-base-uncased',\n","                        help=\"Path to pre-trained model or shortcut name.\"\n","                             \"See https://huggingface.co/models\")\n","```\n","\n","This would leads to\n","\n","```\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using custom data configuration default\n","```\n","\n","Should check https://huggingface.co/models for other models."]}]}