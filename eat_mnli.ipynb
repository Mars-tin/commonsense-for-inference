{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eat_mnli.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eJDJ8zF9LH6y",
        "XCTR69kyKAu1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIUz6Xrk7Mn"
      },
      "source": [
        "# EAT: Tuned with Knowledge\n",
        "\n",
        "EECS 595 Final Project, Task 3: EAT\n",
        "\n",
        "Credit: Ziqiao Ma\n",
        "\n",
        "Last update: 2020.12.16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7BQNAknKvsn"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrMYC3StmXpm"
      },
      "source": [
        "## Colab setups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LeDbHL6oLFI"
      },
      "source": [
        "Run this cell load the autoreload extension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUiA5dX3yFGX"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np_T5dXFoSrk"
      },
      "source": [
        "Run the following cell to mount your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s36tQoeYVLA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f174af-aee2-4366-9049-4b7ef5f5231c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KMR3UxKoedX"
      },
      "source": [
        "Fill in the Google Drive path where you uploaded the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKGqTElTolf8"
      },
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/eecs595/eat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLAALYTAo25M"
      },
      "source": [
        "Test if script files are located."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KVidViv0AO2"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "#GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "#sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "#print(os.listdir(GOOGLE_DRIVE_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB4aTTM14sCE"
      },
      "source": [
        "Check if dataset file is located, you should see `eat_test_unlabeled.json` and `eat_train.json` in the folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lrrOs4d4lkE",
        "outputId": "1b338cb7-da40-4cf9-cef5-dc2064b8aa56"
      },
      "source": [
        "!ls /content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/EAT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eat_test_unlabeled.json  eat_train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4WOO9tvye13"
      },
      "source": [
        "## Dependency installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m03X-0vLyLM0"
      },
      "source": [
        "import json\n",
        "import codecs\n",
        "import pandas as pd\n",
        "\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "# from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCt4UVaGNCgI"
      },
      "source": [
        "Install `sentencepiece` for `XLNetTokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmfU0rSbNCzm",
        "outputId": "8467cc3c-a786-453f-a8cb-e891817f4fc3"
      },
      "source": [
        "!pip install sentencepiece\n",
        "\n",
        "import sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 14.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 14.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 11.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 11.6MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 11.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 11.6MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 11.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 11.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 11.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 11.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 11.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 11.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aern7KsY01YS"
      },
      "source": [
        "Install `transformers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hTWa5DGyvQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6859e312-c1ab-4fb4-b87f-1f139435eb8b"
      },
      "source": [
        "!pip install transformers\n",
        "# !pip install transformers==2.0.0\n",
        "from transformers import  AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import BertModel, RobertaModel\n",
        "from transformers import (AdamW, get_linear_schedule_with_warmup, AutoModelForQuestionAnswering,\n",
        "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
        "                          XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
        "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
        "                          GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2702a67559ed0e8bda954c1e0ebae3ba4903d16ce6e3c518589c6e6083c6610b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h30zNEx1QmcC"
      },
      "source": [
        "Install `sentence-transformers` for [Sentence Bert](https://arxiv.org/abs/1908.10084) [1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33oNDf08QmzW"
      },
      "source": [
        "# !pip install -U sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzgCrnTGzNYv"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tv9jG1yzT5m"
      },
      "source": [
        "SEED = 0\n",
        "DEVICE = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def select_field(feature_list, field_name):\n",
        "    return [\n",
        "        [choice[field_name] for choice in feature.choices_features]\n",
        "        for feature in feature_list\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_model(model='all'):\n",
        "    if model == 'bert':\n",
        "        return BertConfig, BertForSequenceClassification, BertTokenizer\n",
        "    elif model == 'xlnet':\n",
        "        return XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer\n",
        "    elif model == 'roberta':\n",
        "        return RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n",
        "    elif model == 'gpt2':\n",
        "        raise NotImplemented\n",
        "        # return GPT2Config, AutoModelForQuestionAnswering, GPT2Tokenizer\n",
        "    raise NotImplemented\n",
        "\n",
        "\n",
        "def load_optimizer(args, model, train_size, learning_rate):\n",
        "    num_training_steps = train_size * args.num_train_epochs\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters, lr=learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "    return model, optimizer, scheduler\n",
        "\n",
        "\n",
        "def freeze(model, model_name):\n",
        "    if model_name == 'bert':\n",
        "        for param in model.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif model_name == 'xlnet':\n",
        "        for param in model.xlnet.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif model_name == 'roberta':\n",
        "        for param in model.roberta.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif model_name == 'gpt2':\n",
        "        for param in model.gpt2.parameters():\n",
        "            param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_accuracy(preds, labels, lengths=None, inspect=60):\n",
        "    if inspect:\n",
        "        print('\\nThe first {} predictions'.format(inspect), preds[0:inspect])\n",
        "    if lengths is None:\n",
        "        return (preds == labels).mean()\n",
        "    \n",
        "    \n",
        "    i, correct = 0, 0\n",
        "    for n in lengths:\n",
        "        pred, label = 1, 1\n",
        "        for p in preds[i:i + n]:\n",
        "            pred *= p\n",
        "        for l in labels[i:i + n]:\n",
        "            label *= l\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "        i += n\n",
        "    return correct / len(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tABNFRGMKz6Y"
      },
      "source": [
        "# Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ofGNJGHKOK7"
      },
      "source": [
        "## Task Dataset\n",
        "\n",
        "EAT (Everyday Actions in Text) is from the SLED (Situated Language and Embodied Dialogue) group created by Shane Storks. The dataset is in the form of a story of 5 sequential events represented by natural language texts $\\{t_1,t_2,\\cdots,t_5\\}$ respectively. The model aims to identify whether the story is plausible using common sense reasoning and specify at which event the story becomes implausible, if any. Such plausible inference requires the model to have a strong background knowledge and a comprehensive ability to perform common sense reasoning and causal reasoning, since whether an event is plausible in the story is high dependent on the previous events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hOJK9v-KQo2"
      },
      "source": [
        "def load_data(dataset='commonsense_qa', preview=-1):\n",
        "\n",
        "    assert dataset in {'commonsense_qa', 'conv_entail', 'eat'}\n",
        "\n",
        "    if dataset == 'commonsense_qa':\n",
        "        ds = load_dataset('commonsense_qa')\n",
        "\n",
        "        if preview > 0:\n",
        "            print('\\nLoading an example...')\n",
        "            data_tr = ds.data['train']\n",
        "            question = data_tr['question']\n",
        "            choices = data_tr['choices']\n",
        "            answerKey = data_tr['answerKey']\n",
        "            print(question[preview])\n",
        "            for label, text in zip(choices[preview]['label'], choices[preview]['text']):\n",
        "                print(label, text)\n",
        "            print('Ans:', answerKey[preview])\n",
        "\n",
        "    elif dataset == 'conv_entail':\n",
        "        dev_file = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/Conversational_Entailment/dev_set.json'\n",
        "        act_file = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/Conversational_Entailment/act_tag.json'\n",
        "        dev_set = codecs.open(dev_file, 'r', encoding='utf-8').read()\n",
        "        act_tag = codecs.open(act_file, 'r', encoding='utf-8').read()\n",
        "        ds = json.loads(dev_set), json.loads(act_tag)\n",
        "\n",
        "        if preview > 0:\n",
        "            print('Preview not yet implemented for this dataset.')\n",
        "\n",
        "    else:\n",
        "        file_name = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/EAT/eat_train.json'\n",
        "        eat = codecs.open(file_name, 'r', encoding='utf-8').read()\n",
        "        ds = json.loads(eat)\n",
        "\n",
        "        if preview > 0:\n",
        "            print('\\nLoading an example...')\n",
        "            story = ds[preview]['story']\n",
        "            label = ds[preview]['label']\n",
        "            bp = ds[preview]['breakpoint']\n",
        "            for line in story:\n",
        "                print(line)\n",
        "            print(label)\n",
        "            print(bp)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def load_data_frame(preview=True, split='train'):\n",
        "    if split == 'train':\n",
        "      file_name = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/EAT/eat_train.json'\n",
        "    else:\n",
        "      file_name = '/content/drive/Shareddrives/EECS595-Fall2020/Final_Project_Common/EAT/eat_test_unlabeled.json'\n",
        "      \n",
        "    df = pd.read_json(file_name)\n",
        "    \n",
        "    if preview:\n",
        "        print(df.head())\n",
        "        print(len(df))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRiu77jjKfoH"
      },
      "source": [
        "Run the following code to preview the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Er-oLRB6dmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e708ab8-c2b3-4fac-d7fb-acff3d90df4c"
      },
      "source": [
        "ds = load_data(dataset='eat', preview=5)\n",
        "print('\\nDataset size:', len(ds))\n",
        "df = load_data_frame()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading an example...\n",
            "John grabbed the ladder and put it in his truck.\n",
            "John put a drill and rope in the bucket and also put that in this truck.\n",
            "John drove the bicycle to the neighbor's house.\n",
            "John picked up a few other things and put them near the bucket in the truck.\n",
            "John drove his truck to a neighbor's house to help them repair their storm-damaged shed.\n",
            "0\n",
            "3\n",
            "\n",
            "Dataset size: 1044\n",
            "                                               story  ...       id\n",
            "0  [Tom took out the cooked meat., Tom took out t...  ...  train_0\n",
            "1  [John went to the bathroom., John decided to t...  ...  train_1\n",
            "2  [Ann drank from her water bottle., Ann decided...  ...  train_2\n",
            "3  [Tom put on his shoes., Tom packed a suitcase....  ...  train_3\n",
            "4  [Ann sat down on the couch., Ann reached for t...  ...  train_4\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "1044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Vq_hBp7PNB"
      },
      "source": [
        "Get the length of all the messages in the train set to choose appropriate a padding length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "HMBZjpF27Jyg",
        "outputId": "6ea0db21-98fa-4094-cd76-c213d3fba6ef"
      },
      "source": [
        "df = load_data_frame(preview=False)\n",
        "df['text'] = df['story'].apply(lambda x: ' '.join(x).lower())\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(\n",
        "    df['text'], df['label'], random_state=SEED, test_size=0.15, stratify=df['label'])\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdb909fddd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIxGTEYezc1L"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS5CDydVzfAB"
      },
      "source": [
        "MAXLENGTH = 320\n",
        "\n",
        "def embed(df):\n",
        "    sent_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "    embedding = df['story'].apply(lambda x: sent_model.encode(x, convert_to_tensor=True, device=device))\n",
        "    return embedding\n",
        "\n",
        "\n",
        "class EATProcessor():\n",
        "    def __init__(self, tokenizer, args=None):\n",
        "        test_size = 0.15\n",
        "        if args is not None:\n",
        "            test_size = args.test_size\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.double_sep = 'roberta' in tokenizer.name_or_path\n",
        "        self.cls_token = tokenizer.cls_token\n",
        "        self.sep_token = tokenizer.sep_token\n",
        "\n",
        "        df = load_data_frame(preview=False)\n",
        "        self.train_story, self.val_story, self.train_bp, self.val_bp = train_test_split(\n",
        "            df['story'], df['breakpoint'], random_state=SEED, test_size=test_size, stratify=df['breakpoint'])\n",
        "        df_test = load_data_frame(preview=False, split='test')\n",
        "        self.test_story = df_test['story']\n",
        "        self.test_id = df_test['id']\n",
        "\n",
        "        \n",
        "    def truncate_seq_pair(self, token_a, token_b, max_len=MAXLENGTH-3):\n",
        "        \"\"\"\n",
        "        Truncates a sequence pair in place to the maximum length.\n",
        "\n",
        "        This is a simple heuristic which will always truncate the longer sequence one token at a time.\n",
        "        This makes more sense than truncating an equal percent of tokens from each,\n",
        "        since if one sequence is very short then each token that's truncated\n",
        "        likely contains more information than a longer sequence.\n",
        "\n",
        "        However, since we'd better not to remove tokens of options and questions,\n",
        "        you can choose to use a bigger length or only pop from context\n",
        "        \"\"\"\n",
        "\n",
        "        while True:\n",
        "            total_length = len(token_a) + len(token_b)\n",
        "            if total_length <= max_len:\n",
        "                break\n",
        "            if len(token_a) > len(token_b):\n",
        "                token_a.pop()\n",
        "            else:\n",
        "                warning = 'Attention! you are removing from token_b (swag task is ok). ' \\\n",
        "                          'If you are training ARC and RACE (you are popping question + options), ' \\\n",
        "                          'you need to try to use a bigger max seq length!'\n",
        "                print(warning)\n",
        "                token_b.pop()\n",
        "\n",
        "    def tokenize_and_encode(self, prev, next, max_len=MAXLENGTH):\n",
        "        \"\"\"\n",
        "        Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "        Max is 512 if using BERT-based models, higher for longformer (2000+)\n",
        "        \"\"\"\n",
        "\n",
        "        num_seps = 1\n",
        "        extra_tokens = 3\n",
        "        if self.double_sep:\n",
        "            num_seps += 1\n",
        "            extra_tokens += 1\n",
        "\n",
        "        token_a = self.tokenizer.tokenize(prev)\n",
        "        token_b = self.tokenizer.tokenize(next)\n",
        "        self.truncate_seq_pair(token_a, token_b, max_len-extra_tokens)\n",
        "\n",
        "        token_a = [self.cls_token] + token_a + ([self.sep_token] * num_seps)\n",
        "        token_type_ids = [0] * len(token_a)\n",
        "        token_b = token_b + [self.sep_token]\n",
        "        token_type_ids += [1] * len(token_b)\n",
        "        tokens = token_a + token_b\n",
        "\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        mask = [1] * len(input_ids)\n",
        "\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        input_ids += ([0] * padding_length)\n",
        "        mask += ([0] * padding_length)\n",
        "        token_type_ids += ([0] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_len\n",
        "        assert len(token_type_ids) == max_len\n",
        "        assert len(mask) == max_len\n",
        "            \n",
        "        return input_ids, token_type_ids, mask\n",
        "\n",
        "    def load_features(self, split='train'):\n",
        "        if split == 'train':\n",
        "            stories, bps = self.train_story, self.train_bp\n",
        "        elif split == 'val':\n",
        "            stories, bps = self.val_story, self.val_bp\n",
        "        else:\n",
        "            stories  = self.test_story\n",
        "            seqs, segs, masks, lengths = [], [], [], []\n",
        "            for story in stories:\n",
        "              lengths.append(len(story)-1)\n",
        "              for i in range(1, len(story)):\n",
        "                prev = ' '.join(story[:i])\n",
        "                next = story[i]\n",
        "                seq, seg, mask = self.tokenize_and_encode(prev, next)\n",
        "                seqs.append(seq)\n",
        "                segs.append(seg)\n",
        "                masks.append(mask)\n",
        "\n",
        "            seqs = torch.tensor(seqs)\n",
        "            segs = torch.tensor(segs)\n",
        "            masks = torch.tensor(masks)\n",
        "            dataset = TensorDataset(seqs, masks, segs)\n",
        "\n",
        "            return dataset, lengths, self.test_id\n",
        "\n",
        "\n",
        "        labels, seqs, segs, masks, lengths = [], [], [], [], []\n",
        "        for story, breakpoint in zip(stories, bps):\n",
        "\n",
        "            if breakpoint == -1:\n",
        "                lengths.append(len(story)-1)\n",
        "                for i in range(1, len(story)):\n",
        "                    prev = ' '.join(story[:i])\n",
        "                    next = story[i]\n",
        "                    seq, seg, mask = self.tokenize_and_encode(prev, next)\n",
        "                    seqs.append(seq)\n",
        "                    segs.append(seg)\n",
        "                    masks.append(mask)\n",
        "                    labels.append(1)\n",
        "            else:\n",
        "                lengths.append(breakpoint)\n",
        "                for i in range(1, breakpoint):\n",
        "                    prev = ' '.join(story[:i])\n",
        "                    next = story[i]\n",
        "                    seq, seg, mask = self.tokenize_and_encode(prev, next)\n",
        "                    seqs.append(seq)\n",
        "                    segs.append(seg)\n",
        "                    masks.append(mask)\n",
        "                    labels.append(1)\n",
        "                prev = ' '.join(story[:breakpoint])\n",
        "                next = story[breakpoint]\n",
        "                seq, seg, mask = self.tokenize_and_encode(prev, next)\n",
        "                seqs.append(seq)\n",
        "                segs.append(seg)\n",
        "                masks.append(mask)\n",
        "                labels.append(0)\n",
        "\n",
        "        labels = torch.tensor(labels)\n",
        "        seqs = torch.tensor(seqs)\n",
        "        segs = torch.tensor(segs)\n",
        "        masks = torch.tensor(masks)\n",
        "\n",
        "        dataset = TensorDataset(seqs, masks, segs, labels)\n",
        "\n",
        "        return dataset, lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ORg7HJqjWO"
      },
      "source": [
        "# tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# tokenizer_rbt = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "\n",
        "# processor_tmp = EATProcessor(tokenizer_bert)\n",
        "# prev = 'John grabbed the ladder and put it in his truck.'\n",
        "# next = 'John put a drill and rope in the bucket and also put that in this truck.'\n",
        "# seq, seg, mask = processor_tmp.tokenize_and_encode(prev, next)\n",
        "# print(prev, next)\n",
        "# print(seq)\n",
        "# print(seg)\n",
        "# print(mask)\n",
        "# print(len(seq), len(seg), len(mask))\n",
        "\n",
        "# processor_tmp = EATProcessor(tokenizer_rbt)\n",
        "# prev = 'John grabbed the ladder and put it in his truck.'\n",
        "# next = 'John put a drill and rope in the bucket and also put that in this truck.'\n",
        "# seq, seg, mask = processor_tmp.tokenize_and_encode(prev, next)\n",
        "# print(prev, next)\n",
        "# print(seq)\n",
        "# print(seg)\n",
        "# print(mask)\n",
        "# print(len(seq), len(seg), len(mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy237KUt4bfZ"
      },
      "source": [
        "## Knowledge Dataset\n",
        "\n",
        "Situations with Adversarial Generations ([SWAG](https://rowanzellers.com/swag/)) from Zellers et al. is a benchmark dataset of about 113,000 beginnings of small texts each with four possible endings [2]. Given the context each text provides, systems decide which of the four endings is most plausible in\n",
        "a task referred to as commonsense NLI.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApoNGj8A4gYY"
      },
      "source": [
        "# !wget https://storage.googleapis.com/ai2-mosaic/public/physicaliqa/physicaliqa-train-dev.zip\n",
        "# !unzip physicaliqa-train-dev.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScP8KsAt5Mf0"
      },
      "source": [
        "# def load_piqa(preview=False):\n",
        "    \n",
        "#     file_name = 'physicaliqa-train-dev/train.jsonl'\n",
        "#     df = pd.read_json(path_or_buf=file_name, lines=True)\n",
        "#     with open('physicaliqa-train-dev/train-labels.lst') as f:\n",
        "#       labels = [int(i) for i in list(f.read().splitlines())]\n",
        "#     df['labels'] = labels\n",
        "\n",
        "#     if preview:\n",
        "#         print(len(df))\n",
        "#         row = df.iloc[[preview]]\n",
        "#         print(row['goal'])\n",
        "#         print(row['sol1'])\n",
        "#         print(row['sol2'])\n",
        "#         print(row['labels'])\n",
        "#     return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdAjdixUfq9b"
      },
      "source": [
        "# df = load_piqa(preview=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXZR0GqLb0n3"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxw1c_kc5pWY"
      },
      "source": [
        "# class PIQAProcessor():\n",
        "#     \"\"\"\n",
        "#     Load the processed SWAG dataset\n",
        "#     max(size) = 73546\n",
        "#     \"\"\"\n",
        "#     def __init__(self, tokenizer, args=None):\n",
        "#         self.test_size = 0.15\n",
        "#         if args is not None:\n",
        "#             self.test_size = args.test_size\n",
        "\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.double_sep = 'roberta' in tokenizer.name_or_path\n",
        "#         self.cls_token = tokenizer.cls_token\n",
        "#         self.sep_token = tokenizer.sep_token\n",
        "\n",
        "#         self.df = load_piqa()\n",
        "    \n",
        "#     def truncate_seq_pair(self, token_a, token_b, max_len=MAXLENGTH-3):\n",
        "#         \"\"\"\n",
        "#         Truncates a sequence pair in place to the maximum length.\n",
        "\n",
        "#         This is a simple heuristic which will always truncate the longer sequence one token at a time.\n",
        "#         This makes more sense than truncating an equal percent of tokens from each,\n",
        "#         since if one sequence is very short then each token that's truncated\n",
        "#         likely contains more information than a longer sequence.\n",
        "\n",
        "#         However, since we'd better not to remove tokens of options and questions,\n",
        "#         you can choose to use a bigger length or only pop from context\n",
        "#         \"\"\"\n",
        "\n",
        "#         while True:\n",
        "#             total_length = len(token_a) + len(token_b)\n",
        "#             if total_length <= max_len:\n",
        "#                 break\n",
        "#             if len(token_a) > len(token_b):\n",
        "#                 token_a.pop()\n",
        "#             else:\n",
        "#                 warning = 'Attention! you are removing from token_b (swag task is ok). ' \\\n",
        "#                           'If you are training ARC and RACE (you are popping question + options), ' \\\n",
        "#                           'you need to try to use a bigger max seq length!'\n",
        "#                 print(warning)\n",
        "#                 token_b.pop()\n",
        "\n",
        "#     def tokenize_and_encode(self, prev, next, max_len=MAXLENGTH):\n",
        "#         \"\"\"\n",
        "#         Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "#         Max is 512 if using BERT-based models, higher for longformer (2000+)\n",
        "#         \"\"\"\n",
        "\n",
        "#         num_seps = 1\n",
        "#         extra_tokens = 3\n",
        "#         if self.double_sep:\n",
        "#             num_seps += 1\n",
        "#             extra_tokens += 1\n",
        "\n",
        "#         token_a = self.tokenizer.tokenize(prev)\n",
        "#         token_b = self.tokenizer.tokenize(next)\n",
        "#         self.truncate_seq_pair(token_a, token_b, max_len-extra_tokens)\n",
        "\n",
        "#         token_a = [self.cls_token] + token_a + ([self.sep_token] * num_seps)  \n",
        "#         token_type_ids = [0] * len(token_a)\n",
        "#         token_b = token_b + [self.sep_token]\n",
        "#         token_type_ids += [1] * len(token_b)\n",
        "#         tokens = token_a + token_b\n",
        "\n",
        "#         input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "#         mask = [1] * len(input_ids)\n",
        "\n",
        "#         padding_length = max_len - len(input_ids)\n",
        "#         input_ids += ([0] * padding_length)\n",
        "#         mask += ([0] * padding_length)\n",
        "#         token_type_ids += ([0] * padding_length)\n",
        "\n",
        "#         assert len(input_ids) == max_len\n",
        "#         assert len(token_type_ids) == max_len\n",
        "#         assert len(mask) == max_len\n",
        "            \n",
        "#         return input_ids, token_type_ids, mask\n",
        "\n",
        "#     def load_features(self, size=16113):\n",
        "\n",
        "#         print(\"Creating features from dataset...\")\n",
        "#         labels, seqs, segs, masks = [], [], [], []\n",
        "#         for row in df.iterrows():\n",
        "\n",
        "#             label = row[1]['labels']\n",
        "#             goal = row[1]['goal']\n",
        "#             sol1 = row[1]['sol1']\n",
        "#             sol2 = row[1]['sol2']\n",
        "\n",
        "#             if label == 0:\n",
        "#                 seq, seg, mask = self.tokenize_and_encode(goal, sol1)\n",
        "#                 seqs.append(seq)\n",
        "#                 segs.append(seg)\n",
        "#                 masks.append(mask)\n",
        "#                 labels.append(1)\n",
        "\n",
        "#                 seq, seg, mask = self.tokenize_and_encode(goal, sol2)\n",
        "#                 seqs.append(seq)\n",
        "#                 segs.append(seg)\n",
        "#                 masks.append(mask)\n",
        "#                 labels.append(0)\n",
        "#             else:\n",
        "#                 seq, seg, mask = self.tokenize_and_encode(goal, sol1)\n",
        "#                 seqs.append(seq)\n",
        "#                 segs.append(seg)\n",
        "#                 masks.append(mask)\n",
        "#                 labels.append(0)\n",
        "\n",
        "#                 seq, seg, mask = self.tokenize_and_encode(goal, sol2)\n",
        "#                 seqs.append(seq)\n",
        "#                 segs.append(seg)\n",
        "#                 masks.append(mask)\n",
        "#                 labels.append(1)\n",
        "                        \n",
        "#             size -= 1\n",
        "#             if size == 0:\n",
        "#                 break\n",
        "\n",
        "#         train_seq, val_seq, train_seg, val_seg, train_mask, val_mask, train_labels, val_labels = train_test_split(\n",
        "#             seqs, segs, masks, labels, random_state=SEED, test_size=self.test_size, stratify=labels)\n",
        "        \n",
        "#         train_seq = torch.tensor(train_seq)\n",
        "#         val_seq = torch.tensor(val_seq)\n",
        "#         train_seg = torch.tensor(train_seg)\n",
        "#         val_seg = torch.tensor(val_seg)\n",
        "#         train_mask = torch.tensor(train_mask)\n",
        "#         val_mask = torch.tensor(val_mask)\n",
        "#         train_labels = torch.tensor(train_labels)\n",
        "#         train_y = torch.tensor(train_labels)\n",
        "#         val_y = torch.tensor(val_labels)\n",
        "\n",
        "#         train_data = TensorDataset(train_seq, train_mask, train_seg, train_y)\n",
        "#         val_data = TensorDataset(val_seq, val_mask, val_seg, val_y)\n",
        "\n",
        "#         return train_data, val_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92jFJkifK6Cf"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ0BLRx-zmep"
      },
      "source": [
        "## Pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJKT-_9boL-c"
      },
      "source": [
        "Self-designed model for flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew2T52OZq0tD"
      },
      "source": [
        "class MyBertClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=1, dropout_rate=0.2, freeze=False):\n",
        "        super(MyBertClassifier, self).__init__()\n",
        "        \n",
        "        if num_labels == 1:\n",
        "            self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            self.loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(self.num_embed(model_name), 64)\n",
        "        self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(64, num_labels)\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def num_embed(self, model_name):\n",
        "        embedding_size = {\n",
        "            'bert-base-uncased': 768,\n",
        "            'bert-large-uncased': 1024\n",
        "        }\n",
        "        return embedding_size[model_name]\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
        "\n",
        "        out = self.bert(input_ids=input_ids, \n",
        "                        attention_mask=attention_mask, \n",
        "                        token_type_ids=token_type_ids)\n",
        "        x = out.pooler_output\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # x = np.argmax(x, axis=1)\n",
        "            loss = self.loss_fct(x.flatten(), labels.float())\n",
        "        \n",
        "        return loss, x\n",
        "\n",
        "\n",
        "class MyRobertaClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, model_name='roberta-base', num_labels=2, dropout_rate=0.2, freeze=False):\n",
        "        super(MyRobertaClassifier, self).__init__()\n",
        "        \n",
        "        if num_labels == 1:\n",
        "            self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            self.loss_fct = torch.nn.CrossEntropyLoss(torch.tensor([0.9,0.1],dtype=torch.float).to(DEVICE))\n",
        "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(self.num_embed(model_name), 64)\n",
        "        self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(64, num_labels)\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.roberta.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def num_embed(self, model_name):\n",
        "        embedding_size = {\n",
        "            'roberta-base': 768,\n",
        "            'roberta-large': 1024,\n",
        "        }\n",
        "        return embedding_size[model_name]\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
        "\n",
        "        out = self.roberta(input_ids=input_ids, \n",
        "                           attention_mask=attention_mask, \n",
        "                           token_type_ids=None)\n",
        "        x = out.pooler_output\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # x = np.argmax(x, axis=1)\n",
        "            loss = self.loss_fct(x, labels)\n",
        "        \n",
        "        return loss, x\n",
        "\n",
        "\n",
        "class myRoberta_mnliClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_model, num_labels=2, freeze=False):\n",
        "        super(myRoberta_mnliClassifier, self).__init__()\n",
        "        \n",
        "        if num_labels == 1:\n",
        "            self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            self.loss_fct = torch.nn.NLLLoss(torch.tensor([0.7,0.3],dtype=torch.float).to(DEVICE))\n",
        "        self.roberta = pretrained_model\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.roberta.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
        "\n",
        "        out = self.roberta(input_ids=input_ids, \n",
        "                           attention_mask=attention_mask, \n",
        "                           token_type_ids=None,\n",
        "                           labels=labels)\n",
        "        if labels is not None:\n",
        "            x = out[1]\n",
        "            x = self.softmax(x)\n",
        "            x = torch.stack((x[:,0],1-x[:,0]),dim=1)\n",
        "            x = torch.log(x)\n",
        "\n",
        "            loss = None\n",
        "            if labels is not None:\n",
        "                # x = np.argmax(x, axis=1)\n",
        "                loss = self.loss_fct(x, labels)\n",
        "            return loss, x\n",
        "\n",
        "        else:\n",
        "            x = out.logits\n",
        "            x = self.softmax(x)\n",
        "            x = torch.stack((x[:,0],1-x[:,0]),dim=1)\n",
        "            x = torch.log(x)\n",
        "            return x\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWb1PKeZztGS"
      },
      "source": [
        "## Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Sn_8byoIR2"
      },
      "source": [
        "Training and evaluation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3xQdSF8zl1q"
      },
      "source": [
        "def fine_tune(args, model, tokenizer):\n",
        "\n",
        "    processor = PIQAProcessor(tokenizer, args)\n",
        "    dataset_tr, dataset_val = processor.load_features(size=args.pretrain_size)\n",
        "\n",
        "    print('\\n Loading training dataset')\n",
        "    sampler_tr = RandomSampler(dataset_tr)\n",
        "    dataloader_tr = DataLoader(dataset_tr, sampler=sampler_tr, batch_size=args.batch_size)\n",
        "\n",
        "    print('\\n Loading validation dataset')\n",
        "    sampler_val = SequentialSampler(dataset_val)\n",
        "    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n",
        "\n",
        "    model, optimizer, scheduler = load_optimizer(args, model, len(dataloader_tr), args.learning_rate_tune)\n",
        "\n",
        "    tr_loss = 0.00\n",
        "    num_steps = 0\n",
        "\n",
        "    model.train()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=False, leave=True, position=1)\n",
        "\n",
        "    for _ in train_iterator:\n",
        "        disable = False\n",
        "        if len(dataloader_tr) > 1000:\n",
        "            disable = True\n",
        "        epoch_iterator = tqdm(dataloader_tr, desc=\"Iteration\", disable=disable, leave=True, position=1)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            batch = tuple(b.to(args.device) for b in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n",
        "                      'labels': batch[3]}\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            num_steps += 1\n",
        "\n",
        "            if args.logging_steps_tune > 0 and num_steps % args.logging_steps_tune == 0:\n",
        "                results, tn, fn = evaluate(args, model, dataloader_val)\n",
        "                print(\"\\n val acc: {}, val loss: {}\"\n",
        "                      .format(str(results['val_acc']), str(results['val_loss'])))\n",
        "\n",
        "    loss = tr_loss / num_steps\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(args, model, tokenizer):\n",
        "    train_epoch = 1\n",
        "    \n",
        "    processor = EATProcessor(tokenizer, args)\n",
        "    dataset_tr, lengths_tr = processor.load_features(split='train')\n",
        "    dataset_val, lengths_val = processor.load_features(split='val')\n",
        "\n",
        "    print('\\n Loading training dataset')\n",
        "    sampler_tr = RandomSampler(dataset_tr)\n",
        "    dataloader_tr = DataLoader(dataset_tr, sampler=sampler_tr, batch_size=args.batch_size)\n",
        "\n",
        "    print('\\n Loading validation dataset')\n",
        "    sampler_val = SequentialSampler(dataset_val)\n",
        "    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n",
        "\n",
        "    num_steps = 0\n",
        "    best_steps = 0\n",
        "    tr_loss = 0.0\n",
        "    best_val_acc, best_val_loss = 0.0, 99999999999.0\n",
        "    best_model = None\n",
        "\n",
        "    _, optimizer, scheduler = load_optimizer(args, model, len(dataloader_tr), args.learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    train_iterator = trange(int(train_epoch), desc=\"Epoch\", disable=False, leave=True, position=1)\n",
        "\n",
        "    for _ in train_iterator:\n",
        "\n",
        "        epoch_iterator = tqdm(dataloader_tr, desc=\"Iteration\", disable=False, leave=True, position=1)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            batch = tuple(b.to(args.device) for b in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n",
        "                      'labels': batch[3]}\n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            \n",
        "            loss = outputs[0]\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            num_steps += 1\n",
        "\n",
        "            if args.logging_steps > 0 and num_steps % args.logging_steps == 0:\n",
        "                results, tn, fn = evaluate(args, model, dataloader_val, lengths_val)\n",
        "                \n",
        "                print(\"\\n how many 0 is predicted 0: {}, how many 1 is predicted 0: {}, val acc: {}, val loss: {}\"\n",
        "                      .format(str(tn), str(fn), str(results['val_acc']), str(results['val_loss'])))\n",
        "                if results[\"val_loss\"] < best_val_loss:\n",
        "                    best_val_acc, best_val_loss = results[\"val_acc\"], results[\"val_loss\"]\n",
        "                    best_steps = num_steps\n",
        "                    best_model = deepcopy(model)\n",
        "\n",
        "    loss = tr_loss / num_steps\n",
        "    results, tn, fn = evaluate(args, model, dataloader_tr, lengths_tr)\n",
        "    print(\"\\n how many 0 is predicted 0: {}, how many 1 is predicted 0: {}, val acc: {}, val loss: {}\"\n",
        "                      .format(str(tn), str(fn), str(results['val_acc']), str(results['val_loss'])))\n",
        "    print(\"\\n Best val acc: {}, Best val loss: {}\".format(best_val_acc, best_val_loss))\n",
        "\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def evaluate(args, model, dataloader, lengths=None):\n",
        "\n",
        "    val_loss = 0.0\n",
        "    num_steps = 0\n",
        "    preds, labels = None, None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Validation\", disable=True, leave=True, position=1):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None,\n",
        "                      'labels': batch[3]}\n",
        "            \n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs[1]\n",
        "\n",
        "            loss = outputs[0]\n",
        " \n",
        "            val_loss += loss.mean().item()\n",
        "\n",
        "        num_steps += 1\n",
        "\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            labels = inputs['labels'].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    loss = val_loss / num_steps\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    preds[preds==2] = 1\n",
        "    acc = get_accuracy(preds, labels, lengths)\n",
        "    result = {\"val_acc\": acc, \"val_loss\": loss}\n",
        "    results.update(result)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds, normalize='true').ravel()\n",
        "    # classification_report(labels, preds)\n",
        "\n",
        "\n",
        "    return results, tn, fn\n",
        "    # how many 0 is truely predicted 0:tn\n",
        "    # how many 1 is falsely predicted 0:fn\n",
        "\n",
        "\n",
        "def test(args, tokenizer, model):\n",
        "\n",
        "    processor = EATProcessor(tokenizer, args)\n",
        "    dataset_val, lengths_val = processor.load_features(split='val')\n",
        "    sampler_val = SequentialSampler(dataset_val)\n",
        "    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n",
        "    results, tn, fn = evaluate(args, model, dataloader_val, lengths_val)\n",
        "    \n",
        "    print(\"\\n how many 0 is predicted 0: {}, how many 1 is predicted 0: {}, val acc: {}, val loss: {}\"\n",
        "                      .format(str(tn), str(fn), str(results['val_acc']), str(results['val_loss'])))\n",
        "\n",
        "def return_story_label_bp(preds, lengths):\n",
        "    my_bp = []\n",
        "    story_label = []\n",
        "    i = 0\n",
        "    for n in lengths:\n",
        "        pred = 1\n",
        "        breakpoint = -1\n",
        "        count = 0\n",
        "        for p in preds[i:i + n]:\n",
        "            count += 1\n",
        "            if p==0:\n",
        "                breakpoint = count\n",
        "                pred = 0\n",
        "                break\n",
        "        my_bp.append(breakpoint)\n",
        "        story_label.append(pred)\n",
        "        i += n\n",
        "    return story_label, my_bp\n",
        "\n",
        "def test_on_test_dataset(args, tokenizer, model):\n",
        "\n",
        "    processor = EATProcessor(tokenizer, args)\n",
        "    dataset_test, lengths_test, test_id = processor.load_features(split='test')\n",
        "    sampler_test = SequentialSampler(dataset_test)\n",
        "    dataloader_test = DataLoader(dataset_test, sampler=sampler_test, batch_size=args.batch_size)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    num_steps = 0\n",
        "    preds, labels = None, None\n",
        "    results = {}\n",
        "\n",
        "    for batch in tqdm(dataloader_test, desc=\"Validation\", disable=True, leave=True, position=1):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet'] else None}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs\n",
        "\n",
        "        num_steps += 1\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    preds[preds==2] = 1\n",
        "    story_label, my_bp = return_story_label_bp(preds, lengths_test)\n",
        "\n",
        "    print(preds)\n",
        "    print(lengths_test)\n",
        "\n",
        "    print(story_label)\n",
        "    print(my_bp)\n",
        "    \n",
        "    out = [{\"id\": test_id[i], \"pred_label\":story_label[i], \"pred_breakpoint\": my_bp[i]} for i in range(len(my_bp))]\n",
        "    out_file = open(\"/content/drive/MyDrive/Colab Notebooks/eecs595/eat/EAT_team2_preds.json\", 'w')\n",
        "    json.dump(out, out_file)\n",
        "    out_file.close()\n",
        "    \n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/eecs595/eat/myfile.pkl\", \"wb\") as f:\n",
        "      pickle.dump([preds,lengths_test,story_label,my_bp], f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyv6xGVDhfEZ"
      },
      "source": [
        "my_args = None\n",
        "my_tokenizer = None\n",
        "\n",
        "def main(args):\n",
        "    print('Using device', args.device)\n",
        "    print('Using model', args.model_type)\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    num_labels = 2\n",
        "\n",
        "    config = AutoConfig.from_pretrained(args.model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "    \n",
        "    my_args = args\n",
        "    my_tokenizer = tokenizer\n",
        "\n",
        "    pretrained_model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
        "    # model = MyBertClassifier(model_name=args.config_name, num_labels=1)\n",
        "    # model = MyRobertaClassifier()\n",
        "    model = myRoberta_mnliClassifier(pretrained_model)\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    # print('\\nTuning...')\n",
        "    #model = fine_tune(args, model, tokenizer)\n",
        "    # model = freeze(model, args.model_name)\n",
        "\n",
        "    print('\\nTraining...')\n",
        "    best_model = train(args, model, tokenizer)\n",
        "    #torch.save(best_model.state_dict(), args.model_name+\"-eat.torch\")\n",
        "\n",
        "    print('\\nTesting...')\n",
        "    test_on_test_dataset(args, tokenizer, best_model)\n",
        "\n",
        "    return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGOc8j4Hz7OC"
      },
      "source": [
        "The default model is based on `bert`, and\n",
        "```\n",
        "parser.add_argument(\"--model_name\", type=str, default='bert-base-uncased',\n",
        "                    help=\"Path to pre-trained model or shortcut name. See https://huggingface.co/models\")\n",
        "```\n",
        "\n",
        "This would leads to\n",
        "\n",
        "```\n",
        "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
        "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
        "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Using custom data configuration default\n",
        "```\n",
        "\n",
        "Should check https://huggingface.co/models for other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfotIM0w1oX-"
      },
      "source": [
        "Setup parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_F0_crozIjr"
      },
      "source": [
        "def run(model_type='bert',\n",
        "        model_name='bert-base-uncased',\n",
        "        task_name=None,\n",
        "        batch_size=64,\n",
        "        lr=1e-5,\n",
        "        lr_tune=1e-5,\n",
        "        epochs=1,\n",
        "        pretrain_size=1000,\n",
        "        logging_steps=50,\n",
        "        logging_steps_tune=200):\n",
        "  \n",
        "    parser = argparse.ArgumentParser(description=\"Common sense question answering\")\n",
        "    parser.add_argument(\"--model_type\", type=str, default=model_type,\n",
        "                        help=\"Model: <str> [ bert | xlnet | roberta | gpt2 ]\")\n",
        "    parser.add_argument(\"--task_name\", default=task_name, type=str, required=False,\n",
        "                        help=\"The name of the task to train: <str> [ commonqa ]\")\n",
        "    parser.add_argument(\"--model_name\", type=str,\n",
        "                        default=model_name,\n",
        "                        help=\"Path to pre-trained model or shortcut name.\"\n",
        "                              \"See https://huggingface.co/models\")\n",
        "    parser.add_argument(\"--config_name\", type=str,\n",
        "                        default=model_name,\n",
        "                        help=\"Pre-trained config name or path\")\n",
        "    parser.add_argument(\"--tokenizer_name\", default=model_name, type=str,\n",
        "                        help=\"Pre-trained tokenizer name or path if not the same as model_name\")\n",
        "\n",
        "    parser.add_argument(\"--max_seq_length\", default=100, type=int,\n",
        "                        help=\"The maximum total input sequence length after tokenization. \"\n",
        "                                \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
        "    parser.add_argument(\"--batch_size\", default=batch_size, type=int,\n",
        "                        help=\"Batch size for training.\")\n",
        "\n",
        "    parser.add_argument(\"--learning_rate\", default=lr, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--learning_rate_tune\", default=lr_tune, type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "                        help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--test_size\", default=0.2, type=float,\n",
        "                        help=\"The ratio of size of validation set\")\n",
        "    parser.add_argument(\"--pretrain_size\", default=pretrain_size, type=float,\n",
        "                        help=\"The ratio of size of validation set\")\n",
        "        \n",
        "    parser.add_argument(\"--num_train_epochs\", default=epochs, type=int,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--warmup_steps\", default=epochs//6+2, type=int,\n",
        "                        help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument('--logging_steps', type=int, default=logging_steps,\n",
        "                        help=\"Log every n updates steps.\")\n",
        "    parser.add_argument('--logging_steps_tune', type=int, default=logging_steps_tune,\n",
        "                        help=\"Log every n updates steps.\")\n",
        "\n",
        "    parser.add_argument('--fp16', type=bool, default=True,\n",
        "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "    parser.add_argument('--opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                              \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed: <int>\")\n",
        "    parser.add_argument(\"--device\", default=DEVICE)\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    best_model = main(args)\n",
        "    return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmE312LRWvvR"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GJvvByG0O-j"
      },
      "source": [
        "### RoBerta\n",
        "\n",
        "Check this for `</s>` rules: https://github.com/pytorch/fairseq/issues/1654"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rprO1FhzIJj"
      },
      "source": [
        "Try `roberta-large-mnli`, but this one has a `neutral` label (`num_labels = 3`).\n",
        "\n",
        "There is an [online entailment](https://huggingface.co/roberta-large-mnli?text=John+grabbed+the+ladder+and+put+it+in+his+truck.+%3C%2Fs%3E%3C%2Fs%3E+John+put+a+drill+and+rope+in+the+bucket+and+also+put+that+in+this+truck) model to play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A29iZ1dduQHK"
      },
      "source": [
        "# Try roberta-large-mnli\n",
        "model_type = 'roberta'\n",
        "model_name = 'roberta-large-mnli'\n",
        "task_name = 'mnli'\n",
        "batch_size = 8\n",
        "epochs = 1\n",
        "lr_tune = 1e-6\n",
        "lr = 5e-6\n",
        "# pretrain_size = 73546\n",
        "pretrain_size = 1000\n",
        "logging_steps = 100\n",
        "logging_steps_tune = 25\n",
        "\n",
        "best_model = run(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc9XjPVaA-lR"
      },
      "source": [
        "# def arg_generator(model_type='bert',\n",
        "#         model_name='bert-base-uncased',\n",
        "#         task_name=None,\n",
        "#         batch_size=64,\n",
        "#         lr=1e-5,\n",
        "#         lr_tune=1e-5,\n",
        "#         epochs=1,\n",
        "#         pretrain_size=1000,\n",
        "#         logging_steps=50,\n",
        "#         logging_steps_tune=200):\n",
        "#     parser = argparse.ArgumentParser(description=\"Common sense question answering\")\n",
        "#     parser.add_argument(\"--model_type\", type=str, default=model_type,\n",
        "#                         help=\"Model: <str> [ bert | xlnet | roberta | gpt2 ]\")\n",
        "#     parser.add_argument(\"--task_name\", default=task_name, type=str, required=False,\n",
        "#                         help=\"The name of the task to train: <str> [ commonqa ]\")\n",
        "#     parser.add_argument(\"--model_name\", type=str,\n",
        "#                         default=model_name,\n",
        "#                         help=\"Path to pre-trained model or shortcut name.\"\n",
        "#                               \"See https://huggingface.co/models\")\n",
        "#     parser.add_argument(\"--config_name\", type=str,\n",
        "#                         default=model_name,\n",
        "#                         help=\"Pre-trained config name or path\")\n",
        "#     parser.add_argument(\"--tokenizer_name\", default=model_name, type=str,\n",
        "#                         help=\"Pre-trained tokenizer name or path if not the same as model_name\")\n",
        "\n",
        "#     parser.add_argument(\"--max_seq_length\", default=100, type=int,\n",
        "#                         help=\"The maximum total input sequence length after tokenization. \"\n",
        "#                                 \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
        "#     parser.add_argument(\"--batch_size\", default=batch_size, type=int,\n",
        "#                         help=\"Batch size for training.\")\n",
        "\n",
        "#     parser.add_argument(\"--learning_rate\", default=lr, type=float,\n",
        "#                         help=\"The initial learning rate for Adam.\")\n",
        "#     parser.add_argument(\"--learning_rate_tune\", default=lr_tune, type=float,\n",
        "#                         help=\"The initial learning rate for Adam.\")\n",
        "#     parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
        "#                         help=\"Weight decay if we apply some.\")\n",
        "#     parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "#                         help=\"Epsilon for Adam optimizer.\")\n",
        "#     parser.add_argument(\"--test_size\", default=0.2, type=float,\n",
        "#                         help=\"The ratio of size of validation set\")\n",
        "#     parser.add_argument(\"--pretrain_size\", default=pretrain_size, type=float,\n",
        "#                         help=\"The ratio of size of validation set\")\n",
        "        \n",
        "#     parser.add_argument(\"--num_train_epochs\", default=epochs, type=int,\n",
        "#                         help=\"Total number of training epochs to perform.\")\n",
        "#     parser.add_argument(\"--warmup_steps\", default=epochs//6+2, type=int,\n",
        "#                         help=\"Linear warmup over warmup_steps.\")\n",
        "#     parser.add_argument('--logging_steps', type=int, default=logging_steps,\n",
        "#                         help=\"Log every n updates steps.\")\n",
        "#     parser.add_argument('--logging_steps_tune', type=int, default=logging_steps_tune,\n",
        "#                         help=\"Log every n updates steps.\")\n",
        "\n",
        "#     parser.add_argument('--fp16', type=bool, default=True,\n",
        "#                         help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
        "#     parser.add_argument('--opt_level', type=str, default='O1',\n",
        "#                         help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "#                               \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "\n",
        "#     parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed: <int>\")\n",
        "#     parser.add_argument(\"--device\", default=DEVICE)\n",
        "#     args, unknown = parser.parse_known_args()\n",
        "#     return args\n",
        "\n",
        "# PATH = '/content/drive/MyDrive/EECS595_final_project/EAT_new/roberta-large-mnli-eat.torch'\n",
        "\n",
        "# model_type = 'roberta'\n",
        "# model_name = 'roberta-large-mnli'\n",
        "# task_name = 'mnli'\n",
        "# batch_size = 8\n",
        "# epochs = 1\n",
        "# lr_tune = 1e-6\n",
        "# lr = 5e-6\n",
        "# # pretrain_size = 73546\n",
        "# pretrain_size = 1000\n",
        "# logging_steps = 100\n",
        "# logging_steps_tune = 25\n",
        "\n",
        "# args = arg_generator(model_type, model_name, task_name, batch_size, lr, lr_tune, epochs, pretrain_size, logging_steps, logging_steps_tune)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "\n",
        "# pretrained_model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
        "# model = myRoberta_mnliClassifier(pretrained_model)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.to(args.device)\n",
        "\n",
        "# test_on_test_dataset(args, tokenizer, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjCtLAvVUnGL"
      },
      "source": [
        "# processor = EATProcessor(tokenizer, args)\n",
        "# dataset_val, lengths_val = processor.load_features(split='val')\n",
        "\n",
        "# print('\\n Loading validation dataset')\n",
        "# sampler_val = SequentialSampler(dataset_val)\n",
        "# dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size)\n",
        "\n",
        "# results, tn, fn = evaluate(args, model, dataloader_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grA7K5Y-LGM5"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK7co7dm1QZN"
      },
      "source": [
        "## Validation Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8iClE231TEs"
      },
      "source": [
        "The validation accuracy without finetuning is listed as follows:\n",
        "* Bert:  54.1% (base), 50.3% (large)\n",
        "* XLNet: 51.0%\n",
        "* RoBerta: 50.3% (base), 51.0% (large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJDJ8zF9LH6y"
      },
      "source": [
        "## Todo\n",
        "\n",
        "A few things to do in the future:\n",
        "1. Try fine-tune with benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCTR69kyKAu1"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\n",
        "\n",
        "[2] Zellers, R., Bisk, Y., Schwartz, R., & Choi, Y. (2018). Swag: A large-scale adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326."
      ]
    }
  ]
}