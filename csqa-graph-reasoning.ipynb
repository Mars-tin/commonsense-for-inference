{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"csqa-graph-reasoning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"edbe73f1e2aa40adbe9fba8eba36ba06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f85b3b6cf097480794d13e9b5f1eef59","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_50a81f95d2bd44dfbf78bd6bcde5ef01","IPY_MODEL_7fc817b7d9c94afabb246a851d23f8dd"]}},"f85b3b6cf097480794d13e9b5f1eef59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"50a81f95d2bd44dfbf78bd6bcde5ef01":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c434ae19c1c4441eb0cbcf242e3eece0","_dom_classes":[],"description":"Downloading: ","_model_name":"FloatProgressModel","bar_style":"success","max":1586,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1586,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4db2da94883f4a29bc5eb5c0f76e2b70"}},"7fc817b7d9c94afabb246a851d23f8dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cb5a95c7b5c9451a89db90ac8a03c568","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.37k/? [00:03&lt;00:00, 1.22kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6be29e19788446ba89dfcb90474d8f33"}},"c434ae19c1c4441eb0cbcf242e3eece0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4db2da94883f4a29bc5eb5c0f76e2b70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb5a95c7b5c9451a89db90ac8a03c568":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6be29e19788446ba89dfcb90474d8f33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"693c4beb47e84570bf4c97647d2cbe4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a18df4a9cb7843d6888567d9237b8a2d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a797d03716a04cfa886ce64d6a77f96f","IPY_MODEL_516a1f0e13c84d0e82e1fd9ed80dd5fa"]}},"a18df4a9cb7843d6888567d9237b8a2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a797d03716a04cfa886ce64d6a77f96f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c6761a75f803462bb466116d77b90370","_dom_classes":[],"description":"Downloading: ","_model_name":"FloatProgressModel","bar_style":"success","max":1055,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1055,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aec2b9f26b06420fb6d4571bf1f6bd50"}},"516a1f0e13c84d0e82e1fd9ed80dd5fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6b84b239584b44bdb9592cde768f4377","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.31k/? [00:00&lt;00:00, 32.8kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_221609bc6c354ecfa88d0c4db4a5e15c"}},"c6761a75f803462bb466116d77b90370":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"aec2b9f26b06420fb6d4571bf1f6bd50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b84b239584b44bdb9592cde768f4377":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"221609bc6c354ecfa88d0c4db4a5e15c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc6e0e9558f54260a965fe0c2e7bd90e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3645674ebbf94018935941d6d2fc5032","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_525d9114df044f7896b362c05a1a77e4","IPY_MODEL_13be332e7fe74ee2a57d66c558c852c4"]}},"3645674ebbf94018935941d6d2fc5032":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"525d9114df044f7896b362c05a1a77e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_42a5a922f96d452ab3f0b0c90a020f80","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":3785890,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3785890,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce0e065ffe554fd5b264e2c82ea6030b"}},"13be332e7fe74ee2a57d66c558c852c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5e7cae342e104ddbae103ce12b5c2ea9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3.79M/3.79M [00:02&lt;00:00, 1.69MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9514dfd636ad42f0a1a8a93545b038d1"}},"42a5a922f96d452ab3f0b0c90a020f80":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ce0e065ffe554fd5b264e2c82ea6030b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e7cae342e104ddbae103ce12b5c2ea9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9514dfd636ad42f0a1a8a93545b038d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea8c6b7dcdef47c99fdb54b617a1770f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2a53bc1c988b4cb793821140883429cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7462a441a5444b2cbca6726efb0297b0","IPY_MODEL_13f48c6d36184797a7ce1e991db6a6a6"]}},"2a53bc1c988b4cb793821140883429cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7462a441a5444b2cbca6726efb0297b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_867ba5259c1f476cb954e4fea60458fb","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":423148,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":423148,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dbdbb204e916483680e964730afef4b0"}},"13f48c6d36184797a7ce1e991db6a6a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_60ef2aa02c6b471b97d4f00b43a35520","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 423k/423k [00:01&lt;00:00, 407kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ac3c7e7f04745389dae7e8662f5f381"}},"867ba5259c1f476cb954e4fea60458fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dbdbb204e916483680e964730afef4b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"60ef2aa02c6b471b97d4f00b43a35520":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0ac3c7e7f04745389dae7e8662f5f381":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b2b6657138d4a3e995ebb4439ac5680":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8553634968a1483d910ad42d83fec45d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e5f792ef74949718ddc3b7f0f60a276","IPY_MODEL_6d47f577967b495fb2ca89d4151cbb83"]}},"8553634968a1483d910ad42d83fec45d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e5f792ef74949718ddc3b7f0f60a276":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_26f0c2c5644647db880bc19afb380269","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":471653,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":471653,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6834669b52454a818b192af7c417804c"}},"6d47f577967b495fb2ca89d4151cbb83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_92167e8f37c84d62ae7d09da655d64c3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 472k/472k [00:00&lt;00:00, 755kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d58ea0b46d0482da6d16eebd355732a"}},"26f0c2c5644647db880bc19afb380269":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6834669b52454a818b192af7c417804c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"92167e8f37c84d62ae7d09da655d64c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7d58ea0b46d0482da6d16eebd355732a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43249d3952074726958a693b01280c3f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c7f9aa2754664c9c9391a85f2ab389e1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_354fe4c5f5564c438e4d6a213d752870","IPY_MODEL_22c7b84b01574da0b36df6993cf24389"]}},"c7f9aa2754664c9c9391a85f2ab389e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"354fe4c5f5564c438e4d6a213d752870":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a3c453209c0b439cb47e692aa32ab8ae","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_55c0dc3ed3844ca384936d39bce0dea2"}},"22c7b84b01574da0b36df6993cf24389":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f57f3846486496ebae4a0cd92aa0d10","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9741/0 [00:00&lt;00:00, 13961.72 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_db97be77d5244be2a54a5126e5c11323"}},"a3c453209c0b439cb47e692aa32ab8ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"55c0dc3ed3844ca384936d39bce0dea2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f57f3846486496ebae4a0cd92aa0d10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"db97be77d5244be2a54a5126e5c11323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a190c304055456a8dc524f502cb331a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c7fb074caeb9412b87ed3801b10429bb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7ba6140f2b4e46f997e67e53d1b0b264","IPY_MODEL_664d401c5bf34e0c9535b16731406527"]}},"c7fb074caeb9412b87ed3801b10429bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ba6140f2b4e46f997e67e53d1b0b264":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_49c4519e35f84f3fbafb8616245e6743","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_207e1bbb85854f0fa91ba0885cd03aa0"}},"664d401c5bf34e0c9535b16731406527":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1522a099d8354d5395304db40aa13cd6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1221/0 [00:00&lt;00:00, 11547.04 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2956b8c46d3b447e8d1a32ead9f52d58"}},"49c4519e35f84f3fbafb8616245e6743":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"207e1bbb85854f0fa91ba0885cd03aa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1522a099d8354d5395304db40aa13cd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2956b8c46d3b447e8d1a32ead9f52d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1c3e69f8889a4d1ca2e30f9f3509ccb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0215030738374f2d91acc4f42fd3b0b9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ac64e48de1514ab598afddfd851226cf","IPY_MODEL_6117a59aeef24f1d87a87368b77f1ed0"]}},"0215030738374f2d91acc4f42fd3b0b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac64e48de1514ab598afddfd851226cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2b1ca34e7bf44823b457dc8a927e15dc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ef319a40f524c30b814759d3b23fe1b"}},"6117a59aeef24f1d87a87368b77f1ed0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0694e6f9ae24e11b4d79bb3a8abe5a5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1140/0 [00:00&lt;00:00, 9849.63 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a2bc66b0c7da48539e0c53457ae94dd6"}},"2b1ca34e7bf44823b457dc8a927e15dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3ef319a40f524c30b814759d3b23fe1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0694e6f9ae24e11b4d79bb3a8abe5a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a2bc66b0c7da48539e0c53457ae94dd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"9eIUz6Xrk7Mn"},"source":["# Commonsense QA: Graph Attention Based Reasoning\n","\n","EECS 595 Final Project, Task 1: Commonsense QA\n","\n","* Team ID: 2\n","* Credit: Ziqiao Ma\n","* Last update: 2020.12.16"]},{"cell_type":"markdown","metadata":{"id":"j4o4bJEbNxhq"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"PrMYC3StmXpm"},"source":["## Colab setups"]},{"cell_type":"markdown","metadata":{"id":"2LeDbHL6oLFI"},"source":["Run this cell load the autoreload extension."]},{"cell_type":"code","metadata":{"id":"xUiA5dX3yFGX","executionInfo":{"status":"ok","timestamp":1608097708682,"user_tz":300,"elapsed":362,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"np_T5dXFoSrk"},"source":["Run the following cell to mount your Google Drive."]},{"cell_type":"code","metadata":{"id":"s36tQoeYVLA0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608097709080,"user_tz":300,"elapsed":751,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"b4b2ff82-5eeb-48d7-b2cf-10b1f9872f26"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4KMR3UxKoedX"},"source":["Fill in the Google Drive path where you uploaded the file."]},{"cell_type":"code","metadata":{"id":"JKGqTElTolf8","executionInfo":{"status":"ok","timestamp":1608097709081,"user_tz":300,"elapsed":747,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/eecs595/commonsense_qa'"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLAALYTAo25M"},"source":["Test if files are located."]},{"cell_type":"code","metadata":{"id":"9KVidViv0AO2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608097709502,"user_tz":300,"elapsed":1157,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"8765e5f9-e877-4225-ce46-7d84ddc39810"},"source":["import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['roberta_fairseq.ipynb', 'csqa-baseline.ipynb', 'csqa-new.ipynb', 'csqa-graph-reasoning.ipynb']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n4WOO9tvye13"},"source":["## Dependency installation"]},{"cell_type":"markdown","metadata":{"id":"MuqhqrBxn7G-"},"source":["Import libraries."]},{"cell_type":"code","metadata":{"id":"m03X-0vLyLM0","executionInfo":{"status":"ok","timestamp":1608097709503,"user_tz":300,"elapsed":1153,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["from __future__ import absolute_import\n","\n","import argparse\n","import os\n","import re\n","import random\n","import sys\n","from io import open\n","import csv\n","import json\n","from collections import defaultdict"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"hax3JeJtqnBf","executionInfo":{"status":"ok","timestamp":1608097709504,"user_tz":300,"elapsed":1150,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["import logging\n","from tqdm import tqdm, trange\n","from itertools import cycle\n","from pprint import pprint\n","\n","logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                    datefmt = '%m/%d/%Y %H:%M:%S', level = logging.INFO)\n","\n","logger = logging.getLogger(__name__)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"udsTogfgqvbQ","executionInfo":{"status":"ok","timestamp":1608097713049,"user_tz":300,"elapsed":4691,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["import numpy as np\n","import torch\n","from torch import nn\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","from torch.utils.data.distributed import DistributedSampler"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2OCyMtR0xMb"},"source":["Install `datasets`."]},{"cell_type":"code","metadata":{"id":"FI1b9vGk0tfK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608097726704,"user_tz":300,"elapsed":18336,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"af49b746-b2a2-4a45-9e64-8b814cbfa855"},"source":["!pip install datasets\n","from datasets import load_dataset"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n","\u001b[K     |████████████████████████████████| 163kB 8.1MB/s \n","\u001b[?25hCollecting pyarrow>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n","\u001b[K     |████████████████████████████████| 17.7MB 205kB/s \n","\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n","\u001b[K     |████████████████████████████████| 245kB 48.9MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: pyarrow, xxhash, datasets\n","  Found existing installation: pyarrow 0.14.1\n","    Uninstalling pyarrow-0.14.1:\n","      Successfully uninstalled pyarrow-0.14.1\n","Successfully installed datasets-1.1.3 pyarrow-2.0.0 xxhash-2.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gCt4UVaGNCgI"},"source":["Install `sentencepiece` for `XLNetTokenizer`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmfU0rSbNCzm","executionInfo":{"status":"ok","timestamp":1608097729567,"user_tz":300,"elapsed":21189,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"b896ab33-9b96-4d94-b5e1-3713899aac74"},"source":["!pip install sentencepiece\n","import sentencepiece"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 9.2MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.94\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aern7KsY01YS"},"source":["Install `transformers`"]},{"cell_type":"code","metadata":{"id":"5hTWa5DGyvQh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608097736472,"user_tz":300,"elapsed":28082,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"09eaff2f-bf2f-422c-822e-4f3dcd4ac2c9"},"source":["!pip install transformers\n","\n","from transformers import (AdamW, get_linear_schedule_with_warmup,\n","                          XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer)\n","\n","from transformers.models.xlnet.modeling_xlnet import XLNetLayer, XLNetPreTrainedModel\n","from transformers.modeling_utils import SequenceSummary"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 9.7MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 23.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 34.5MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=d86f5a45ec300faa03033258f260674dbbd8e0122cd779f930f3eaceded78714\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9V0UaKJFpJlj"},"source":["Define global helper functions."]},{"cell_type":"code","metadata":{"id":"eNXGcxTtpI6D","executionInfo":{"status":"ok","timestamp":1608097736481,"user_tz":300,"elapsed":28085,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def accuracy(out, labels):\n","    outputs = np.argmax(out, axis=1)\n","    return np.sum(outputs == labels)\n","\n","\n","def select_field(features, field):\n","    return [\n","        [\n","            choice[field]\n","            for choice in feature.choices_features\n","        ]\n","        for feature in features\n","    ]"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtyQ8cSzmedn"},"source":["# Benchmark"]},{"cell_type":"markdown","metadata":{"id":"b0nGiQZZmhSs"},"source":["## Dataset\n","\n","As a question answering benchmark, [Commonsense QA](https://arxiv.org/abs/1811.00937) presents a natural language question $Q$ of $m$ tokens $\\{q_1,q_2,\\cdots,q_m\\}$ and 5 choices $\\{a_1,a_2,\\cdots,a_5\\}$ labeled with $\\{A,B,\\cdots,E\\}$ regarding each question [1]. Notably, the questions do not entail a inference basis in themselves, so the lack of evidence requires the model to hold a comprehensive understanding on common sense knowledge and a strong reasoning ability to make the right choice."]},{"cell_type":"code","metadata":{"id":"4tv9jG1yzT5m","executionInfo":{"status":"ok","timestamp":1608097736483,"user_tz":300,"elapsed":28082,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def load_data(dataset='commonsense_qa', preview=-1):\n","\n","    assert dataset in {'commonsense_qa', 'conv_entail', 'eat'}\n","\n","    if dataset == 'commonsense_qa':\n","        ds = load_dataset('commonsense_qa')\n","\n","        if preview > 0:\n","            print('\\nLoading an example...')\n","            data_tr = ds.data['train']\n","            question = data_tr['question']\n","            choices = data_tr['choices']\n","            answerKey = data_tr['answerKey']\n","            print(question[preview])\n","            for label, text in zip(choices[preview]['label'], choices[preview]['text']):\n","                print(label, text)\n","            print('Ans:', answerKey[preview])\n","\n","    elif dataset == 'conv_entail':\n","        dev_set = codecs.open('data/conv_entail/dev_set.json', 'r', encoding='utf-8').read()\n","        act_tag = codecs.open('data/conv_entail/act_tag.json', 'r', encoding='utf-8').read()\n","        ds = json.loads(dev_set), json.loads(act_tag)\n","\n","        if preview > 0:\n","            print('Preview not yet implemented for this dataset.')\n","\n","    else:\n","        eat = codecs.open('data/eat/eat_train.json', 'r', encoding='utf-8').read()\n","        ds = json.loads(eat)\n","\n","        if preview > 0:\n","            print('\\nLoading an example...')\n","            story = ds[preview]['story']\n","            label = ds[preview]['label']\n","            bp = ds[preview]['breakpoint']\n","            for line in story:\n","                print(line)\n","            print(label)\n","            print(bp)\n","\n","    return ds"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZoC7xOeWm9gq"},"source":["Run the following code to preview the dataset:"]},{"cell_type":"code","metadata":{"id":"-Er-oLRB6dmw","colab":{"base_uri":"https://localhost:8080/","height":866,"referenced_widgets":["edbe73f1e2aa40adbe9fba8eba36ba06","f85b3b6cf097480794d13e9b5f1eef59","50a81f95d2bd44dfbf78bd6bcde5ef01","7fc817b7d9c94afabb246a851d23f8dd","c434ae19c1c4441eb0cbcf242e3eece0","4db2da94883f4a29bc5eb5c0f76e2b70","cb5a95c7b5c9451a89db90ac8a03c568","6be29e19788446ba89dfcb90474d8f33","693c4beb47e84570bf4c97647d2cbe4b","a18df4a9cb7843d6888567d9237b8a2d","a797d03716a04cfa886ce64d6a77f96f","516a1f0e13c84d0e82e1fd9ed80dd5fa","c6761a75f803462bb466116d77b90370","aec2b9f26b06420fb6d4571bf1f6bd50","6b84b239584b44bdb9592cde768f4377","221609bc6c354ecfa88d0c4db4a5e15c","cc6e0e9558f54260a965fe0c2e7bd90e","3645674ebbf94018935941d6d2fc5032","525d9114df044f7896b362c05a1a77e4","13be332e7fe74ee2a57d66c558c852c4","42a5a922f96d452ab3f0b0c90a020f80","ce0e065ffe554fd5b264e2c82ea6030b","5e7cae342e104ddbae103ce12b5c2ea9","9514dfd636ad42f0a1a8a93545b038d1","ea8c6b7dcdef47c99fdb54b617a1770f","2a53bc1c988b4cb793821140883429cd","7462a441a5444b2cbca6726efb0297b0","13f48c6d36184797a7ce1e991db6a6a6","867ba5259c1f476cb954e4fea60458fb","dbdbb204e916483680e964730afef4b0","60ef2aa02c6b471b97d4f00b43a35520","0ac3c7e7f04745389dae7e8662f5f381","1b2b6657138d4a3e995ebb4439ac5680","8553634968a1483d910ad42d83fec45d","2e5f792ef74949718ddc3b7f0f60a276","6d47f577967b495fb2ca89d4151cbb83","26f0c2c5644647db880bc19afb380269","6834669b52454a818b192af7c417804c","92167e8f37c84d62ae7d09da655d64c3","7d58ea0b46d0482da6d16eebd355732a","43249d3952074726958a693b01280c3f","c7f9aa2754664c9c9391a85f2ab389e1","354fe4c5f5564c438e4d6a213d752870","22c7b84b01574da0b36df6993cf24389","a3c453209c0b439cb47e692aa32ab8ae","55c0dc3ed3844ca384936d39bce0dea2","4f57f3846486496ebae4a0cd92aa0d10","db97be77d5244be2a54a5126e5c11323","6a190c304055456a8dc524f502cb331a","c7fb074caeb9412b87ed3801b10429bb","7ba6140f2b4e46f997e67e53d1b0b264","664d401c5bf34e0c9535b16731406527","49c4519e35f84f3fbafb8616245e6743","207e1bbb85854f0fa91ba0885cd03aa0","1522a099d8354d5395304db40aa13cd6","2956b8c46d3b447e8d1a32ead9f52d58","1c3e69f8889a4d1ca2e30f9f3509ccb3","0215030738374f2d91acc4f42fd3b0b9","ac64e48de1514ab598afddfd851226cf","6117a59aeef24f1d87a87368b77f1ed0","2b1ca34e7bf44823b457dc8a927e15dc","3ef319a40f524c30b814759d3b23fe1b","b0694e6f9ae24e11b4d79bb3a8abe5a5","a2bc66b0c7da48539e0c53457ae94dd6"]},"executionInfo":{"status":"ok","timestamp":1608097742041,"user_tz":300,"elapsed":33632,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"42c98305-a296-4585-bdcf-c550ebee9aae"},"source":["ds = load_data(dataset='commonsense_qa', preview=5)\n","print('\\nDataset statistics:')\n","print(ds)"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edbe73f1e2aa40adbe9fba8eba36ba06","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1586.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"693c4beb47e84570bf4c97647d2cbe4b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1055.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using custom data configuration default\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Downloading and preparing dataset commonsense_qa/default (download: 4.46 MiB, generated: 2.08 MiB, post-processed: Unknown size, total: 6.54 MiB) to /root/.cache/huggingface/datasets/commonsense_qa/default/0.1.0/0e60f0ee8c8509e854ed897f65eb5b2e6ca22578d64cbc3812c79b527d7a7a29...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc6e0e9558f54260a965fe0c2e7bd90e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3785890.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea8c6b7dcdef47c99fdb54b617a1770f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=423148.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b2b6657138d4a3e995ebb4439ac5680","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=471653.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43249d3952074726958a693b01280c3f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a190c304055456a8dc524f502cb331a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c3e69f8889a4d1ca2e30f9f3509ccb3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rDataset commonsense_qa downloaded and prepared to /root/.cache/huggingface/datasets/commonsense_qa/default/0.1.0/0e60f0ee8c8509e854ed897f65eb5b2e6ca22578d64cbc3812c79b527d7a7a29. Subsequent calls will reuse this data.\n","\n","Loading an example...\n","What home entertainment equipment requires cable?\n","A radio shack\n","B substation\n","C cabinet\n","D television\n","E desk\n","Ans: D\n","\n","Dataset statistics:\n","DatasetDict({\n","    train: Dataset({\n","        features: ['answerKey', 'question', 'choices'],\n","        num_rows: 9741\n","    })\n","    validation: Dataset({\n","        features: ['answerKey', 'question', 'choices'],\n","        num_rows: 1221\n","    })\n","    test: Dataset({\n","        features: ['answerKey', 'question', 'choices'],\n","        num_rows: 1140\n","    })\n","})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wO8K_i0Dgols"},"source":["## Commonsense Knowledge"]},{"cell_type":"markdown","metadata":{"id":"URk5XdnFfrga"},"source":["Many knowledge sources are available, including structured knowledge like [ConceptNet](https://conceptnet.io/) and unstructured knowledge like Wikipedia plain texts [3]. Particularly, graph-structured knowledge is proved to be powerful in many application, because of its ability to represent words as individual nodes and relationships between words as edges.\n","\n","We will use the preprocessed knowledge data from [2] for this notebook."]},{"cell_type":"code","metadata":{"id":"fDfJSDehHglf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608097803959,"user_tz":300,"elapsed":95541,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}},"outputId":"064b78f6-b3b6-4cc5-a0be-af6d37a12d57"},"source":["!wget -cO - https://6ipv4q.dm.files.1drv.com/y4m6omS9iWd4efq5NhX-gZSX2MDniu9p0ZyPcxzGOKAMM_OGuKnUlONExIzWlUx_tkU2w_-oWzwnODBGk-StxlP_V4WPSlmnRqFLj4V88V9sYyZWUoH5ZoZC8Ul-rRMK7kprx4jkN87PVnQ_SUu3yxNT1S5GlCwsmzENE3zzNnFKJpTVavpyGJJqTCuH3TQu8L6hK4vVpk3jgOl3rEjgsQz-Q > data.zip\n","!unzip data.zip\n","\n","!rm -f data.zip\n","!mv AAAI2020-data/data/ .\n","!rm -r AAAI2020-data/"],"execution_count":14,"outputs":[{"output_type":"stream","text":["--2020-12-16 05:49:01--  https://6ipv4q.dm.files.1drv.com/y4m6omS9iWd4efq5NhX-gZSX2MDniu9p0ZyPcxzGOKAMM_OGuKnUlONExIzWlUx_tkU2w_-oWzwnODBGk-StxlP_V4WPSlmnRqFLj4V88V9sYyZWUoH5ZoZC8Ul-rRMK7kprx4jkN87PVnQ_SUu3yxNT1S5GlCwsmzENE3zzNnFKJpTVavpyGJJqTCuH3TQu8L6hK4vVpk3jgOl3rEjgsQz-Q\n","Resolving 6ipv4q.dm.files.1drv.com (6ipv4q.dm.files.1drv.com)... 13.107.42.12\n","Connecting to 6ipv4q.dm.files.1drv.com (6ipv4q.dm.files.1drv.com)|13.107.42.12|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 312773826 (298M) [application/zip]\n","Saving to: ‘STDOUT’\n","\n","-                   100%[===================>] 298.28M  15.6MB/s    in 16s     \n","\n","2020-12-16 05:49:18 (19.1 MB/s) - written to stdout [312773826/312773826]\n","\n","Archive:  data.zip\n","   creating: AAAI2020-data/\n","   creating: AAAI2020-data/data/\n","  inflating: AAAI2020-data/data/dev.jsonl.concept  \n","  inflating: AAAI2020-data/data/dev.jsonl.wikigraphnew  \n","  inflating: AAAI2020-data/data/dev.jsonl_NL  \n","  inflating: AAAI2020-data/data/dev_rand_split.jsonl  \n","  inflating: AAAI2020-data/data/test.jsonl.concept  \n","  inflating: AAAI2020-data/data/test.jsonl.wikigraphnew  \n","  inflating: AAAI2020-data/data/test.jsonl_NL  \n","  inflating: AAAI2020-data/data/test_rand_split_no_answers.jsonl  \n","  inflating: AAAI2020-data/data/train.jsonl.concept  \n","  inflating: AAAI2020-data/data/train.jsonl.wikigraphnew  \n","  inflating: AAAI2020-data/data/train.jsonl_NL  \n","  inflating: AAAI2020-data/data/train_rand_split.jsonl  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rIxGTEYezc1L"},"source":["## Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"KuTjD9TjISMz"},"source":["Define helper classes `Example`, `InputFeatures` and `KnowledgeGraph`."]},{"cell_type":"code","metadata":{"id":"HS5CDydVzfAB","executionInfo":{"status":"ok","timestamp":1608097803963,"user_tz":300,"elapsed":95543,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["class Example(object):\n","    \"\"\"\n","    A single training/test example for the SWAG dataset.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 idx,\n","                 context_sentence,\n","                 ending_0,\n","                 ending_1,\n","                 ending_2,\n","                 ending_3,\n","                 ending_4,\n","                 nodes,\n","                 adj_matrix,\n","                 label = None):\n","        self.idx = idx\n","        self.context_sentence = context_sentence\n","        self.endings = [\n","            ending_0,\n","            ending_1,\n","            ending_2,\n","            ending_3,\n","            ending_4,\n","        ]\n","        self.label = label\n","        self.nodes=nodes\n","        self.adj_matrixs=adj_matrix\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","    def __repr__(self):\n","        l = [\n","            \"id: {}\".format(self.idx),\n","            \"context_sentence: {}\".format(self.context_sentence),\n","            \"ending_0: {}\".format(self.endings[0]),\n","            \"ending_1: {}\".format(self.endings[1]),\n","            \"ending_2: {}\".format(self.endings[2]),\n","            \"ending_3: {}\".format(self.endings[3]),\n","            \"ending_4: {}\".format(self.endings[4]),\n","        ]\n","\n","        if self.label is not None:\n","            l.append(\"label: {}\".format(self.label))\n","\n","        return \"\\n\".join(l)\n","\n","\n","class InputFeatures(object):\n","    \"\"\"\n","    A single feature converted from an example.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 example_id,\n","                 choices_features,\n","                 label):\n","        self.example_id = example_id\n","        self.choices_features = [\n","            {\n","                'input_ids': input_ids,\n","                'input_mask': input_mask,\n","                'segment_ids': segment_ids,\n","                'node_ids': nodes_ids,\n","                'adj_mask':adj_mask,\n","            }\n","            for _, input_ids, input_mask, segment_ids,nodes_ids,adj_mask in choices_features\n","        ]\n","        self.label = label\n","\n","\n","class KnowledgeGraph:\n","    \"\"\"\n","    A knowledge graph.\n","    \"\"\"\n","\n","    def __init__(self, directed=False):\n","        self.graph = defaultdict(list)\n","        self.directed = directed\n","\n","    def addEdge(self, head, tail):\n","        self.graph[head].append(tail)\n","        if self.directed:\n","            self.graph[tail] = self.graph[tail]\n","        elif head == tail:\n","            self.graph[tail] = []\n","\n","    def topologySortHelper(self, s, visited, sortlist):\n","        visited[s] = True\n","        for i in self.graph[s]:\n","            if not visited[i]:\n","                self.topologySortHelper(i, visited, sortlist)\n","        sortlist.insert(0, s)\n","\n","    def topologySort(self):\n","        visited = {i: False for i in self.graph}\n","        sortlist = []\n","        for key in self.graph:\n","            self.graph[key] = sorted(self.graph[key])\n","        keys=list(self.graph)\n","        for v in sorted(keys):\n","            if not visited[v]:\n","                self.topologySortHelper(v, visited, sortlist)\n","        return sortlist "],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nko9m-2SJWDp"},"source":["Read in the files."]},{"cell_type":"code","metadata":{"id":"MP-Mfq4dv662","executionInfo":{"status":"ok","timestamp":1608097804474,"user_tz":300,"elapsed":96050,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def read_examples(input_file, is_training):\n","    \"\"\"\n","    Preprocess the knowledge files into examples.\n","    \"\"\"\n","    cont=0\n","    examples=[]\n","\n","    with open(input_file+'.concept') as f1, open(input_file+'_NL') as f2, open(input_file+'.wikigraphnew') as f3:\n","\n","        for line, line2, line3 in zip(f1, f2, f3):\n","\n","            if cont % 1000==0:\n","                logger.info('read cont:{}'.format(cont))\n","\n","            js=json.loads(line.strip())\n","            js2=json.loads(line2.strip())\n","            js3=json.loads(line3.strip())\n","\n","            qa_list=[]\n","            context=js['question']['stem']\n","            node_lists=[]\n","            adj_matrix_lists=[]\n","\n","            # temp: .concept | temp2: add one _NL | temp3: wikipedia\n","            for temp, temp2, temp3 in zip(js['question']['choices'],\n","                                          js2['question']['choices'],\n","                                          js3['question']['choices']):\n","\n","                #############################################################\n","                #                ConceptNet graph construction              #\n","                #############################################################\n","\n","                # max 20 nodes in Concept-Graph\n","                k=20\n","                g = KnowledgeGraph(directed=True)\n","                temp[\"node\"] = temp[\"node\"][:k]\n","\n","                # add edges according to the edges provided by Jingjing\n","                for r in temp['relation']:\n","                    if r[0] < k and r[1] < k:\n","                        g.addEdge(r[0],r[1])\n","                        \n","                # add one edge pointing to itself\n","                for i in range(len(temp[\"node\"])):\n","                    g.addEdge(i, i)\n","\n","                # get the sequence according to topology sort algorithm\n","                topsort_seq = g.topologySort()\n","                sorted_node = []\n","\n","                # topsort_seq contains the idx of nodes, only containing numbers\n","                for i in topsort_seq:\n","                    # temp['node'][i]: get the evidence corresponding to node i\n","                    sorted_node.append(temp['node'][i])\n","\n","                # sorted_node contains the sorted ConceptNet evidence\n","                sorted_adj_matrix=[]    \n","                adj_matrix_list=[]\n","                for tmp in temp['evidence_edges']:\n","                    # adj_matrix_list: the list of edges. each 1 or 0 represents two nodes are connected or not.\n","                    adj_matrix_list.append(tmp)\n","                    # sorted_adj_matrix: each line represents the nodes linked to the node i\n","                    sorted_adj_matrix.append([0 for i in range(len(tmp))])\n","\n","                for i in range(min(k,len(adj_matrix_list))):\n","                    for j in range(min(k,len(adj_matrix_list[i]))):\n","                        # sorted_adj_matrix[i][j]=1 means node i is connected to j, 0 means not.\n","                        sorted_adj_matrix[i][j]=adj_matrix_list[topsort_seq[i]][topsort_seq[j]]\n","\n","                #############################################################\n","                #                 Wikepedia graph construction              #\n","                #############################################################\n","\n","                # Wiki-Graph has at most 10 sentences.\n","                k=10\n","                wiki_nodes=[]\n","\n","                # temp3['node'] means srl results.\n","                for n in temp3['node']:\n","                    wiki_nodes.append(''.join(n.split()))\n","                wiki_evidences=[]\n","\n","                # wiki_evidences contains the origin wikipedia evidence\n","                for n in temp3['searched_evidence']['basic']:\n","                    wiki_evidences.append(''.join(n['text'].split()))\n","\n","                # each node belongs to which evidence\n","                node2evidences={}\n","\n","                for idx,n in enumerate(wiki_nodes):\n","                    for idx1,e in enumerate(wiki_evidences):\n","                        # the condition is not influential by ''.join()\n","                        if n in e:\n","                            if idx not in node2evidences: \n","                                node2evidences[idx]=[]\n","                            node2evidences[idx].append(idx1)\n","    \n","                g = KnowledgeGraph(directed=True)                \n","                wiki_adj, sorted_wiki_adj = [], []\n","\n","                # wiki_adj and sorted_wiki_adj: both k*k, 10*10 matrix\n","                for i in range(min(k,len(wiki_evidences))):\n","                    wiki_adj.append([0 for j in range(len(wiki_evidences))])\n","                    sorted_wiki_adj.append([0 for j in range(len(wiki_evidences))])\n","\n","                # add edges between two evidences if the components of them are connected...\n","                for r in temp3['relation']:\n","                    for i in node2evidences[r[0]]:\n","                        for j in node2evidences[r[1]]:\n","                            if i < k and j < k:\n","                                # construct a directed graph.\n","                                g.addEdge(i,j)\n","                                wiki_adj[i][j]=1\n","                                wiki_adj[j][i]=1\n","                wiki_evidences=[]\n","\n","                for tmp in temp3['searched_evidence']['basic'][:k]:\n","                    wiki_evidences.append(tmp['text']) \n","\n","                for i in range(len(wiki_evidences)):\n","                    g.addEdge(i,i)\n","\n","                topsort_seq = g.topologySort()\n","\n","                sorted_evidences=[]\n","                for i in topsort_seq:\n","                    sorted_evidences.append(wiki_evidences[i])\n","                sorted_adj_matrix = []\n","\n","                # the re-ordered wikipedia evidence\n","                for i in range(min(k, len(wiki_adj))):\n","                    for j in range(min(k, len(wiki_adj[i]))):\n","                        sorted_wiki_adj[i][j] = wiki_adj[topsort_seq[i]][topsort_seq[j]]\n","\n","                #############################################################\n","                #                    Example construction                   #\n","                #############################################################\n","                \n","                t = sorted_evidences \n","                t1 = sorted_node\n","\n","                # Only wiki evidence\n","                if len(t) == 0:\n","                    t = [\"None\"]\n","\n","                # Only Concept node   \n","                if len(t1) == 0:\n","                    t1 = [\"None\"]\n","\n","                # Add Wiki-Graph triple nodes\n","                # 10 sentences, one sentence contains which triples\n","                srl_triples_index = [[] for i in range(len(wiki_evidences))]\n","                \n","                index = 0\n","                evidence2arguments = {}\n","                for srl in temp3['searched_evidence']['basic']:\n","                    srl_triple_index = []\n","                    srl_triples = srl['srl_triple']\n","                    # print('srl_triples:{}'.format(srl_triples))\n","\n","                    srl_verbs = []\n","                    if len(srl['srl']['verbs']) > 0:\n","                        srl_verbs = srl['srl']['verbs']\n","                    else:\n","                        srl_verbs = [srl['srl']['verbs']]\n","\n","                    # for triple, verb, evidence in zip(srl_triples, srl_verbs, wiki_evidences):\n","                    for verb in srl_verbs:\n","                        if len(verb) == 0:\n","                            continue\n","                        text = verb['description']\n","                        res = re.findall(r\"\\[(.*?)\\]\", text)\n","\n","                        for temp_res in res:\n","                            if len(temp_res.split(':'))>=2:\n","                                temp_res2 = temp_res.split(':')[1].strip()\n","                                text = text.replace('[' + temp_res + ']', temp_res2)\n","                        if len(text.split(' ')) != len(verb['tags']):\n","                            continue\n","\n","                        # add nodes and edges\n","                        if text not in evidence2arguments:\n","                            evidence2arguments[text] = []\n","\n","                        tags = verb['tags']\n","                        if not ('B-ARG0' in tags and 'B-V' in tags and 'B-ARG1' in tags):\n","                            continue\n","\n","                        # for ARG0\n","                        start = tags.index('B-ARG0')\n","                        end = start\n","                        for temp_i in range(start+1, len(tags)):\n","                            if tags[temp_i] == 'I-ARG0':\n","                                end += 1\n","                            else:\n","                                break\n","                        text = text.split(' ')\n","                        # print('ARG0 {} {} {}'.format(text[start:end+1], start, end))\n","                        evidence2arguments[' '.join(text)].append([' '.join(text[start:end+1]), start, end])\n","\n","                        # for verb\n","                        start = tags.index('B-V')\n","                        end = start\n","                        for temp_i in range(start+1, len(tags)):\n","                            if tags[temp_i] == 'I-V':\n","                                end += 1\n","                            else:\n","                                break\n","                        # print('VERB {} {}'.format(text[start:end+1],start, end))\n","                        evidence2arguments[' '.join(text)].append([' '.join(text[start:end + 1]), start, end])\n","\n","                        # for ARG1\n","                        start = tags.index('B-ARG1')\n","                        end = start\n","                        for temp_i in range(start+1, len(tags)):\n","                            if tags[temp_i] == 'I-ARG1':\n","                                end += 1\n","                            else:\n","                                break\n","                        # print('ARG1 {} {}'.format(text[start:end+1],start, end))\n","                        evidence2arguments[' '.join(text)].append([' '.join(text[start:end + 1]), start, end])\n","\n","                qa_list.append(([temp['text'],'##'.join(t),'##'.join([temp2['text']] + t1),sorted_wiki_adj,evidence2arguments]))\n","\n","                # node_lists contains the nodes in ConceptNet\n","                # adj_matrix_lists contains the adjacent matrix of Wiki-Graph\n","                node_lists.append(sorted_node)\n","                adj_matrix_lists.append(sorted_wiki_adj)\n","\n","            cont += 1\n","            examples.append(\n","                Example(\n","                        idx = cont,\n","                        context_sentence = context,\n","                        ending_0 = qa_list[0],\n","                        ending_1 = qa_list[1],\n","                        ending_2 = qa_list[2],\n","                        ending_3 = qa_list[3],\n","                        ending_4 = qa_list[4],\n","                        nodes=node_lists,\n","                        adj_matrix=adj_matrix_lists,\n","                        label = ord(js['answerKey'])-ord('A') if is_training else None\n","                        ) \n","            )\n","\n","    return examples"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqdp2v5Gwzql","executionInfo":{"status":"ok","timestamp":1608098746321,"user_tz":300,"elapsed":520,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","    \"\"\"\n","    Truncates a sequence pair in place to the maximum length.\n","  \n","    This is a simple heuristic which will always truncate the longer sequence\n","    one token at a time. This makes more sense than truncating an equal percent\n","    of tokens from each, since if one sequence is very short then each token\n","    that's truncated likely contains more information than a longer sequence.\n","    \"\"\"\n","\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_length:\n","            break\n","        if len(tokens_a) > len(tokens_b):\n","            tokens_a.pop()\n","        else:\n","            tokens_b.pop()\n","\n","            \n","def convert_examples_to_features(examples, tokenizer, max_seq_length, is_training):\n","    \"\"\"\n","    Loads a data file into a list of `InputBatch`s.\n","    \n","    Swag is a multiple choice task. To perform this task using Bert,\n","    we will use the formatting proposed in \"Improving Language\n","    Understanding by Generative Pre-Training\" and suggested by\n","    @jacobdevlin-google in this issue\n","    https://github.com/google-research/bert/issues/38.\n","    \n","    Each choice will correspond to a sample on which we run the\n","    inference. For a given Swag example, we will create the 4\n","    following inputs:\n","    - [CLS] context [SEP] choice_1 [SEP]\n","    - [CLS] context [SEP] choice_2 [SEP]\n","    - [CLS] context [SEP] choice_3 [SEP]\n","    - [CLS] context [SEP] choice_4 [SEP]\n","    The model will output a single value for each input. \n","    To get the final decision of the model, \n","    we will run a softmax over these 4 outputs.\n","    \"\"\"\n","\n","    features = []\n","    for example_index, example in enumerate(examples):\n","\n","        choices_features = []\n","        if example_index % 1000 == 0 and example_index > 0:\n","            logger.info('convert example to feature:{}'.format(example_index))\n","\n","        # change example:\n","        for ending_index, (ending,node,adj_matrix) in enumerate(zip(example.endings,example.nodes,example.adj_matrixs)):\n","\n","            # We create a copy of the context tokens in order to be\n","            # able to shrink it according to ending_tokens\n","\n","            # Question + Answer\n","            ending_tokens = tokenizer.tokenize(example.context_sentence) + tokenizer.tokenize(\n","                'The answer is') + tokenizer.tokenize(ending[0])\n","            evidence2arguments = ending[-1]\n","\n","            # Conceptnet evidence\n","            concept_context_tokens_choice = tokenizer.tokenize(ending[2])\n","            _truncate_seq_pair(concept_context_tokens_choice, [], 128)\n","            concept_context_tokens_choice.append(\"<sep>\")\n","\n","            tokens_choice = concept_context_tokens_choice\n","            tokens_choice.extend(tokenizer.tokenize('# Wikipedia #'))\n","            wiki_nodes_choice = []\n","\n","            for key in evidence2arguments:\n","                # one key, one evidence\n","                words2tokens = {}\n","                evidence_tokens = []\n","                key = key.split(' ')\n","\n","                for temp_i in range(len(key)):\n","                    tokens = tokenizer.tokenize(key[temp_i])\n","                    words2tokens[temp_i] = (len(evidence_tokens), len(tokens) + len(evidence_tokens))\n","                    evidence_tokens.extend(tokens)\n","\n","                if len(tokens_choice) + len(evidence_tokens) > 256 - len(ending_tokens) - 3 - 2: # 2 for ##\n","                    break\n","\n","                origin_length = len(tokens_choice)\n","                tokens_choice.extend(evidence_tokens)\n","                tokens_choice.extend(tokenizer.tokenize('##'))\n","\n","                # update the start and end index of each argument\n","                for item in evidence2arguments[' '.join(key)]:\n","                    start = words2tokens[item[1]][0] + origin_length\n","                    end = words2tokens[item[2]][1] + origin_length\n","                    wiki_nodes_choice.append([item[0], start, end])\n","\n","            tokens_choice.append(\"<sep>\")\n","            tokens_choice.extend(ending_tokens)\n","            tokens_choice.append('<cls>')\n","\n","            tokens = tokens_choice\n","            segment_ids = [0] * (len(tokens) - len(ending_tokens) - 2) + [1] * (len(ending_tokens) + 1)+[2]\n","            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            #update the index because padding is at the beginning of the input\n","            padding_length = max_seq_length - len(input_ids)\n","            for node in wiki_nodes_choice:\n","                node[1] += padding_length\n","                node[2] += padding_length\n","\n","            temp=' '.join(str(x) for x in input_ids)\n","            temp=temp.split('17 7967 20631 17 7967')[0]\n","            temp=temp.split('7967 7967')\n","            temp=[len(x.split()) for x in temp]\n","            input_mask = [1] * len(input_ids)\n","            padding_length = max_seq_length - len(input_ids)\n","\n","            input_ids = ([0] * padding_length) + input_ids\n","            input_mask = ([0] * padding_length) + input_mask\n","            segment_ids = ([4] * padding_length) + segment_ids\n","\n","            skip=padding_length+temp[0]+2\n","            node=[]\n","            for t in temp[1:]:\n","                vector=np.zeros(max_seq_length)\n","                vector[skip:t+skip]=1\n","                skip+=t+2\n","                node.append(vector[None,:])\n","\n","            node=node[:50]\n","            for i in range(50-len(node)):\n","                vector=np.zeros(max_seq_length)\n","                node.append(vector[None,:])\n","\n","            node_size=len(temp)-1\n","            matrix=np.zeros((150,150))\n","            for i,val in enumerate(adj_matrix):\n","                for j,v in enumerate(adj_matrix[i]):\n","                    if v==1 and i<50 and j<50 and i<node_size and j<node_size:\n","                        matrix[i,j]=1\n","       \n","            #############################################################\n","\n","            temp=' '.join(str(x) for x in input_ids)\n","            temp=temp.split('17 7967 20631 17 7967')\n","            skip=len(temp[0].split())+5\n","            temp = temp[1] if len(temp) > 1 else temp[0]\n","\n","            for item in wiki_nodes_choice:\n","                vector = np.zeros(max_seq_length)\n","                for temp in range(item[1], item[2]):\n","                    vector[temp] = 1\n","                node.append(vector[None,:])\n","\n","            node = node[:150]\n","            for i in range(150 - len(node)):\n","                vector = np.zeros(max_seq_length)\n","                node.append(vector[None, :])\n","            node = np.concatenate(node, 0)\n","\n","            node_size = len(wiki_nodes_choice) - 1  # 0.. len(wiki_nodes_choices)-1\n","\n","            for idx1, node1 in enumerate(wiki_nodes_choice):\n","                for idx2, node2 in enumerate(wiki_nodes_choice):\n","                    if node1[0].lower() == node2[0].lower() and idx1<100 and idx2<100 and idx1<node_size and idx2<node_size :\n","                        matrix[idx1+50, idx2+50] = 1\n","\n","            assert len(input_ids) == max_seq_length\n","            assert len(input_mask) == max_seq_length\n","            assert len(segment_ids) == max_seq_length\n","\n","            choices_features.append((tokens, input_ids, input_mask, segment_ids,node,matrix))\n","\n","\n","        label = example.label\n","        if example_index < 1 and is_training:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"idx: {}\".format(example.idx))\n","            for choice_idx, (tokens, input_ids, input_mask, segment_ids,_,_) in enumerate(choices_features):\n","                logger.info(\"choice: {}\".format(choice_idx))\n","                logger.info(\"tokens: {}\".format(' '.join(tokens).replace('\\u2581','_')))\n","                logger.info(\"input_ids: {}\".format(' '.join(map(str, input_ids))))\n","                logger.info(\"input_mask: {}\".format(' '.join(map(str, input_mask))))\n","                logger.info(\"segment_ids: {}\".format(' '.join(map(str, segment_ids))))\n","                logger.info(\"label: {}\".format(label))\n","\n","        features.append(\n","            InputFeatures(\n","                example_id = example.idx,\n","                choices_features = choices_features,\n","                label = label\n","            )\n","        )\n","\n","    return features\n"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KueQ6lZ-OGIR"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"y6bcx5Xql6-8"},"source":["## Graph-based Model\n","\n","The [Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering](https://arxiv.org/abs/1909.05311) addresses the Question Answering problem and perform experiment on CommonsenseQA [2]. \n","\n","One major contribution of the work is that they are the first to propose a model that leverage evidence from multiple knowledge sources. In the experiment, ConceptNet and Wikipedia Plain Text are preprocessed into knowledge graphs.\n","\n","The Graph-based Reasoning module consists of a graph-based contextual representation learning module and a graph-based inference module.\n","\n","* The graph-based contextual representation learning module is built upon XLNet. The module assigns a closer distance of those related works in different evidence sentences by using graph information. Algorithmically, Topology Sort Algorithm is applied to re-order the input evidence according to the constructed knowledge graphs.\n","\n","* The graph-based inference module tries to aggregate evidence at the graph-level for predictions. Specifically, a Graph Convolutional Network (GCN) is used to retrieve the node representation, and a graph attention layer is applied for prediction.\n","\n","See official implementation [here](https://github.com/DecstionBack/AAAI_2020_CommonsenseQA)."]},{"cell_type":"markdown","metadata":{"id":"OIJGpAcWImZO"},"source":["```\n","GraphBasedXLNetModel(\n","  (word_embedding): Embedding(32000, 1024)\n","  (layer): ModuleList(\n","    (0): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (4): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (5): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (6): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (7): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (8): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (9): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (10): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (11): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (12): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (13): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (14): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (15): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (16): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (17): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (18): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (19): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (20): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (21): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (22): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (23): XLNetLayer(\n","      (rel_attn): XLNetRelativeAttention(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): XLNetFeedForward(\n","        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n","        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (map): Linear(in_features=64, out_features=1024, bias=True)\n","  (map_node_emb): Linear(in_features=1024, out_features=64, bias=True)\n","  (GCN_W): ModuleList(\n","    (0): Linear(in_features=64, out_features=64, bias=True)\n","    (1): Linear(in_features=64, out_features=64, bias=True)\n","  )\n","  (GCN_W_self): ModuleList(\n","    (0): Linear(in_features=64, out_features=64, bias=True)\n","    (1): Linear(in_features=64, out_features=64, bias=True)\n","  )\n",")\n","```"]},{"cell_type":"markdown","metadata":{"id":"WMd7VaR1ljbr"},"source":["Define the graph based transformer module."]},{"cell_type":"code","metadata":{"id":"AglO5qu5POcx","executionInfo":{"status":"ok","timestamp":1608098521645,"user_tz":300,"elapsed":1179,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["class GraphBasedXLNetModel(XLNetPreTrainedModel):\n","    \"\"\"\n","    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n","        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n","            Sequence of hidden-states at the last layer of the model.\n","        **mems**:\n","            list of ``torch.FloatTensor`` (one for each layer):\n","            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n","            (see `mems` input above). Can be used to speed up sequential decoding and attend to longer context.\n","        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n","            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n","            of shape ``(batch_size, sequence_length, hidden_size)``:\n","            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n","        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n","            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n","            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(GraphBasedXLNetModel, self).__init__(config)\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","\n","        self.mem_len = config.mem_len\n","        self.reuse_len = config.reuse_len\n","        self.d_model = config.d_model\n","        self.same_length = config.same_length\n","        self.attn_type = config.attn_type\n","        self.bi_data = config.bi_data\n","        self.clamp_len = config.clamp_len\n","        self.n_layer = config.n_layer\n","\n","        self.word_embedding = nn.Embedding(32000, config.d_model)\n","        self.mask_emb = nn.Parameter(torch.Tensor(1, 1, config.d_model))\n","        self.layer = nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])\n","        self.dropout = nn.Dropout(config.dropout)\n","        \n","        self.map = nn.Linear(64, config.d_model)\n","        self.map_node_emb = nn.Linear(config.d_model, 64)\n","        \n","        self.GCN_W = []\n","        self.GCN_W_self = []\n","        for i in range(2):\n","            self.GCN_W.append(nn.Linear(64, 64))\n","            self.GCN_W_self.append(\n","                nn.Linear(64, 64) \n","                )\n","        self.GCN_W = nn.ModuleList(self.GCN_W)\n","        self.GCN_W_self = nn.ModuleList(self.GCN_W_self)\n","\n","        self.init_weights()\n","\n","    def _resize_token_embeddings(self, new_num_tokens):\n","        self.word_embedding = self._get_resized_embeddings(self.word_embedding, new_num_tokens)\n","        return self.word_embedding\n","\n","    def _prune_heads(self, heads_to_prune):\n","        raise NotImplementedError\n","\n","    def create_mask(self, qlen, mlen):\n","        \"\"\"\n","        Creates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked.\n","        Args:\n","            qlen: TODO Lysandre didn't fill\n","            mlen: TODO Lysandre didn't fill\n","        ::\n","                  same_length=False:      same_length=True:\n","                  <mlen > <  qlen >       <mlen > <  qlen >\n","               ^ [0 0 0 0 0 1 1 1 1]     [0 0 0 0 0 1 1 1 1]\n","                 [0 0 0 0 0 0 1 1 1]     [1 0 0 0 0 0 1 1 1]\n","            qlen [0 0 0 0 0 0 0 1 1]     [1 1 0 0 0 0 0 1 1]\n","                 [0 0 0 0 0 0 0 0 1]     [1 1 1 0 0 0 0 0 1]\n","               v [0 0 0 0 0 0 0 0 0]     [1 1 1 1 0 0 0 0 0]\n","        \"\"\"\n","        attn_mask = torch.ones([qlen, qlen])\n","        mask_up = torch.triu(attn_mask, diagonal=1)\n","        attn_mask_pad = torch.zeros([qlen, mlen])\n","        ret = torch.cat([attn_mask_pad, mask_up], dim=1)\n","        if self.same_length:\n","            mask_lo = torch.tril(attn_mask, diagonal=-1)\n","            ret = torch.cat([ret[:, :qlen] + mask_lo, ret[:, qlen:]], dim=1)\n","\n","        ret = ret.to(next(self.parameters()))\n","        return ret\n","\n","    def cache_mem(self, curr_out, prev_mem):\n","        \"\"\"\n","        cache hidden states into memory.\n","        \"\"\"\n","        if self.mem_len is None or self.mem_len == 0:\n","            return None\n","        else:\n","            if self.reuse_len is not None and self.reuse_len > 0:\n","                curr_out = curr_out[:self.reuse_len]\n","\n","            if prev_mem is None:\n","                new_mem = curr_out[-self.mem_len:]\n","            else:\n","                new_mem = torch.cat([prev_mem, curr_out], dim=0)[-self.mem_len:]\n","\n","        return new_mem.detach()\n","\n","    @staticmethod\n","    def positional_embedding(pos_seq, inv_freq, bsz=None):\n","        sinusoid_inp = torch.einsum('i,d->id', pos_seq, inv_freq)\n","        pos_emb = torch.cat([torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)\n","        pos_emb = pos_emb[:, None, :]\n","\n","        if bsz is not None:\n","            pos_emb = pos_emb.expand(-1, bsz, -1)\n","\n","        return pos_emb\n","\n","    def relative_positional_encoding(self, qlen, klen, bsz=None):\n","        \"\"\"create relative positional encoding.\"\"\"\n","        freq_seq = torch.arange(0, self.d_model, 2.0, dtype=torch.float)\n","        inv_freq = 1 / torch.pow(10000, (freq_seq / self.d_model))\n","\n","        if self.attn_type == 'bi':\n","            # beg, end = klen - 1, -qlen\n","            beg, end = klen, -qlen\n","        elif self.attn_type == 'uni':\n","            # beg, end = klen - 1, -1\n","            beg, end = klen, -1\n","        else:\n","            raise ValueError('Unknown `attn_type` {}.'.format(self.attn_type))\n","\n","        if self.bi_data:\n","            fwd_pos_seq = torch.arange(beg, end, -1.0, dtype=torch.float)\n","            bwd_pos_seq = torch.arange(-beg, -end, 1.0, dtype=torch.float)\n","\n","            if self.clamp_len > 0:\n","                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n","                bwd_pos_seq = bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n","\n","            if bsz is not None:\n","                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz//2)\n","                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz//2)\n","            else:\n","                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n","                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n","\n","            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)\n","\n","        else:\n","            fwd_pos_seq = torch.arange(beg, end, -1.0)\n","            if self.clamp_len > 0:\n","                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n","            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)\n","\n","        pos_emb = pos_emb.to(next(self.parameters()))\n","        return pos_emb\n","\n","    def forward(self, input_ids, token_type_ids=None, input_mask=None, attention_mask=None,\n","                mems=None, perm_mask=None, target_mapping=None, head_mask=None,node_mask=None,adj_mask=None):\n","        \"\"\"\n","        The original code for XLNet uses shapes [len, bsz] with the batch dimension at the end,\n","        but we want a unified interface in the library with the batch size on the first dimension,\n","        so we move here the first dimension (batch) to the end\n","        \"\"\"\n","\n","        input_ids = input_ids.transpose(0, 1).contiguous()\n","        token_type_ids = token_type_ids.transpose(0, 1).contiguous() if token_type_ids is not None else None\n","        input_mask = input_mask.transpose(0, 1).contiguous() if input_mask is not None else None\n","        attention_mask = attention_mask.transpose(0, 1).contiguous() if attention_mask is not None else None\n","        perm_mask = perm_mask.permute(1, 2, 0).contiguous() if perm_mask is not None else None\n","        target_mapping = target_mapping.permute(1, 2, 0).contiguous() if target_mapping is not None else None\n","\n","        qlen, bsz = input_ids.shape[0], input_ids.shape[1]\n","        mlen = mems[0].shape[0] if mems is not None else 0\n","        klen = mlen + qlen\n","\n","        dtype_float = next(self.parameters()).dtype\n","        device = next(self.parameters()).device\n","\n","        ##### Attention mask\n","        # causal attention mask\n","        if self.attn_type == 'uni':\n","            attn_mask = self.create_mask(qlen, mlen)\n","            attn_mask = attn_mask[:, :, None, None]\n","        elif self.attn_type == 'bi':\n","            attn_mask = None\n","        else:\n","            raise ValueError('Unsupported attention type: {}'.format(self.attn_type))\n","\n","        # data mask: input mask & perm mask\n","        assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \"\n","        \"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\n","        if input_mask is None and attention_mask is not None:\n","            input_mask = 1.0 - attention_mask\n","        if input_mask is not None and perm_mask is not None:\n","            data_mask = input_mask[None] + perm_mask\n","        elif input_mask is not None and perm_mask is None:\n","            data_mask = input_mask[None]\n","        elif input_mask is None and perm_mask is not None:\n","            data_mask = perm_mask\n","        else:\n","            data_mask = None\n","\n","        if data_mask is not None:\n","            # all mems can be attended to\n","            mems_mask = torch.zeros([data_mask.shape[0], mlen, bsz]).to(data_mask)\n","            data_mask = torch.cat([mems_mask, data_mask], dim=1)\n","            if attn_mask is None:\n","                attn_mask = data_mask[:, :, :, None]\n","            else:\n","                attn_mask += data_mask[:, :, :, None]\n","\n","        if attn_mask is not None:\n","            attn_mask = (attn_mask > 0).to(dtype_float)\n","\n","        if attn_mask is not None:\n","            non_tgt_mask = -torch.eye(qlen).to(attn_mask)\n","            non_tgt_mask = torch.cat([torch.zeros([qlen, mlen]).to(attn_mask), non_tgt_mask], dim=-1)\n","            non_tgt_mask = ((attn_mask + non_tgt_mask[:, :, None, None]) > 0).to(attn_mask)\n","        else:\n","            non_tgt_mask = None\n","\n","        ##### Word embeddings and prepare h & g hidden states\n","        word_emb_k = self.word_embedding(input_ids)\n","        output_h = self.dropout(word_emb_k)\n","        if target_mapping is not None:\n","            word_emb_q = self.mask_emb.expand(target_mapping.shape[0], bsz, -1)\n","        # else:  # We removed the inp_q input which was same as target mapping\n","        #     inp_q_ext = inp_q[:, :, None]\n","        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n","            output_g = self.dropout(word_emb_q)\n","        else:\n","            output_g = None\n","\n","        ##### Segment embedding\n","        if token_type_ids is not None:\n","            # Convert `token_type_ids` to one-hot `seg_mat`\n","            mem_pad = torch.zeros([mlen, bsz], dtype=torch.long, device=device)\n","            cat_ids = torch.cat([mem_pad, token_type_ids], dim=0)\n","\n","            # `1` indicates not in the same segment [qlen x klen x bsz]\n","            seg_mat = (token_type_ids[:, None] != cat_ids[None, :]).long()\n","            seg_mat = torch.cat([1-seg_mat.unsqueeze(-1),seg_mat.unsqueeze(-1)],-1).to(dtype_float)\n","        else:\n","            seg_mat = None\n","\n","        ##### Positional encoding\n","        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz)\n","        pos_emb = self.dropout(pos_emb)\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n","        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n","                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n","            elif head_mask.dim() == 2:\n","                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n","            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.n_layer\n","\n","        new_mems = ()\n","        if mems is None:\n","            mems = [None] * len(self.layer)\n","\n","        attentions = []\n","        hidden_states = []\n","        for i, layer_module in enumerate(self.layer):\n","            # cache new mems\n","            new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n","            if self.output_hidden_states:\n","                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n","\n","            outputs = layer_module(output_h, output_g, attn_mask_h=non_tgt_mask, attn_mask_g=attn_mask,\n","                                   r=pos_emb, seg_mat=seg_mat, mems=mems[i], target_mapping=target_mapping,\n","                                   head_mask=head_mask[i])\n","            output_h, output_g = outputs[:2]\n","            if self.output_attentions:\n","                attentions.append(outputs[2])\n","\n","        # Add last hidden state\n","        if self.output_hidden_states:\n","            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n","\n","        output = self.dropout(output_g if output_g is not None else output_h)\n","\n","        output = output.permute(1, 0, 2).contiguous().view(node_mask.shape[0],node_mask.shape[1],output.shape[0],output.shape[-1])\n","\n","        node_mask = node_mask.float()\n","        adj_mask = adj_mask.float()\n","        \n","        node_emb = torch.einsum('abcd,abde->abce', node_mask, output)\n","        node_emb = node_emb/(node_mask.sum(-1)+1e-30)[:,:,:,None]\n","        node_emb = nn.functional.tanh(self.map_node_emb(node_emb))\n","        \n","        for i in range(1):\n","            node_emb_avg  =torch.einsum('abcd,abde->abce', adj_mask,self.GCN_W[i](node_emb))\n","            node_emb_avg = node_emb_avg/(adj_mask.sum(-1)+1e-30)[:,:,:,None]\n","            node_emb = nn.functional.tanh(self.GCN_W_self[i](node_emb)+node_emb_avg)\n","            \n","        output = output[:,:,-1]\n","        mask = node_mask.max(-1)[0]\n","        attn_score = torch.einsum('abc,abdc->abd', output,nn.functional.tanh(self.map(node_emb)))\n","        attn_score = attn_score - 1e30 * (1-mask)\n","        PR = nn.functional.softmax(attn_score,-1)\n","        kb_emb = PR[:,:,:,None]*node_emb\n","        kb_emb = kb_emb.sum(2).view(-1,kb_emb.shape[-1])          \n","        \n","        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n","        output = self.dropout(output_g if output_g is not None else output_h)\n","        outputs = (output.permute(1, 0, 2).contiguous(), self.dropout(kb_emb),new_mems)\n","        if self.output_hidden_states:\n","            if output_g is not None:\n","                hidden_states = tuple(h.permute(1, 0, 2).contiguous() for hs in hidden_states for h in hs)\n","            else:\n","                hidden_states = tuple(hs.permute(1, 0, 2).contiguous() for hs in hidden_states)\n","            outputs = outputs + (hidden_states,)\n","        if self.output_attentions:\n","            attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)\n","            outputs = outputs + (attentions,)\n","\n","        return outputs  # outputs, new_mems, (hidden_states), (attentions)\n"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i0vwmnazlqMr"},"source":["Define the corresponding `MultipleChoice` module."]},{"cell_type":"code","metadata":{"id":"bStclF7cU-3t","executionInfo":{"status":"ok","timestamp":1608097805549,"user_tz":300,"elapsed":97114,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["class GraphBasedXLNetForMultipleChoice(XLNetPreTrainedModel):\n","\n","    def __init__(self, config):\n","        super(GraphBasedXLNetForMultipleChoice, self).__init__(config)\n","        self.num_labels = config.num_labels\n","        self.transformer = GraphBasedXLNetModel(config)\n","        self.sequence_summary = SequenceSummary(config)\n","        self.sequence_summary1 = SequenceSummary(config)\n","        self.ln = nn.Linear(64, 64)\n","        self.ln1 = nn.Linear(config.d_model, config.d_model)\n","        self.last_dropout = nn.Dropout(config.summary_last_dropout)\n","        self.classifier = nn.Linear(config.d_model, 1)\n","        self.classifier1 = nn.Linear(64, 1)\n","        self.apply(self.init_weights)\n","\n","\n","    def forward(self, input_ids, token_type_ids=None, input_mask=None, attention_mask=None,\n","                mems=None, perm_mask=None, target_mapping=None,\n","                labels=None, head_mask=None, node_mask=None, adj_mask=None):\n","        num_choices = self.num_labels\n","\n","        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n","        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        flat_input_mask = input_mask.view(-1, token_type_ids.size(-1)) if input_mask is not None else None\n","        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        flat_mems = mems.view(-1, mems.size(-1)) if mems is not None else None\n","        flat_perm_mask = perm_mask.view(-1, perm_mask.size(-1)) if perm_mask is not None else None\n","        flat_target_mapping = target_mapping.view(-1, target_mapping.size(-1)) if target_mapping is not None else None\n","        flat_head_mask = head_mask.view(-1, head_mask.size(-1)) if head_mask is not None else None\n","        \n","        transformer_outputs = self.transformer(flat_input_ids, token_type_ids=flat_token_type_ids,\n","                                               input_mask=flat_input_mask, attention_mask=flat_attention_mask,\n","                                               mems=flat_mems, perm_mask=flat_perm_mask, target_mapping=flat_target_mapping,\n","                                               head_mask=flat_head_mask,node_mask=node_mask,adj_mask=adj_mask)\n","        \n","        output,output_kb = transformer_outputs[0][:,-1],transformer_outputs[1]\n","        \n","        output = self.sequence_summary(torch.cat((output,nn.functional.tanh(self.ln(output_kb))),-1))\n","        logits = self.classifier(output)        \n","\n","        reshaped_logits = logits.view(-1, num_choices)\n","\n","        outputs = reshaped_logits  # add hidden states and attention if they are here\n","\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(reshaped_logits, labels)\n","            outputs = loss\n","\n","        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W50kKdDSly0z"},"source":["Define a wrapper for model loading."]},{"cell_type":"code","metadata":{"id":"hMju1OEeOO72","executionInfo":{"status":"ok","timestamp":1608097805549,"user_tz":300,"elapsed":97110,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def load_model(model='all'):\n","\n","    print('Loading model', model)\n","    \n","    if model == 'graph_xlnet':\n","        return XLNetConfig, GraphBasedXLNetModel, XLNetTokenizer\n","    elif model == 'xlnet':\n","        return XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer\n","    raise NotImplemented"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2bSYy3SUTXb"},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{"id":"sQ0BLRx-zmep"},"source":["## Runtime"]},{"cell_type":"code","metadata":{"id":"tN6Jh3YLS-ak","executionInfo":{"status":"ok","timestamp":1608097806375,"user_tz":300,"elapsed":97933,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def main(args):\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n","        args.n_gpu = torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend='nccl')\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","                        datefmt = '%m/%d/%Y %H:%M:%S',\n","                        level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n","    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","                    args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n","    \n","    # Set seed\n","    set_seed(args)\n","\n","    try:\n","        os.makedirs(args.output_dir)\n","    except:\n","        pass\n","\n","    config_class, model_class, tokenizer_class = load_model(args.model_type)\n","\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name,\n","        num_labels=5, finetuning_task=args.task_name)\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name,\n","        do_lower_case=True)\n","    \n","    model = model_class.from_pretrained(\n","        args.model_name, from_tf=bool('.ckpt' in args.model_name), config=config)\n","\n","    if args.fp16:\n","        model.half()\n","    model.to(device)\n","\n","    if args.local_rank != -1:\n","        try:\n","            from apex.parallel import DistributedDataParallel as DDP\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n","\n","        model = DDP(model)\n","    elif args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","    \n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","\n","    # Prepare data loader\n","    train_examples = read_examples(os.path.join(args.data_dir, 'train.jsonl'), is_training = True)\n","    train_features = convert_examples_to_features(\n","        train_examples, tokenizer, args.max_seq_length, True)\n","    all_input_ids = torch.tensor(select_field(train_features, 'input_ids'), dtype=torch.long)\n","    all_input_mask = torch.tensor(select_field(train_features, 'input_mask'), dtype=torch.long)\n","    all_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'), dtype=torch.long)\n","    all_node_ids = torch.tensor(select_field(train_features, 'node_ids'), dtype=torch.long)\n","    all_adj_mask = torch.tensor(select_field(train_features, 'adj_mask'), dtype=torch.long) \n","    all_label = torch.tensor([f.label for f in train_features], dtype=torch.long)\n","    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label,all_node_ids,all_adj_mask)\n","    logger.info(all_input_ids.size())\n","    if args.local_rank == -1:\n","        train_sampler = RandomSampler(train_data)\n","    else:\n","        train_sampler = DistributedSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps)\n","\n","    num_train_optimization_steps =  args.train_steps\n","\n","    # Prepare optimizer\n","    param_optimizer = list(model.named_parameters())\n","\n","    # hack to remove pooler, which is not used\n","    # thus it produce None grad that break apex\n","    param_optimizer = [n for n in param_optimizer]\n","\n","    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=10000)\n","        \n","    global_step = 0\n","\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_examples))\n","    logger.info(\"  Batch size = %d\", args.train_batch_size)\n","    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n","        \n","    model.train()\n","    best_acc=0\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0        \n","    bar = tqdm(range(num_train_optimization_steps),total=num_train_optimization_steps, disable=False, leave=True, position=1)\n","    train_dataloader=cycle(train_dataloader)\n","        \n","    for step in bar:\n","\n","        batch = next(train_dataloader)\n","        batch = tuple(t.to(device) for t in batch)\n","        input_ids, input_mask, segment_ids, label_ids,node_ids,adj_mask = batch\n","        loss = model(input_ids=input_ids, \n","                     token_type_ids=segment_ids, \n","                     attention_mask=input_mask, \n","                     labels=label_ids,\n","                     node_mask=node_ids,\n","                     adj_mask=adj_mask)\n","        if args.n_gpu > 1:\n","            loss = loss.mean() # mean() to average on multi-gpu.\n","        if args.fp16 and args.loss_scale != 1.0:\n","            loss = loss * args.loss_scale\n","        if args.gradient_accumulation_steps > 1:\n","            loss = loss / args.gradient_accumulation_steps\n","        tr_loss += loss.item()\n","        train_loss=round(tr_loss*args.gradient_accumulation_steps/(nb_tr_steps+1),4)\n","        bar.set_description(\"loss {}\".format(train_loss))\n","        nb_tr_examples += input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","        if args.fp16:\n","            optimizer.backward(loss)\n","        else:\n","            loss.backward()\n","\n","        if (nb_tr_steps + 1) % args.gradient_accumulation_steps == 0:\n","            if args.fp16:\n","                # modify learning rate with special warm up BERT uses\n","                # if args.fp16 is False, BertAdam is used that handles this automatically\n","                lr_this_step = args.learning_rate * warmup_linear.get_lr(global_step, args.warmup_proportion)\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = lr_this_step\n","            scheduler.step()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            global_step += 1\n","\n","        if (step + 1) %(args.eval_steps*args.gradient_accumulation_steps)==0:\n","            tr_loss = 0\n","            nb_tr_examples, nb_tr_steps = 0, 0 \n","            logger.info(\"***** Report result *****\")\n","            logger.info(\"  %s = %s\", 'global_step', str(global_step))\n","            logger.info(\"  %s = %s\", 'train loss', str(train_loss))\n","\n","        if (step + 1) %(args.eval_steps * args.gradient_accumulation_steps)==0:\n","\n","             for file in ['dev.jsonl']:\n","                eval_examples = read_examples(os.path.join(args.data_dir, file), is_training = True)\n","                inference_labels=[]\n","                gold_labels=[]\n","                eval_examples = read_examples(os.path.join(args.data_dir, file), is_training = True)\n","                eval_features = convert_examples_to_features(eval_examples, tokenizer, args.max_seq_length,False)\n","                all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n","                all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n","                all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n","\n","                all_node_ids = torch.tensor(select_field(eval_features, 'node_ids'), dtype=torch.long)\n","                all_adj_mask = torch.tensor(select_field(eval_features, 'adj_mask'), dtype=torch.long)                     \n","\n","                all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n","\n","                eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label,all_node_ids,all_adj_mask)\n","                        \n","                logger.info(\"***** Running evaluation *****\")\n","                logger.info(\"  Num examples = %d\", len(eval_examples))\n","                logger.info(\"  Batch size = %d\", args.eval_batch_size)  \n","                        \n","                # Run prediction for full data\n","                eval_sampler = SequentialSampler(eval_data)\n","                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n","\n","                model.eval()\n","\n","                eval_loss, eval_accuracy = 0, 0\n","                nb_eval_steps, nb_eval_examples = 0, 0\n","                for input_ids, input_mask, segment_ids, label_ids,node_ids,adj_mask in eval_dataloader:\n","                    input_ids = input_ids.to(device)\n","                    input_mask = input_mask.to(device)\n","                    segment_ids = segment_ids.to(device)\n","                    label_ids = label_ids.to(device)\n","                    node_ids = node_ids.to(device)\n","                    adj_mask = adj_mask.to(device) \n","\n","                    with torch.no_grad():\n","                        tmp_eval_loss= model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, labels=label_ids,node_mask=node_ids,adj_mask=adj_mask)\n","                        logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask,node_mask=node_ids,adj_mask=adj_mask)\n","\n","                    logits = logits.detach().cpu().numpy()\n","                    label_ids = label_ids.to('cpu').numpy()\n","                    tmp_eval_accuracy = accuracy(logits, label_ids)\n","                    inference_labels.append(np.argmax(logits, axis=1))\n","                    gold_labels.append(label_ids)\n","                    eval_loss += tmp_eval_loss.mean().item()\n","                    eval_accuracy += tmp_eval_accuracy\n","\n","                    nb_eval_examples += input_ids.size(0)\n","                    nb_eval_steps += 1\n","\n","                eval_loss = eval_loss / nb_eval_steps\n","                eval_accuracy = eval_accuracy / nb_eval_examples\n","\n","                result = {'eval_loss': eval_loss,\n","                          'eval_accuracy': eval_accuracy,\n","                          'global_step': global_step+1,\n","                          'loss': train_loss}\n","\n","                output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","                with open(output_eval_file, \"a\") as writer:\n","                    for key in sorted(result.keys()):\n","                        logger.info(\"  %s = %s\", key, str(result[key]))\n","                        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","                    writer.write('*'*80)\n","                    writer.write('\\n')\n","                if eval_accuracy>best_acc and 'dev' in file:\n","                    print(\"=\"*80)\n","                    print(\"Best Acc\",eval_accuracy)\n","                    print(\"Saving Model......\")\n","                    best_acc=eval_accuracy\n","\n","                    # Save a trained model\n","                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","                    output_model_file = os.path.join(args.output_dir, \"pytorch_model.bin\")\n","                    torch.save(model_to_save.state_dict(), output_model_file)\n","                    print(\"=\"*80)\n","                    inference_labels=np.concatenate(inference_labels,0)\n","                    gold_labels=np.concatenate(gold_labels,0)\n","\n","                    with open(os.path.join(args.output_dir, \"error_output.txt\"),'w') as f:\n","                        for i in range(len(eval_examples)):\n","                            if inference_labels[i]!=gold_labels[i]:\n","                                try:\n","                                    f.write(str(repr(eval_examples[i]))+'\\n')\n","                                    f.write(str(inference_labels[i])+'\\n')\n","                                    f.write(\"=\"*80+'\\n')\n","                                except:\n","                                    print('Failed to write the file.')\n","                                    pass\n","                else:\n","                    print(\"=\"*80) \n","\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"2IugYpn4pb6c","executionInfo":{"status":"ok","timestamp":1608098755435,"user_tz":300,"elapsed":334,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["def run(data_dir, output_dir,\n","        model_type='graph_xlnet',\n","        model_name='xlnet-large-cased',\n","        task_name=None):\n","\n","    parser = argparse.ArgumentParser(description=\"Common sense question answering\")\n","\n","    # Required parameters\n","    parser.add_argument(\"--data_dir\", default=data_dir, type=str,\n","                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n","    parser.add_argument(\"--output_dir\", default=output_dir, type=str,\n","                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n","    \n","    # Training parameters\n","    parser.add_argument(\"--model_type\", type=str, default=model_type,\n","                        help=\"Model: <str> [ bert | xlnet | roberta | gpt2 ]\")\n","    parser.add_argument(\"--task_name\", default=task_name, type=str, required=False,\n","                        help=\"The name of the task to train: <str> [ commonqa ]\")\n","    parser.add_argument(\"--model_name\", type=str,\n","                        default=model_name,\n","                        help=\"Path to pre-trained model or shortcut name.\"\n","                              \"See https://huggingface.co/models\")\n","    parser.add_argument(\"--config_name\", type=str,\n","                        default=model_name,\n","                        help=\"Pre-trained config name or path\")\n","    parser.add_argument(\"--tokenizer_name\", default=model_name, type=str,\n","                        help=\"Pre-trained tokenizer name or path if not the same as model_name\")\n","\n","    # Other parameters\n","    parser.add_argument(\"--max_seq_length\", default=256, type=int,\n","                        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n","                             \"than this will be truncated, sequences shorter will be padded.\")\n","    parser.add_argument(\"--do_lower_case\", action='store_true',\n","                        help=\"Set this flag if you are using an uncased model.\")\n","\n","    parser.add_argument(\"--per_gpu_train_batch_size\", default=2, type=int,\n","                        help=\"Batch size per GPU/CPU for training.\")\n","    parser.add_argument(\"--per_gpu_eval_batch_size\", default=8, type=int,\n","                        help=\"Batch size per GPU/CPU for evaluation.\")\n","    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n","                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n","    parser.add_argument(\"--learning_rate\", default=5e-6, type=float,\n","                        help=\"The initial learning rate for Adam.\")\n","    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n","                        help=\"Weight deay if we apply some.\")\n","    parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float,\n","                        help=\"Epsilon for Adam optimizer.\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n","                        help=\"Max gradient norm.\")\n","    parser.add_argument(\"--num_train_epochs\", default=3.0, type=float,\n","                        help=\"Total number of training epochs to perform.\")\n","    parser.add_argument(\"--max_steps\", default=-1, type=int,\n","                        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n","    parser.add_argument(\"--eval_steps\", default=200, type=int,\n","                        help=\"\")\n","    parser.add_argument(\"--train_steps\", default=10000, type=int,\n","                        help=\"\")  # Original Paper: 40000\n","    parser.add_argument(\"--report_steps\", default=1000, type=int,\n","                        help=\"\")\n","    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n","                        help=\"Linear warmup over warmup_steps.\")\n","    \n","    parser.add_argument('--logging_steps', type=int, default=50,\n","                        help=\"Log every X updates steps.\")\n","    parser.add_argument('--save_steps', type=int, default=50,\n","                        help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n","                        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\")\n","    parser.add_argument(\"--no_cuda\", action='store_true',\n","                        help=\"Avoid using CUDA when available\")\n","    parser.add_argument('--seed', type=int, default=0,\n","                        help=\"random seed for initialization\")\n","\n","    parser.add_argument('--fp16', action='store_true',\n","                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n","    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n","                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n","                             \"See details at https://nvidia.github.io/apex/amp.html\")\n","    parser.add_argument(\"--local_rank\", type=int, default=-1,\n","                        help=\"For distributed training: local_rank\")\n","    parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n","    parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n","    args, unknown = parser.parse_known_args()\n","    \n","    main(args)\n"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EhQU98dnbLPj"},"source":["## Finetuning"]},{"cell_type":"code","metadata":{"id":"ar_rPSg3iH9D","executionInfo":{"status":"ok","timestamp":1608097806573,"user_tz":300,"elapsed":98126,"user":{"displayName":"Ziqiao Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghhgw57PFddhvv4DzLKlzBIwvKXHtTrpBmNJ8kKUw=s64","userId":"06419588537249170261"}}},"source":["!mkdir csqa_out"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enuJEni4Go32"},"source":["Run the graph based `graph_xlnet`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ve1Jc8BlqDyi","outputId":"021b3031-b983-43f4-d96a-0960bb87519f"},"source":["data_dir = 'data/'\n","output_dir = 'csqa_out/'\n","model_type = 'graph_xlnet'\n","model_name = 'xlnet-large-cased'\n","task_name = 'commonqa'\n","\n","run(data_dir, output_dir, model_type, model_name, task_name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12/16/2020 06:05:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n"],"name":"stderr"},{"output_type":"stream","text":["Loading model graph_xlnet\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of GraphBasedXLNetModel were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['transformer.map.weight', 'transformer.map.bias', 'transformer.map_node_emb.weight', 'transformer.map_node_emb.bias', 'transformer.GCN_W.0.weight', 'transformer.GCN_W.0.bias', 'transformer.GCN_W.1.weight', 'transformer.GCN_W.1.bias', 'transformer.GCN_W_self.0.weight', 'transformer.GCN_W_self.0.bias', 'transformer.GCN_W_self.1.weight', 'transformer.GCN_W_self.1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","12/16/2020 06:06:46 - INFO - __main__ -   read cont:0\n","12/16/2020 06:06:57 - INFO - __main__ -   read cont:1000\n","12/16/2020 06:07:08 - INFO - __main__ -   read cont:2000\n","12/16/2020 06:07:17 - INFO - __main__ -   read cont:3000\n","12/16/2020 06:07:27 - INFO - __main__ -   read cont:4000\n","12/16/2020 06:07:39 - INFO - __main__ -   read cont:5000\n","12/16/2020 06:07:49 - INFO - __main__ -   read cont:6000\n","12/16/2020 06:07:59 - INFO - __main__ -   read cont:7000\n","12/16/2020 06:08:10 - INFO - __main__ -   read cont:8000\n","12/16/2020 06:08:21 - INFO - __main__ -   read cont:9000\n","12/16/2020 06:08:28 - INFO - __main__ -   *** Example ***\n","12/16/2020 06:08:28 - INFO - __main__ -   idx: 1\n","12/16/2020 06:08:28 - INFO - __main__ -   choice: 0\n","12/16/2020 06:08:28 - INFO - __main__ -   tokens: _punishing _has _sub even t _ignore # # pun ish ing _ , _ignore <sep> _ # _wiki pedia _ # _ mit ig ative _efforts _largely _ignore _gender _ . _ # # _is ra eli _textbooks _and _school _curriculum _also _often _ignore _pale stin ian _history _ . _ # # _to _his _disappointment _ , _most _fans _seemed _to _ignore _the _detail _ . _ # # _during _the _school _year _ , _kids _would _harass _ , _bull y _ , _and _ignore _her _ . _ # # _they _burn _the _money _ , _striking _a _blow _against _ fal cone _he _can _not _ignore _ . _ # # _it _just _seemed _to _have _something _that _you _could _ n ' t _ignore _ . _ \" _ # # _it _was _around _this _time _ , _when _many _countries _started _to _ignore _the _un _sanctions _ . _ # # _land ai s _was _continuing _to _ignore _the _flagship _ ' s _efforts _to _communicate _ . _ # # _should _sister _bear _ignore _the _new _girl _at _school _just _because _her _friends _do _ ? _ # # _do _not _ignore _the _song _ ' s _album _version _ , _which _kicks _an _old _ - _school _soul _vibe _ . _ \" _ # # <sep> _the _sanctions _against _the _school _were _a _punishing _blow , _and _they _seemed _to _what _the _efforts _the _school _had _made _to _change ? _the _answer _is _ignore <cls>\n","12/16/2020 06:08:28 - INFO - __main__ -   input_ids: 0 0 25975 51 1466 6287 46 6983 7967 7967 7338 1406 56 17 19 6983 4 17 7967 19348 27123 17 7967 17 3888 3033 2944 1074 2561 6983 6392 17 9 17 7967 7967 27 840 8252 21178 21 297 8400 77 437 6983 6827 13743 884 614 17 9 17 7967 7967 22 45 9931 17 19 127 1934 1395 22 6983 18 4089 17 9 17 7967 7967 181 18 297 119 17 19 1886 74 23755 17 19 6446 117 17 19 21 6983 62 17 9 17 7967 7967 63 6494 18 356 17 19 6800 24 4309 157 17 9939 15779 43 64 50 6983 17 9 17 7967 7967 36 125 1395 22 47 359 29 44 121 17 180 26 46 6983 17 9 17 12 17 7967 7967 36 30 199 52 92 17 19 90 142 452 505 22 6983 18 422 3811 17 9 17 7967 7967 883 5936 23 30 3304 22 6983 18 15802 17 26 23 1074 22 6775 17 9 17 7967 7967 170 2251 4151 6983 18 109 1615 38 297 125 149 62 1003 112 17 82 17 7967 7967 112 50 6983 18 1160 17 26 23 818 1040 17 19 59 17522 48 532 17 13 297 3776 26234 17 9 17 12 17 7967 7967 4 18 3811 157 18 297 55 24 25975 4309 19 21 63 1395 22 113 18 1074 18 297 54 140 22 459 82 18 1543 27 6983 3\n","12/16/2020 06:08:28 - INFO - __main__ -   input_mask: 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/16/2020 06:08:28 - INFO - __main__ -   segment_ids: 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n","12/16/2020 06:08:28 - INFO - __main__ -   label: 0\n","12/16/2020 06:08:28 - INFO - __main__ -   choice: 1\n","12/16/2020 06:08:28 - INFO - __main__ -   tokens: _enforce # # n one <sep> _ # _wiki pedia _ # _by _1986 _ , _93 _percent _of _whites _endorsed _the _principle _ , _but _only _26 _percent _endorsed _government _efforts _to _enforce _school _integration _ . _ # # _he _made _many _enemies _through _his _efforts _to _enforce _the _metropolitan _excise _law _ . _ # # _the _shutdown _undermined _efforts _by _the _united _states _to _prevent _money _la under ing _and _to _enforce _economic _sanctions _on _ ir an _ , _north _ kor ea _and _other _countries _ . _ # # _the _ na zi s _made _strenuous _efforts _to _enforce _a _view _of _ d zier zon _as _a _ ger man _ . _ # # _the _ john son _administration _made _only _modest _efforts _to _enforce _restrictions _on _this _boat _traffic _ . _ # # _in _1994 _ , _ \" _ ko ote nay _ \" _was _sent _to _enforce _the _sanctions _on _ha iti _ . _ # # _some _ eu _member _states _enforce _their _competition _laws _with _criminal _sanctions _ . _ # # _the _bankruptcy _court _still _has _various _sanctions _available _to _enforce _its _judgment s _ : _ # # <sep> _the _sanctions _against _the _school _were _a _punishing _blow , _and _they _seemed _to _what _the _efforts _the _school _had _made _to _change ? _the _answer _is _enforce <cls>\n","12/16/2020 06:08:28 - INFO - __main__ -   input_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9178 7967 7967 180 1112 4 17 7967 19348 27123 17 7967 37 4698 17 19 12306 141 20 17664 10691 18 4926 17 19 57 114 1558 141 10691 146 1074 22 9178 297 6241 17 9 17 7967 7967 43 140 142 6989 135 45 1074 22 9178 18 13126 30165 431 17 9 17 7967 7967 18 19045 22769 1074 37 18 9114 1035 22 1665 356 2483 3912 56 21 22 9178 531 3811 31 17 1121 262 17 19 1012 17 13204 4428 21 86 452 17 9 17 7967 7967 18 17 597 2928 23 140 29037 1074 22 9178 24 985 20 17 66 20350 9379 34 24 17 2371 249 17 9 17 7967 7967 18 17 22116 672 1048 140 114 6637 1074 22 9178 5062 31 52 2694 1958 17 9 17 7967 7967 25 2228 17 19 17 12 17 1507 7822 17007 17 12 30 797 22 9178 18 3811 31 10118 6225 17 9 17 7967 7967 106 17 4269 552 1035 9178 58 1475 2006 33 2437 3811 17 9 17 7967 7967 18 5051 593 194 51 807 3811 387 22 9178 81 5424 23 17 60 17 7967 7967 4 18 3811 157 18 297 55 24 25975 4309 19 21 63 1395 22 113 18 1074 18 297 54 140 22 459 82 18 1543 27 9178 3\n","12/16/2020 06:08:28 - INFO - __main__ -   input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/16/2020 06:08:28 - INFO - __main__ -   segment_ids: 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n","12/16/2020 06:08:28 - INFO - __main__ -   label: 0\n","12/16/2020 06:08:28 - INFO - __main__ -   choice: 2\n","12/16/2020 06:08:28 - INFO - __main__ -   tokens: _punishing _is _used _for _authoritarian # # n one <sep> _ # _wiki pedia _ # _this _was _the _way _that _he _ran _his _school _ , _as _an _authoritarian _ . _ # # _ mur illo _carried _the _same _authoritarian _tendencies _as _ nar va ez _but _made _serious _efforts _to _advance _span ish _industry _and _commerce _ . _ # # _once _upon _a _time _in _high _school _ \" _portrayed _the _authoritarian _society _of _the _1970 s _through _a _notoriously _violent _high _school _ . _ # # _in _1996 _ , _ fer ris _portrayed _the _ bru t ish _ , _authoritarian _school _head mi stress _a g atha _ tru nch bull _in _ \" _mat il da _ \" _ . _ # # _the _had awi y ya _school _of _is lam ic _law _ , _the _only _authoritarian _one _for _the _ za y di y ya _ , _stems _from _him _ . _ # # _she _demonstrated _that _ , _although _the _authoritarian _style _predict s _poor _school _achievement _in _ european _ american _children _ , _the _parenting _style _predict s _excellent _school _achievement _in _ chinese _ american _children _ . _ # # <sep> _the _sanctions _against _the _school _were _a _punishing _blow , _and _they _seemed _to _what _the _efforts _the _school _had _made _to _change ? _the _answer _is _authoritarian <cls>\n","12/16/2020 06:08:28 - INFO - __main__ -   input_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 25975 27 179 28 21864 7967 7967 180 1112 4 17 7967 19348 27123 17 7967 52 30 18 162 29 43 1662 45 297 17 19 34 48 21864 17 9 17 7967 7967 17 11066 9291 1708 18 219 21864 25406 34 17 6616 1598 2754 57 140 1363 1074 22 2511 6950 1406 624 21 10991 17 9 17 7967 7967 497 975 24 92 25 227 297 17 12 10386 18 21864 1814 20 18 2373 23 135 24 26689 4400 227 297 17 9 17 7967 7967 25 2025 17 19 17 2755 4156 10386 18 17 10997 46 1406 17 19 21864 297 291 2344 15545 24 299 17235 17 6380 5620 14699 25 17 12 10714 902 1011 17 12 17 9 17 7967 7967 18 54 14546 117 1489 297 20 27 6798 556 431 17 19 18 114 21864 65 28 18 17 2051 117 1528 117 1489 17 19 14756 40 103 17 9 17 7967 7967 85 6520 29 17 19 1082 18 21864 1393 6099 23 1557 297 7676 25 17 30707 17 15916 341 17 19 18 23307 1393 6099 23 2712 297 7676 25 17 30564 17 15916 341 17 9 17 7967 7967 4 18 3811 157 18 297 55 24 25975 4309 19 21 63 1395 22 113 18 1074 18 297 54 140 22 459 82 18 1543 27 21864 3\n","12/16/2020 06:08:28 - INFO - __main__ -   input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/16/2020 06:08:28 - INFO - __main__ -   segment_ids: 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n","12/16/2020 06:08:28 - INFO - __main__ -   label: 0\n","12/16/2020 06:08:28 - INFO - __main__ -   choice: 3\n","12/16/2020 06:08:28 - INFO - __main__ -   tokens: _punishing _has _sub even t _yell _at # # pun ish ing _ , _yell _at <sep> _ # _wiki pedia _ # _it _is _the _sole _high _school _administered _by _the _western _yell _county _school _district _ . _ # # _the _ ni cho ll s _school _sports _teams _were _named _ \" _the _rebels _ \" _ ; _the _school _newspaper _was _ \" _the _rebel _yell _ \" _ . _ # # _four che _valley _school _was _a _ k - 12 _public _school _in _unincorporated _yell _county _ , _ arkansas _ , _near _brig g sville _ . _ # # _four che _valley _school _district _ # _13 _was _a _school _district _headquartered _in _unincorporated _yell _county _ , _ arkansas _ , _near _brig g sville _ . _ # # _the _following _are _notable _people _associated _with _western _yell _county _high _school _ . _ # # _he _attended _both _ ta com a _ ' s _stadium _high _school _ , _where _he _was _a _yell _leader _ , _and _seat tle _ ' s _ gar field _high _school _ . _ # # <sep> _the _sanctions _against _the _school _were _a _punishing _blow , _and _they _seemed _to _what _the _efforts _the _school _had _made _to _change ? _the _answer _is _yell _at <cls>\n","12/16/2020 06:08:28 - INFO - __main__ -   input_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 25975 51 1466 6287 46 20474 38 7967 7967 7338 1406 56 17 19 20474 38 4 17 7967 19348 27123 17 7967 36 27 18 5703 227 297 10473 37 18 2237 20474 2413 297 1181 17 9 17 7967 7967 18 17 1197 3744 215 23 297 1721 1314 55 812 17 12 18 2445 17 12 17 97 18 297 1535 30 17 12 18 3479 20474 17 12 17 9 17 7967 7967 237 2970 5518 297 30 24 17 267 13 1396 281 297 25 18371 20474 2413 17 19 17 28768 17 19 479 31042 299 7463 17 9 17 7967 7967 237 2970 5518 297 1181 17 7967 646 30 24 297 1181 17451 25 18371 20474 2413 17 19 17 28768 17 19 479 31042 299 7463 17 9 17 7967 7967 18 405 41 5661 104 2019 33 2237 20474 2413 227 297 17 9 17 7967 7967 43 2253 207 17 751 756 101 17 26 23 4252 227 297 17 19 131 43 30 24 20474 691 17 19 21 1876 5979 17 26 23 17 2753 1465 227 297 17 9 17 7967 7967 4 18 3811 157 18 297 55 24 25975 4309 19 21 63 1395 22 113 18 1074 18 297 54 140 22 459 82 18 1543 27 20474 38 3\n","12/16/2020 06:08:28 - INFO - __main__ -   input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/16/2020 06:08:28 - INFO - __main__ -   segment_ids: 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n","12/16/2020 06:08:28 - INFO - __main__ -   label: 0\n","12/16/2020 06:08:28 - INFO - __main__ -   choice: 4\n","12/16/2020 06:08:28 - INFO - __main__ -   tokens: _punishing _has _sub even t _avoid # # av oid _is _a _translation _of _invalid ate # # in val i date _is _a _way _to _change <sep> _ # _wiki pedia _ # _in _1936 _ , _model _high _school _and _the _city _school _ , _mad ison _high _ , _combined _to _enrich _their _programs _and _to _avoid _a _ duplication _of _efforts _ . _ # # _ g wyn _often _had _to _change _his _home _and _his _school _to _avoid _fines _and _imprisonment _ . _ # # _in _reality _ , _however _ , _the _number _could _be _smaller _since _north _ kor ean _companies _regularly _change _names _in _order _to _avoid _sanctions _ . _ # # _to _avoid _controversy _ , _the _upper _school _changed _its _name _to _the _cre feld _school _ . _ # # _due _to _shame _and _embarrassment _ , _they _may _avoid _school _or _work _ , _or _repeatedly _change _jobs _and _move _to _another _town _ . _ # # _in _south _ carolina _ , _efforts _were _being _made _to _avoid _an _unnecessary _confrontation _ . _ # # _efforts _are _being _made _to _develop _sustainable _grazing _practices _to _avoid _desert ification _ . _ # # <sep> _the _sanctions _against _the _school _were _a _punishing _blow , _and _they _seemed _to _what _the _efforts _the _school _had _made _to _change ? _the _answer _is _avoid <cls>\n","12/16/2020 06:08:28 - INFO - __main__ -   input_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 25975 51 1466 6287 46 1685 7967 7967 4057 6744 27 24 5920 20 15032 1167 7967 7967 153 3657 150 7216 27 24 162 22 459 4 17 7967 19348 27123 17 7967 25 9191 17 19 1342 227 297 21 18 285 297 17 19 6321 7232 227 17 19 2670 22 11891 58 973 21 22 1685 24 17 27424 20 1074 17 9 17 7967 7967 17 299 15567 437 54 22 459 45 192 21 45 297 22 1685 13977 21 13461 17 9 17 7967 7967 25 2767 17 19 634 17 19 18 243 121 39 2171 196 1012 17 13204 6866 463 3955 459 1931 25 374 22 1685 3811 17 9 17 7967 7967 22 1685 6289 17 19 18 2969 297 1318 81 304 22 18 9028 8838 297 17 9 17 7967 7967 542 22 8483 21 17131 17 19 63 132 1685 297 49 154 17 19 49 4570 459 1549 21 579 22 245 584 17 9 17 7967 7967 25 1047 17 26625 17 19 1074 55 163 140 22 1685 48 11433 9638 17 9 17 7967 7967 1074 41 163 140 22 1627 6802 23215 3089 22 1685 5675 3804 17 9 17 7967 7967 4 18 3811 157 18 297 55 24 25975 4309 19 21 63 1395 22 113 18 1074 18 297 54 140 22 459 82 18 1543 27 1685 3\n","12/16/2020 06:08:28 - INFO - __main__ -   input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","12/16/2020 06:08:28 - INFO - __main__ -   segment_ids: 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n","12/16/2020 06:08:28 - INFO - __main__ -   label: 0\n","12/16/2020 06:09:11 - INFO - __main__ -   convert example to feature:1000\n","12/16/2020 06:09:54 - INFO - __main__ -   convert example to feature:2000\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"igmfTE57Okua"},"source":["# Results"]},{"cell_type":"markdown","metadata":{"id":"0RU5bFDZKJCC"},"source":["The original paper ran experiments on 2 P100 GPUs with 50 GB RAM, and trained for 40000 epochs. We cannot afford the hardware on Colab. Therefore, the code is tested on a reduced scale."]},{"cell_type":"markdown","metadata":{"id":"rvqXCTrLK7_6"},"source":["Result reported by the paper:\n","* Validation Accuracy: 79.3%\n","* Test Accuracy: 75.3%\n","\n","Our experiement:\n","* Validation Accuracy: 73.0%\n","* Test Accuracy: Not Available"]},{"cell_type":"markdown","metadata":{"id":"KRMR_R3MjHZP"},"source":["# References\n","\n","[1] Talmor, A., Herzig, J., Lourie, N., & Berant, J. (2019). CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. ArXiv, abs/1811.00937.\n","\n","[2] Lv, S., Guo, D., Xu, J., Tang, D., Duan, N., Gong, M., Shou, L., Jiang, D., Cao, G., & Hu, S. (2020). Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering. AAAI.\n","\n","[3] Speer, R., Chin, J., & Havasi, C. (2017). ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. ArXiv, abs/1612.03975."]},{"cell_type":"markdown","metadata":{"id":"4L9W8vLaMKT5"},"source":["```\n","# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n","# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","```"]}]}